{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "SEED = 41\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [['This is a cool story'],\n",
    "        ['Aruba is in Jamaica.'],\n",
    "        ['What the hell is going on here?'],\n",
    "        ['The meanining of a word is its use in the language.'],\n",
    "        ['What goes up comes down? me amigo'],\n",
    "        ['This is going to be a sweet sweet ride.'],\n",
    "        ['Man City quadruple is off.'],\n",
    "        ['Scousers are bin dippers.'],\n",
    "        ['Man Utd will be back because we always come back.']\n",
    "       ]\n",
    "\n",
    "text  = pd.DataFrame(text)\n",
    "text.columns = ['comment_text']\n",
    "text.loc[:, 'toxic']  = [1., 0., 1., 1., 0., 0., 0., 1., 0.]\n",
    "text.loc[:, 'severe_toxic']  = [0., 1., 0., 0., 1., 1., 1., 1., 0.]\n",
    "text.loc[:, 'obscene']  = [0., 1., 0., 0., 1., 1., 1., 1., 0.]\n",
    "text.loc[:, 'threat']  = [0., 1., 0., 0., 1., 1., 1., 1., 0.]\n",
    "text.loc[:, 'insult']  = [0., 1., 0., 0., 1., 1., 1., 1., 0.]\n",
    "text.loc[:, 'identity_hate']  = [0., 1., 0., 0., 1., 1., 1., 1., 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_labels(y): return all(v is None for v in y)\n",
    "\n",
    "def load_w2v_embedding(emb_matrix):\n",
    "    word2vec_dict   = word2vec.KeyedVectors.load_word2vec_format('../data/processed/word2vec.bin.gz', binary=True)\n",
    "    embedding_index = dict()\n",
    "    \n",
    "    for word in word2vec_dict.wv.vocab:\n",
    "        embedding_index[word] = word2vec_dict.word_vec(word)\n",
    "\n",
    "    embed_cnt = 0\n",
    "\n",
    "    for i, word in enumerate(vocab.itos):\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "\n",
    "        if embedding_vector is not None:\n",
    "            emb_matrix[i] = embedding_vector\n",
    "            embed_cnt +=1\n",
    "\n",
    "    del embedding_index\n",
    "    gc.collect()\n",
    "\n",
    "    # fill pad token with all zeros\n",
    "    emb_matrix[vocab.stoi['xxxpad']] = np.zeros(embed_size)\n",
    "    emb_matrix[vocab.stoi['xxxunk']] = np.zeros(embed_size)\n",
    "    print('total embedded {} common words'.format(embed_cnt))\n",
    "    \n",
    "    return emb_matrix\n",
    "    \n",
    "    \n",
    "class Tokenizer():\n",
    "    def __init__(self, lang='en'):\n",
    "        self.tok = spacy.blank(lang, disable=['parser', 'tagger', 'ner'])\n",
    "    \n",
    "    def tokenizer(self, t):\n",
    "        return [t.text for t in self.tok.tokenizer(t)]\n",
    "    \n",
    "class Vocab():\n",
    "    def __init__(self, itos):\n",
    "        self.itos = itos\n",
    "        self.stoi = defaultdict(int, {v:k for k,v in enumerate(self.itos)})\n",
    "    \n",
    "    def numericalize(self, t):\n",
    "        return [self.stoi.get(w, self.stoi['xxxunk']) for w in t]\n",
    "    \n",
    "    def fix_len(self, sent_len, numericalized_tokens):\n",
    "        return [nt[:sent_len] for nt in numericalized_tokens]\n",
    "    \n",
    "    def __getstate__(self):\n",
    "        return {'itos':self.itos}\n",
    "    \n",
    "    def textify(self, nums, sep=' '):\n",
    "        \"Convert a list of `nums` to their tokens.\"\n",
    "        return sep.join([self.itos[i] for i in nums]) if sep is not None else [self.itos[i] for i in nums]\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, tokens, max_vocab, min_freq):\n",
    "        freq = Counter(p for o in tokens for p in o)\n",
    "        itos = [o for o, c in freq.most_common(max_vocab) if c>= min_freq]\n",
    "        itos = cls.add_special_symbols(itos)\n",
    "        return cls(itos)\n",
    "    \n",
    "    @classmethod\n",
    "    def add_special_symbols(cls, itos):\n",
    "        pad_sym = 'xxxpad' #TODO: make sure we use config to introduce this symbol\n",
    "        unk_sym = 'xxxunk'\n",
    "        itos.append(pad_sym)\n",
    "        itos.append(unk_sym)\n",
    "        return itos\n",
    "\n",
    "class TextLMData():\n",
    "    \n",
    "    def __init__(self, path, \n",
    "                 csv,\n",
    "                 test_csv,\n",
    "                 text_col, \n",
    "                 label_cols, \n",
    "                 max_vocab,\n",
    "                 min_freq,\n",
    "                 valid_pct=0.2):\n",
    "        \n",
    "        self.path       = path\n",
    "        self.csv        = csv\n",
    "        self.test_csv   = test_csv\n",
    "        self.text_cols  = text_col\n",
    "        self.label_cols = label_cols\n",
    "        self.valid_pct  = valid_pct\n",
    "        self.max_vocab  = max_vocab\n",
    "        self.min_freq   = min_freq\n",
    "        \n",
    "        self.df      = pd.read_csv(Path(self.path)/self.csv)\n",
    "        if self.test_csv is not None: self.test_df = pd.read_csv(Path(self.path)/self.test_csv) \n",
    "        self.cut     = int(valid_pct * len(self.df)) + 1\n",
    "        \n",
    "    def process(self):\n",
    "        tok = Tokenizer()\n",
    "        \n",
    "        # consider entire corpus as text ( train + test text columns )\n",
    "        if self.test_csv:\n",
    "            text = list(self.df.loc[:, text_col].values) + list(self.test_df.loc[:, text_col])\n",
    "        else:\n",
    "            text = list(self.df.loc[:, text_col].values)\n",
    "        \n",
    "        self.tokens  = [tok.tokenizer(text) for text in text]\n",
    "        self.vocab   = Vocab.create(self.tokens, self.max_vocab, self.min_freq)\n",
    "        \n",
    "        self.ntokens = [self.vocab.numericalize(t) for t in self.tokens]\n",
    "        \n",
    "        # only full training\n",
    "        if self.valid_pct == 0 and self.test_csv is None:\n",
    "            self.trn_ds      = (self.ntokens, self.df.loc[:, label_cols].values)\n",
    "            self.vld_tokens  = ([], [])\n",
    "            self.test_tokens = ([], [])\n",
    "        \n",
    "        # holdout\n",
    "        elif self.valid_pct > 0 and self.test_csv is None:\n",
    "            self.trn_ds  = (self.ntokens[self.cut:], self.df.loc[:, label_cols].values[self.cut:])\n",
    "            self.vld_ds  = (self.ntokens[:self.cut], self.df.loc[:, label_cols].values[:self.cut])\n",
    "            self.tst_ds  = ([], [])\n",
    "        \n",
    "        # holdout and test prediction\n",
    "        elif self.valid_pct > 0 and self.test_csv is not None:\n",
    "            self.trn_tokens  = self.ntokens[:len(self.df)]\n",
    "            self.tst_ds      = (self.ntokens[len(self.df):], [])\n",
    "            \n",
    "            trn_tokens  = self.trn_tokens[self.cut:]\n",
    "            vld_tokens  = self.trn_tokens[:self.cut]\n",
    "            \n",
    "            self.trn_ds = (trn_tokens, self.df.loc[:, label_cols].values[self.cut:])\n",
    "            self.vld_ds = (vld_tokens, self.df.loc[:, label_cols].values[:self.cut])\n",
    "        \n",
    "        # full training and test prediction\n",
    "        else:\n",
    "            self.trn_ds  = (self.ntokens[:len(self.df)], self.df.loc[:, label_cols].values)\n",
    "            self.vld_ds  = ([], [])\n",
    "            self.tst_ds  = (self.ntokens[len(self.df):], [])\n",
    "            \n",
    "        return self.vocab, self.trn_ds, self.vld_ds, self.tst_ds\n",
    "    \n",
    "    def fill_emb_matrix(self, vocab, emb_type, embed_size):\n",
    "        emb_matrix = np.random.random(size=(len(self.vocab.itos), embed_size))\n",
    "        \n",
    "        if emb_type == 'w2v':\n",
    "            emb_matrix = load_w2v_embedding(emb_matrix)\n",
    "            \n",
    "        return emb_matrix\n",
    "        \n",
    "class TextClassData(Dataset):\n",
    "    def __init__(self, vocab, ds):\n",
    "        self.vocab       = vocab\n",
    "        self.ds, self.y  = ds\n",
    "                            \n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "                            \n",
    "    def __getitem__(self, index):\n",
    "        x = torch.LongTensor(self.ds[index])\n",
    "        y = None\n",
    "        if len(self.y) > 0: y = torch.FloatTensor(self.y[index])\n",
    "        \n",
    "        return x, y\n",
    "        \n",
    "def pad_collate(data, pad_idx, sent_len):\n",
    "    if len(data) == 1:\n",
    "        sequences, labels = data[0]\n",
    "        sequences = sequences.view(1, -1)\n",
    "        if labels is not None: labels    = labels.view(1, -1)\n",
    "    else:\n",
    "        sequences, labels = zip(*data)\n",
    "        if not check_labels(labels): labels = torch.cat([l.view(-1, 1) for l in labels], dim=1).t()\n",
    "        sequences = nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=pad_idx)\n",
    "    \n",
    "    sent_len  = min(sequences.size(1), sent_len)\n",
    "    sequences = sequences[:, :sent_len]\n",
    "    \n",
    "    return sequences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path       = Path('../data/processed')\n",
    "csv        = 'sample.csv'\n",
    "text_col   = 'comment_text'\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "test_csv   = None\n",
    "max_vocab  = 100000\n",
    "min_freq   = 3\n",
    "embed_size = 3\n",
    "emb_type   = 'glove'\n",
    "valid_pct  = .2 # change it to 0 for full training modmax_vocab  = 100000\n",
    "\n",
    "tmp = TextLMData(path, \n",
    "                 csv,\n",
    "                 test_csv,\n",
    "                 text_col, \n",
    "                 label_cols,\n",
    "                 max_vocab,\n",
    "                 min_freq,\n",
    "                 valid_pct=valid_pct\n",
    "                )\n",
    "\n",
    "vocab, trn_ds, vld_ds, tst_ds = tmp.process()\n",
    "emb_matrix                    = tmp.fill_emb_matrix(vocab, emb_type, embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_len   = 10\n",
    "collate_fn = partial(pad_collate, pad_idx=vocab.stoi['xxxpad'], sent_len=sent_len) #TODO: make sure pad symbol is defined as a constant\n",
    "\n",
    "trn_ds     = TextClassData(vocab, trn_ds)\n",
    "vld_ds     = TextClassData(vocab, vld_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl     = torch.utils.data.DataLoader(trn_ds, batch_size=2, shuffle=True, collate_fn=collate_fn, num_workers=4)\n",
    "vld_dl     = torch.utils.data.DataLoader(vld_ds, batch_size=2, shuffle=False, collate_fn=collate_fn, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, emb_matrix, vocab_size, embed_size):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.embedding        = nn.Embedding(vocab_size, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(emb_matrix)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.input_size  = 3\n",
    "        self.hidden_size = 5 \n",
    "        self.nfilters    = 7\n",
    "        \n",
    "        self.relu     = nn.ReLU()\n",
    "        self.tanh     = nn.Tanh()\n",
    "        self.lstm     = nn.LSTM(self.input_size, self.hidden_size, num_layers=1)\n",
    "        self.conv     = nn.Conv1d(in_channels=self.hidden_size * 2 + embed_size,\n",
    "                                  out_channels=self.nfilters,\n",
    "                                  kernel_size=1,\n",
    "                                 )\n",
    "        self.projection_layer = nn.Linear(self.nfilters, 6)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        embed         = self.embedding(x)\n",
    "        left_context  = F.pad(embed, (0, 0, 1, 0, 0, 0))[:, :-1, :]\n",
    "        right_context = F.pad(embed, (0, 0, 0, 1, 0, 0))[:, 1:, :]\n",
    "        \n",
    "        left_context  = left_context.permute(1, 0, 2)\n",
    "        fwd, _        = self.lstm(left_context)\n",
    "        \n",
    "        right_context_reversed = torch.flip(right_context, [1])\n",
    "        right_context_reversed = right_context_reversed.permute(1, 0, 2)\n",
    "        \n",
    "        bwd, _   = self.lstm(right_context_reversed)\n",
    "        bwd      = torch.flip(bwd, [1])\n",
    "        \n",
    "        # change from (seq_len, batch, embed_size) -> (batch, seq_len, embed_size)\n",
    "        fwd       = fwd.permute(1, 0, 2)\n",
    "        bwd       = bwd.permute(1, 0, 2)\n",
    "        \n",
    "        out = torch.cat((fwd, embed, bwd), dim=2)\n",
    "        \n",
    "        # change from (batch, seq_len, embed_size) -> (batch, embed_size, seq_len)\n",
    "        out = out.permute(0, 2, 1)\n",
    "        out = self.conv(out)\n",
    "        out = self.tanh(out)\n",
    "        \n",
    "        print(embed.size())\n",
    "        print(out.size())\n",
    "        \n",
    "        # do max pooling\n",
    "        out = out.max(dim=2)[0]\n",
    "        print('after max pooling: {}'.format(out.size()))\n",
    "        out = self.projection_layer(out)\n",
    "        print(out.size())\n",
    "        \n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 3])\n",
      "torch.Size([2, 7, 10])\n",
      "after max pooling: torch.Size([2, 7])\n",
      "torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "x, _  = next(iter(trn_dl))\n",
    "model = CNN(torch.FloatTensor(emb_matrix), len(vocab.itos), embed_size) \n",
    "out   = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss      = 0\n",
    "    per_label_preds = [[], [], [], [], [], []]\n",
    "    per_label_true  = [[], [], [], [], [], []]\n",
    "\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        X, y        = batch        \n",
    "        \n",
    "        X           = X.to(device)\n",
    "        y           = y.to(device)\n",
    "        \n",
    "        predictions = model(X)\n",
    "        \n",
    "        loss = criterion(predictions, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # convert true target\n",
    "        batch_target = y.cpu().detach().numpy()\n",
    "        logits_cpu   = predictions.cpu().detach().numpy()\n",
    "\n",
    "        # per_label_preds\n",
    "        for j in range(6):\n",
    "            label_preds     = logits_cpu[:, j]\n",
    "            per_label_preds[j].extend(label_preds)\n",
    "            per_label_true[j].extend(batch_target[:, j])\n",
    "\n",
    "        # calculate log loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        print('\\r[{} / {}]: Loss = {:.4f}'.format(\n",
    "              i, len(iterator), loss.item(), end=''))\n",
    "    \n",
    "    label_auc = []\n",
    "\n",
    "    for i in range(6):\n",
    "        label_auc.append(roc_auc_score(per_label_true[i], per_label_preds[i]))\n",
    "    \n",
    "    return epoch_loss / len(iterator), np.mean(label_auc)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss      = 0\n",
    "    per_label_preds = [[], [], [], [], [], []]\n",
    "    per_label_true  = [[], [], [], [], [], []]\n",
    "    preds           = []\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "            X, y        = batch\n",
    "            \n",
    "            X           = X.to(device)\n",
    "            predictions = model(X)\n",
    "            \n",
    "            # convert true target\n",
    "            logits_cpu   = predictions.cpu().detach().numpy()\n",
    "\n",
    "            preds.append(logits_cpu)\n",
    "            \n",
    "            if not check_labels(y): \n",
    "                y    = y.to(device)\n",
    "                loss = criterion(predictions, y)\n",
    "                batch_target = y.cpu().detach().numpy()\n",
    "                \n",
    "                \n",
    "                # per_label_preds\n",
    "                for j in range(6):\n",
    "                    label_preds     = logits_cpu[:, j]\n",
    "                    per_label_preds[j].extend(label_preds)\n",
    "                    per_label_true[j].extend(batch_target[:, j])\n",
    "\n",
    "                # calculate log loss\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                print('\\r[{} / {}]: Loss = {:.4f}'.format(\n",
    "                      i, len(iterator), loss.item(), end=''))\n",
    "    \n",
    "    label_auc = []\n",
    "\n",
    "    if len(per_label_preds[0]) > 0:\n",
    "        for i in range(6):\n",
    "            label_auc.append(roc_auc_score(per_label_true[i], per_label_preds[i]))\n",
    "\n",
    "    return epoch_loss / len(iterator), np.mean(label_auc) if len(label_auc) > 0 else 0, np.vstack(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_auc    = train(model, trn_dl, optimizer, criterion)\n",
    "    valid_loss, valid_auc, _ = evaluate(model, vld_dl, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "#         torch.save(model.state_dict(), 'tut4-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train AUC: {train_auc:.2f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. AUC: {valid_auc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
