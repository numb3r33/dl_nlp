{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "SEED = 41\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_PATH        = '../data/raw/'\n",
    "PROCESSED_DATA_PATH  = '../data/processed/' \n",
    "\n",
    "MAX_LEN = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.46 s, sys: 204 ms, total: 1.66 s\n",
      "Wall time: 1.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train       = pd.read_csv(os.path.join(RAW_DATA_PATH, 'train.csv'))\n",
    "test        = pd.read_csv(os.path.join(RAW_DATA_PATH, 'test.csv'))\n",
    "test_labels = pd.read_csv(os.path.join(RAW_DATA_PATH, 'test_labels.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((159571, 8), ' ', (153164, 2), ' ', (153164, 7))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, ' ', test.shape, ' ', test_labels.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    144277\n",
       "1     15294\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.toxic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    157976\n",
       "1      1595\n",
       "Name: severe_toxic, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.severe_toxic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    151122\n",
       "1      8449\n",
       "Name: obscene, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.obscene.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    159093\n",
       "1       478\n",
       "Name: threat, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.threat.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    151694\n",
       "1      7877\n",
       "Name: insult, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.insult.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    158166\n",
       "1      1405\n",
       "Name: identity_hate, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.identity_hate.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>001810bf8c45bf5f</td>\n",
       "      <td>You are gay or antisemmitian? \\n\\nArchangel WH...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>00190820581d90ce</td>\n",
       "      <td>FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0020e7119b96eeeb</td>\n",
       "      <td>Stupid peace of shit stop deleting my stuff as...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0020fd96ed3b8c8b</td>\n",
       "      <td>=Tony Sidaway is obviously a fistfuckee. He lo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                       comment_text  \\\n",
       "6   0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK   \n",
       "42  001810bf8c45bf5f  You are gay or antisemmitian? \\n\\nArchangel WH...   \n",
       "43  00190820581d90ce           FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!   \n",
       "55  0020e7119b96eeeb  Stupid peace of shit stop deleting my stuff as...   \n",
       "56  0020fd96ed3b8c8b  =Tony Sidaway is obviously a fistfuckee. He lo...   \n",
       "\n",
       "    toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "6       1             1        1       0       1              0  \n",
       "42      1             0        1       0       1              1  \n",
       "43      1             0        1       0       1              0  \n",
       "55      1             1        1       0       1              0  \n",
       "56      1             0        1       0       1              0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.query('(toxic == 1) & (insult == 1)').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0020e7119b96eeeb</td>\n",
       "      <td>Stupid peace of shit stop deleting my stuff as...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>006e87872c8b370c</td>\n",
       "      <td>you are a stupid fuck \\n\\nand your mother's cu...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>01208d2b76624130</td>\n",
       "      <td>Hi \\n\\nIm a fucking bitch.\\n\\n50.180.208.181</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>018663f910e0bfe6</td>\n",
       "      <td>What a motherfucking piece of crap those fuckh...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                                       comment_text  \\\n",
       "6    0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK   \n",
       "55   0020e7119b96eeeb  Stupid peace of shit stop deleting my stuff as...   \n",
       "181  006e87872c8b370c  you are a stupid fuck \\n\\nand your mother's cu...   \n",
       "442  01208d2b76624130       Hi \\n\\nIm a fucking bitch.\\n\\n50.180.208.181   \n",
       "579  018663f910e0bfe6  What a motherfucking piece of crap those fuckh...   \n",
       "\n",
       "     toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "6        1             1        1       0       1              0  \n",
       "55       1             1        1       0       1              0  \n",
       "181      1             1        1       0       1              0  \n",
       "442      1             1        1       0       1              0  \n",
       "579      1             1        1       0       1              0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.query('(toxic == 1) & (severe_toxic == 1)').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>001810bf8c45bf5f</td>\n",
       "      <td>You are gay or antisemmitian? \\n\\nArchangel WH...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>00190820581d90ce</td>\n",
       "      <td>FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>001dc38a83d420cf</td>\n",
       "      <td>GET FUCKED UP. GET FUCKEEED UP.  GOT A DRINK T...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0020e7119b96eeeb</td>\n",
       "      <td>Stupid peace of shit stop deleting my stuff as...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                       comment_text  \\\n",
       "6   0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK   \n",
       "42  001810bf8c45bf5f  You are gay or antisemmitian? \\n\\nArchangel WH...   \n",
       "43  00190820581d90ce           FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!   \n",
       "51  001dc38a83d420cf  GET FUCKED UP. GET FUCKEEED UP.  GOT A DRINK T...   \n",
       "55  0020e7119b96eeeb  Stupid peace of shit stop deleting my stuff as...   \n",
       "\n",
       "    toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "6       1             1        1       0       1              0  \n",
       "42      1             0        1       0       1              1  \n",
       "43      1             0        1       0       1              0  \n",
       "51      1             0        1       0       0              0  \n",
       "55      1             1        1       0       1              0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.query('(toxic == 1) & (obscene == 1)').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.sample(frac=.2).to_csv(os.path.join(PROCESSED_DATA_PATH, 'train_sample.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check null rows.\n",
    "train.comment_text.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define target columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLS = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(PROCESSED_DATA_PATH, 'train_sample.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['decent'] = 1 - train.loc[:, TARGET_COLS].max(axis=1)\n",
    "TARGET_COLS     += ['decent']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "tokenizer = nltk.tokenize.WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.66 s, sys: 64 ms, total: 8.72 s\n",
      "Wall time: 8.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train['tokenized_comments'] = list(map(' '.join, map(tokenizer.tokenize, train.comment_text)))\n",
    "test['tokenized_comments']  = list(map(' '.join, map(tokenizer.tokenize, test.comment_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token to count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "token_counts = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.58 s, sys: 24 ms, total: 2.61 s\n",
      "Wall time: 2.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for comment in train.tokenized_comments:\n",
    "    token_counts.update(comment.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 562731),\n",
       " (',', 453829),\n",
       " ('the', 449181),\n",
       " ('to', 291937),\n",
       " ('I', 224460),\n",
       " ('of', 220958),\n",
       " ('and', 212559),\n",
       " ('a', 203959),\n",
       " (\"'\", 199211),\n",
       " ('you', 182078)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens: 240640\n",
      "('.', 562731)\n",
      "(',', 453829)\n",
      "('the', 449181)\n",
      "('to', 291937)\n",
      "('I', 224460)\n",
      "\n",
      "('Superlatives', 1)\n",
      "('Classifying', 1)\n",
      "('CIU', 1)\n"
     ]
    }
   ],
   "source": [
    "print('Total unique tokens: {}'.format(len(token_counts)))\n",
    "print('\\n'.join(map(str, token_counts.most_common(5))))\n",
    "print()\n",
    "print('\\n'.join(map(str, token_counts.most_common()[-3:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(token_counts.values(), range=(0, 10 ** 4), log=True, bins=50)\n",
    "plt.xlabel('Token Counts');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove rare words from the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 10\n",
    "tokens    = {token: count for token, count in token_counts.items() if count >= min_count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 32840\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "UNK, PAD = 'UNK', 'PAD'\n",
    "tokens   = [UNK, PAD] + sorted(tokens)\n",
    "print('Vocabulary size:', len(tokens))\n",
    "\n",
    "assert type(tokens) == list\n",
    "assert 'me' in tokens\n",
    "assert UNK in tokens\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping from token to id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 ms, sys: 0 ns, total: 16 ms\n",
      "Wall time: 18.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "token_to_id = dict(map(reversed, zip(range(len(tokens)), tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's use the vocabulary you've built to map text lines into neural network-digestible matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IX, PAD_IX = map(token_to_id.get, [UNK, PAD])\n",
    "\n",
    "def as_matrix(sequences, max_len=None):\n",
    "    \"\"\" Convert a list of tokens into a matrix with padding \"\"\"\n",
    "    if isinstance(sequences[0], str):\n",
    "        sequences = list(map(str.split, sequences))\n",
    "        \n",
    "    max_len = min(max(map(len, sequences)), max_len or float('inf'))\n",
    "    \n",
    "    matrix = np.full((len(sequences), max_len), np.int32(PAD_IX))\n",
    "    for i,seq in enumerate(sequences):\n",
    "        row_ix = [token_to_id.get(word, UNK_IX) for word in seq[:max_len]]\n",
    "        matrix[i, :len(row_ix)] = row_ix\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines:\n",
      "Learning Hawaiian as a second language Stubby , no citations . If / when improved , it can be put back in the article . When trying to learn Hawaiian as a second language , without a competent teacher and without native speakers of Hawaiian as models , English - speaking learners might mispronounce Hawaiian words by using English values for the letters . Also , learners might not be aware that one cannot simply replace the English words in an English sentence with Hawaiian words as a way to create a Hawaiian sentence . Hawaiian and English have important differences in the order of words in a phrase , and the order of phrases in a sentence . Reviving Hawaiian as a first language Stubby , no citations . If / when improved , it can be put back in the article . There is a certain tension between those who would revive a purist Hawaiian , as spoken in the early 19th century , and those who grew up speaking a colloquial Hawaiian shaped by more than one hundred years of contact with English and pidgin .\n",
      "\" Ban The reality of the matter is that every time Varsovian returned to wikipedia he inserted himself / invented a new dispute . You accuse me of nationalistic \"\" persistent POV - pushing pursuit \"\", where exactly have I done that , eh ? What a joke , you just made that up to justify your ban . Jaroslaw Bilaniuk , Jakiw Palij and Bohdan Koziy are simply not Poles , if somebody tries to add the category \"\" Polish Nazi Collaborator \"\" then he is simply trying to insert untrue information which needs to be removed . It ' s as simple as that . Did you care to check if there are any sources calling these guys \"\" Polish Nazi Collaborators \"\" or is that falls outside your intellectual capabilities ? How is removing wrong information \"\" persistent POV - pushing pursuit \"\"!? I am a member of 3 Wiki Projects , I worked on many different topic areas and improved countless pages , I have created 200 new articles , and a couple DYKs as well . And what exactly has Varsovian contributed to wikipedia apart from constantly creating disputes on Polish topics ? Are you able to name one single page on which Varsovian did some substantial work worthy of mention in one year of \"\" activity \"\" on Wikipedia ? I don ' t ask 30 or 40 pages but ONE single page . Your systematic attempts at trying to equal me with Varsovian are both shameful and disgraceful , but I am not surprised . One can easily guess the morals of a person who blocked me for 72 hours after I simply and politely asked if I am allowed to do something . Or person who first refuses to do AE because there is lack of transparency and a problem with notification ( good one that one ) and then comes back a couple of hours later to do a revenge AE nobody requested . And miraculously the transparency , which just a couple of hours earlier was this big problem , is all of a sudden no problem at all . \"\n",
      "\n",
      "Matrix:\n",
      "[[2450 2054 4514 ...    1    1    1]\n",
      " [  25  967 3681 ...    1    1    1]]\n"
     ]
    }
   ],
   "source": [
    "print('Lines:')\n",
    "print('\\n'.join(train['tokenized_comments'][:2].values), end='\\n\\n')\n",
    "print('Matrix:')\n",
    "print(as_matrix(train['tokenized_comments'])[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size =  25531\n",
      "Validation size =  6383\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_val = train_test_split(train, test_size=0.2, random_state=42)\n",
    "data_train.index     = range(len(data_train))\n",
    "data_val.index       = range(len(data_val))\n",
    "\n",
    "print(\"Train size = \", len(data_train))\n",
    "print(\"Validation size = \", len(data_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Batches\n",
    "\n",
    "```\n",
    "[[1, 2, 3, 4],\n",
    " [4, 1, 4, 1],\n",
    " ....\n",
    "] -> [[0, 1, 1], [1, 1, 0]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(matrix, labels, batch_size, predict_mode='train'):\n",
    "    indices = np.arange(len(matrix))\n",
    "    if predict_mode == 'train':\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, len(matrix), batch_size):\n",
    "        end = min(start + batch_size, len(matrix))\n",
    "        \n",
    "        batch_indices = indices[start: end]\n",
    "        X = matrix[batch_indices]\n",
    "        \n",
    "        if predict_mode != 'train': yield X\n",
    "        else: yield X, labels[batch_indices] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = as_matrix(data_train['tokenized_comments'], max_len=MAX_LEN)\n",
    "labels = data_train.loc[:, TARGET_COLS].values\n",
    "\n",
    "X, y   = next(iterate_batches(matrix, labels, batch_size=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TConvolution(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_classes, PAD_IX):\n",
    "        super(TConvolution, self).__init__()\n",
    "        \n",
    "        self.hidden_dim  = hidden_dim\n",
    "        self.vocab_size  = vocab_size\n",
    "        self.num_classes = num_classes \n",
    "        \n",
    "        self.Cin         = 1\n",
    "        self.Cout        = 1\n",
    "        \n",
    "        self.embedding   = nn.Embedding(self.vocab_size, self.hidden_dim, padding_idx=PAD_IX)\n",
    "        self.conv_layer1 = nn.Conv2d(self.Cout, self.Cin, kernel_size=(1, self.hidden_dim))\n",
    "        self.conv_layer2 = nn.Conv2d(self.Cout, self.Cin, kernel_size=(2, self.hidden_dim))\n",
    "        self.relu        = nn.ReLU()\n",
    "        self.fc          = nn.Linear(2, self.num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        # create a matrix of shape ( N, Cin, max_len, embedding_dim)\n",
    "        emb = emb.unsqueeze(1)\n",
    "        \n",
    "        # pass it through convolutional layer to calculate unigrams\n",
    "        out1 = self.conv_layer1(emb)\n",
    "        out1 = self.relu(out1)\n",
    "        out1 = out1.squeeze(3)\n",
    "        \n",
    "        out2  = self.conv_layer2(emb)\n",
    "        out2  = self.relu(out2)\n",
    "        out2  = out2.squeeze(3)\n",
    "                \n",
    "        # global max pool\n",
    "        out1, _ = torch.max(out1, dim=-1)\n",
    "        out2, _ = torch.max(out2, dim=-1)\n",
    "        \n",
    "        out     = torch.cat((out1, out2), dim=1)\n",
    "        \n",
    "        # fully connected layer\n",
    "        out     = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model     = TConvolution(len(token_to_id), hidden_dim=32, num_classes=7, PAD_IX=PAD_IX).cuda()\n",
    "criterion = nn.BCEWithLogitsLoss().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on a single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X      = torch.cuda.LongTensor(X)\n",
    "y      = torch.cuda.FloatTensor(y)\n",
    "\n",
    "logits = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1132473945617676"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = criterion(logits, y)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Evaluation:\n",
    "\n",
    "Mean columnwise auc score: In other words, the score is the average of the individual AUCs of each predicted column.\n",
    "\n",
    "example:\n",
    "  \n",
    "TRUE: \n",
    "  a  b  c\n",
    "[[1, 0, 1],\n",
    " [0, 1, 0],\n",
    " [0, 0, 1],\n",
    " [0, 1, 1]\n",
    "]\n",
    "\n",
    "PREDS:\n",
    "  a    b    c\n",
    "[[0.3, 0.6, 0.1],\n",
    " [0.1, 0.1, 0.8],\n",
    " [0.2, 0.2, 0.6]\n",
    "]\n",
    "\n",
    "AUC score for column (a) : auc_a = roc_auc_score(true_a, preds_a)\n",
    "AUC score for column (b) : auc_b = roc_auc_score(true_b, preds_b)\n",
    "AUC score for column (c) : auc_c = roc_auc_score(true_c, preds_c)\n",
    "\n",
    "Mean score = (auc_a + auc_b + auc_c) / 3\n",
    "\n",
    "\n",
    "Required:\n",
    "\n",
    "true matrix = one hot encoded data frame of target labels\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, criterion, data, batch_size, optimizer=None):\n",
    "    epoch_loss, total_size = 0, 0\n",
    "    per_label_preds = [[], [], [], [], [], [], []]\n",
    "    per_label_true  = [[], [], [], [], [], [], []]\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    model.train(is_train)\n",
    "    \n",
    "    data, labels = data\n",
    "    batchs_count = math.ceil(data.shape[0] / batch_size)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        for i, (X_batch, y_batch) in enumerate(iterate_batches(data, labels, batch_size)):\n",
    "            X_batch, y_batch = torch.cuda.LongTensor(X_batch), torch.cuda.FloatTensor(y_batch)\n",
    "\n",
    "            logits = model(X_batch)\n",
    "            loss   = criterion(logits, y_batch)\n",
    "            \n",
    "            if is_train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            # convert true target\n",
    "            batch_target = y_batch.cpu().detach().numpy()\n",
    "            logits_cpu   = logits.cpu().detach().numpy()\n",
    "            \n",
    "            # per_label_preds\n",
    "            for j in range(7):\n",
    "                label_preds     = logits_cpu[:, j]\n",
    "                per_label_preds[j].extend(label_preds)\n",
    "                per_label_true[j].extend(batch_target[:, j])\n",
    "                            \n",
    "            # calculate log loss\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            print('\\r[{} / {}]: Loss = {:.4f}'.format(\n",
    "                  i, batchs_count, loss.item(), end=''))\n",
    "    \n",
    "    label_auc = []\n",
    "    \n",
    "    for i in range(7):\n",
    "        label_auc.append(roc_auc_score(per_label_true[i], per_label_preds[i]))\n",
    "    \n",
    "    return epoch_loss / batchs_count, np.mean(label_auc)\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, \n",
    "        batch_size=32, val_data=None, val_batch_size=None):\n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        start_time = time.time()\n",
    "        train_loss, train_auc = do_epoch(\n",
    "            model, criterion, train_data, batch_size, optimizer\n",
    "        )\n",
    "        \n",
    "        output_info = '\\rEpoch {} / {}, Epoch Time = {:.2f}s: Train Loss = {:.4f}, Train AUC = {:.4f}'\n",
    "        if not val_data is None:\n",
    "            val_loss, val_auc   = do_epoch(model, criterion, val_data, val_batch_size, None)\n",
    "            \n",
    "            epoch_time   = time.time() - start_time\n",
    "            output_info += ', Val Loss = {:.4f}, Val AUC = {:.4f}'\n",
    "            print(output_info.format(epoch+1, epochs_count, epoch_time, \n",
    "                                     train_loss,\n",
    "                                     train_auc,\n",
    "                                     val_loss,\n",
    "                                     val_auc\n",
    "                                    ))\n",
    "        else:\n",
    "            epoch_time = time.time() - start_time\n",
    "            print(output_info.format(epoch+1, epochs_count, epoch_time, train_loss, train_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 / 250]: Loss = 0.9652\n",
      "[1 / 250]: Loss = 0.9023\n",
      "[2 / 250]: Loss = 0.8616\n",
      "[3 / 250]: Loss = 0.8374\n",
      "[4 / 250]: Loss = 0.8124\n",
      "[5 / 250]: Loss = 0.7890\n",
      "[6 / 250]: Loss = 0.7742\n",
      "[7 / 250]: Loss = 0.7613\n",
      "[8 / 250]: Loss = 0.7493\n",
      "[9 / 250]: Loss = 0.7399\n",
      "[10 / 250]: Loss = 0.7277\n",
      "[11 / 250]: Loss = 0.7209\n",
      "[12 / 250]: Loss = 0.7138\n",
      "[13 / 250]: Loss = 0.7092\n",
      "[14 / 250]: Loss = 0.7005\n",
      "[15 / 250]: Loss = 0.6939\n",
      "[16 / 250]: Loss = 0.6871\n",
      "[17 / 250]: Loss = 0.6789\n",
      "[18 / 250]: Loss = 0.6728\n",
      "[19 / 250]: Loss = 0.6651\n",
      "[20 / 250]: Loss = 0.6565\n",
      "[21 / 250]: Loss = 0.6488\n",
      "[22 / 250]: Loss = 0.6372\n",
      "[23 / 250]: Loss = 0.6259\n",
      "[24 / 250]: Loss = 0.6150\n",
      "[25 / 250]: Loss = 0.6047\n",
      "[26 / 250]: Loss = 0.5897\n",
      "[27 / 250]: Loss = 0.5767\n",
      "[28 / 250]: Loss = 0.5618\n",
      "[29 / 250]: Loss = 0.5485\n",
      "[30 / 250]: Loss = 0.5349\n",
      "[31 / 250]: Loss = 0.5149\n",
      "[32 / 250]: Loss = 0.5007\n",
      "[33 / 250]: Loss = 0.4847\n",
      "[34 / 250]: Loss = 0.4664\n",
      "[35 / 250]: Loss = 0.4524\n",
      "[36 / 250]: Loss = 0.4324\n",
      "[37 / 250]: Loss = 0.4208\n",
      "[38 / 250]: Loss = 0.3934\n",
      "[39 / 250]: Loss = 0.3877\n",
      "[40 / 250]: Loss = 0.3563\n",
      "[41 / 250]: Loss = 0.3580\n",
      "[42 / 250]: Loss = 0.3228\n",
      "[43 / 250]: Loss = 0.3054\n",
      "[44 / 250]: Loss = 0.2974\n",
      "[45 / 250]: Loss = 0.2788\n",
      "[46 / 250]: Loss = 0.2838\n",
      "[47 / 250]: Loss = 0.2464\n",
      "[48 / 250]: Loss = 0.2394\n",
      "[49 / 250]: Loss = 0.2590\n",
      "[50 / 250]: Loss = 0.2449\n",
      "[51 / 250]: Loss = 0.2259\n",
      "[52 / 250]: Loss = 0.2390\n",
      "[53 / 250]: Loss = 0.2364\n",
      "[54 / 250]: Loss = 0.1808\n",
      "[55 / 250]: Loss = 0.2097\n",
      "[56 / 250]: Loss = 0.2157\n",
      "[57 / 250]: Loss = 0.1903\n",
      "[58 / 250]: Loss = 0.1939\n",
      "[59 / 250]: Loss = 0.1590\n",
      "[60 / 250]: Loss = 0.1860\n",
      "[61 / 250]: Loss = 0.1697\n",
      "[62 / 250]: Loss = 0.1981\n",
      "[63 / 250]: Loss = 0.1853\n",
      "[64 / 250]: Loss = 0.2043\n",
      "[65 / 250]: Loss = 0.1805\n",
      "[66 / 250]: Loss = 0.1522\n",
      "[67 / 250]: Loss = 0.1589\n",
      "[68 / 250]: Loss = 0.1799\n",
      "[69 / 250]: Loss = 0.1656\n",
      "[70 / 250]: Loss = 0.1850\n",
      "[71 / 250]: Loss = 0.1466\n",
      "[72 / 250]: Loss = 0.1559\n",
      "[73 / 250]: Loss = 0.1581\n",
      "[74 / 250]: Loss = 0.1804\n",
      "[75 / 250]: Loss = 0.1812\n",
      "[76 / 250]: Loss = 0.1824\n",
      "[77 / 250]: Loss = 0.1514\n",
      "[78 / 250]: Loss = 0.1554\n",
      "[79 / 250]: Loss = 0.1858\n",
      "[80 / 250]: Loss = 0.1300\n",
      "[81 / 250]: Loss = 0.1445\n",
      "[82 / 250]: Loss = 0.1581\n",
      "[83 / 250]: Loss = 0.1570\n",
      "[84 / 250]: Loss = 0.1664\n",
      "[85 / 250]: Loss = 0.1598\n",
      "[86 / 250]: Loss = 0.1293\n",
      "[87 / 250]: Loss = 0.1682\n",
      "[88 / 250]: Loss = 0.1594\n",
      "[89 / 250]: Loss = 0.1773\n",
      "[90 / 250]: Loss = 0.1506\n",
      "[91 / 250]: Loss = 0.1527\n",
      "[92 / 250]: Loss = 0.1604\n",
      "[93 / 250]: Loss = 0.1498\n",
      "[94 / 250]: Loss = 0.1500\n",
      "[95 / 250]: Loss = 0.1595\n",
      "[96 / 250]: Loss = 0.1268\n",
      "[97 / 250]: Loss = 0.1464\n",
      "[98 / 250]: Loss = 0.1259\n",
      "[99 / 250]: Loss = 0.1215\n",
      "[100 / 250]: Loss = 0.1391\n",
      "[101 / 250]: Loss = 0.1308\n",
      "[102 / 250]: Loss = 0.1307\n",
      "[103 / 250]: Loss = 0.1272\n",
      "[104 / 250]: Loss = 0.1584\n",
      "[105 / 250]: Loss = 0.1326\n",
      "[106 / 250]: Loss = 0.1672\n",
      "[107 / 250]: Loss = 0.1487\n",
      "[108 / 250]: Loss = 0.1479\n",
      "[109 / 250]: Loss = 0.1940\n",
      "[110 / 250]: Loss = 0.2046\n",
      "[111 / 250]: Loss = 0.1627\n",
      "[112 / 250]: Loss = 0.1773\n",
      "[113 / 250]: Loss = 0.1686\n",
      "[114 / 250]: Loss = 0.1577\n",
      "[115 / 250]: Loss = 0.1538\n",
      "[116 / 250]: Loss = 0.1735\n",
      "[117 / 250]: Loss = 0.1363\n",
      "[118 / 250]: Loss = 0.1338\n",
      "[119 / 250]: Loss = 0.1614\n",
      "[120 / 250]: Loss = 0.1278\n",
      "[121 / 250]: Loss = 0.1641\n",
      "[122 / 250]: Loss = 0.1712\n",
      "[123 / 250]: Loss = 0.1551\n",
      "[124 / 250]: Loss = 0.1626\n",
      "[125 / 250]: Loss = 0.1156\n",
      "[126 / 250]: Loss = 0.1478\n",
      "[127 / 250]: Loss = 0.1459\n",
      "[128 / 250]: Loss = 0.1600\n",
      "[129 / 250]: Loss = 0.1471\n",
      "[130 / 250]: Loss = 0.1535\n",
      "[131 / 250]: Loss = 0.1338\n",
      "[132 / 250]: Loss = 0.1228\n",
      "[133 / 250]: Loss = 0.1293\n",
      "[134 / 250]: Loss = 0.1563\n",
      "[135 / 250]: Loss = 0.1759\n",
      "[136 / 250]: Loss = 0.1581\n",
      "[137 / 250]: Loss = 0.1449\n",
      "[138 / 250]: Loss = 0.1559\n",
      "[139 / 250]: Loss = 0.1666\n",
      "[140 / 250]: Loss = 0.1566\n",
      "[141 / 250]: Loss = 0.1466\n",
      "[142 / 250]: Loss = 0.1528\n",
      "[143 / 250]: Loss = 0.1235\n",
      "[144 / 250]: Loss = 0.1845\n",
      "[145 / 250]: Loss = 0.1710\n",
      "[146 / 250]: Loss = 0.1564\n",
      "[147 / 250]: Loss = 0.1367\n",
      "[148 / 250]: Loss = 0.1375\n",
      "[149 / 250]: Loss = 0.1267\n",
      "[150 / 250]: Loss = 0.1302\n",
      "[151 / 250]: Loss = 0.1284\n",
      "[152 / 250]: Loss = 0.1637\n",
      "[153 / 250]: Loss = 0.1479\n",
      "[154 / 250]: Loss = 0.1548\n",
      "[155 / 250]: Loss = 0.1539\n",
      "[156 / 250]: Loss = 0.1343\n",
      "[157 / 250]: Loss = 0.1468\n",
      "[158 / 250]: Loss = 0.1428\n",
      "[159 / 250]: Loss = 0.1192\n",
      "[160 / 250]: Loss = 0.1592\n",
      "[161 / 250]: Loss = 0.1276\n",
      "[162 / 250]: Loss = 0.1397\n",
      "[163 / 250]: Loss = 0.1534\n",
      "[164 / 250]: Loss = 0.1367\n",
      "[165 / 250]: Loss = 0.1456\n",
      "[166 / 250]: Loss = 0.1576\n",
      "[167 / 250]: Loss = 0.1329\n",
      "[168 / 250]: Loss = 0.1531\n",
      "[169 / 250]: Loss = 0.1427\n",
      "[170 / 250]: Loss = 0.1633\n",
      "[171 / 250]: Loss = 0.1078\n",
      "[172 / 250]: Loss = 0.1600\n",
      "[173 / 250]: Loss = 0.1512\n",
      "[174 / 250]: Loss = 0.1417\n",
      "[175 / 250]: Loss = 0.1487\n",
      "[176 / 250]: Loss = 0.1242\n",
      "[177 / 250]: Loss = 0.1269\n",
      "[178 / 250]: Loss = 0.1498\n",
      "[179 / 250]: Loss = 0.1484\n",
      "[180 / 250]: Loss = 0.1245\n",
      "[181 / 250]: Loss = 0.1475\n",
      "[182 / 250]: Loss = 0.1292\n",
      "[183 / 250]: Loss = 0.1377\n",
      "[184 / 250]: Loss = 0.1597\n",
      "[185 / 250]: Loss = 0.1479\n",
      "[186 / 250]: Loss = 0.1039\n",
      "[187 / 250]: Loss = 0.1243\n",
      "[188 / 250]: Loss = 0.1360\n",
      "[189 / 250]: Loss = 0.1296\n",
      "[190 / 250]: Loss = 0.1386\n",
      "[191 / 250]: Loss = 0.1388\n",
      "[192 / 250]: Loss = 0.1330\n",
      "[193 / 250]: Loss = 0.1051\n",
      "[194 / 250]: Loss = 0.1500\n",
      "[195 / 250]: Loss = 0.1451\n",
      "[196 / 250]: Loss = 0.1572\n",
      "[197 / 250]: Loss = 0.1473\n",
      "[198 / 250]: Loss = 0.1428\n",
      "[199 / 250]: Loss = 0.1025\n",
      "[200 / 250]: Loss = 0.1351\n",
      "[201 / 250]: Loss = 0.1920\n",
      "[202 / 250]: Loss = 0.1530\n",
      "[203 / 250]: Loss = 0.1307\n",
      "[204 / 250]: Loss = 0.1319\n",
      "[205 / 250]: Loss = 0.1330\n",
      "[206 / 250]: Loss = 0.1333\n",
      "[207 / 250]: Loss = 0.1571\n",
      "[208 / 250]: Loss = 0.1283\n",
      "[209 / 250]: Loss = 0.1236\n",
      "[210 / 250]: Loss = 0.1364\n",
      "[211 / 250]: Loss = 0.1267\n",
      "[212 / 250]: Loss = 0.1495\n",
      "[213 / 250]: Loss = 0.1375\n",
      "[214 / 250]: Loss = 0.1564\n",
      "[215 / 250]: Loss = 0.1603\n",
      "[216 / 250]: Loss = 0.1450\n",
      "[217 / 250]: Loss = 0.1337\n",
      "[218 / 250]: Loss = 0.1259\n",
      "[219 / 250]: Loss = 0.1401\n",
      "[220 / 250]: Loss = 0.1547\n",
      "[221 / 250]: Loss = 0.1598\n",
      "[222 / 250]: Loss = 0.1769\n",
      "[223 / 250]: Loss = 0.1382\n",
      "[224 / 250]: Loss = 0.1372\n",
      "[225 / 250]: Loss = 0.1109\n",
      "[226 / 250]: Loss = 0.1414\n",
      "[227 / 250]: Loss = 0.1454\n",
      "[228 / 250]: Loss = 0.1242\n",
      "[229 / 250]: Loss = 0.1529\n",
      "[230 / 250]: Loss = 0.1362\n",
      "[231 / 250]: Loss = 0.1284\n",
      "[232 / 250]: Loss = 0.1351\n",
      "[233 / 250]: Loss = 0.1121\n",
      "[234 / 250]: Loss = 0.1389\n",
      "[235 / 250]: Loss = 0.1081\n",
      "[236 / 250]: Loss = 0.1464\n",
      "[237 / 250]: Loss = 0.1428\n",
      "[238 / 250]: Loss = 0.1409\n",
      "[239 / 250]: Loss = 0.1330\n",
      "[240 / 250]: Loss = 0.1367\n",
      "[241 / 250]: Loss = 0.1471\n",
      "[242 / 250]: Loss = 0.1669\n",
      "[243 / 250]: Loss = 0.1575\n",
      "[244 / 250]: Loss = 0.1794\n",
      "[245 / 250]: Loss = 0.1473\n",
      "[246 / 250]: Loss = 0.1649\n",
      "[247 / 250]: Loss = 0.1730\n",
      "[248 / 250]: Loss = 0.1283\n",
      "[249 / 250]: Loss = 0.0960\n",
      "[0 / 32]: Loss = 0.1500\n",
      "[1 / 32]: Loss = 0.1455\n",
      "[2 / 32]: Loss = 0.1584\n",
      "[3 / 32]: Loss = 0.1521\n",
      "[4 / 32]: Loss = 0.1323\n",
      "[5 / 32]: Loss = 0.1656\n",
      "[6 / 32]: Loss = 0.1369\n",
      "[7 / 32]: Loss = 0.1320\n",
      "[8 / 32]: Loss = 0.1430\n",
      "[9 / 32]: Loss = 0.1375\n",
      "[10 / 32]: Loss = 0.1386\n",
      "[11 / 32]: Loss = 0.1405\n",
      "[12 / 32]: Loss = 0.1423\n",
      "[13 / 32]: Loss = 0.1387\n",
      "[14 / 32]: Loss = 0.1275\n",
      "[15 / 32]: Loss = 0.1327\n",
      "[16 / 32]: Loss = 0.1520\n",
      "[17 / 32]: Loss = 0.1441\n",
      "[18 / 32]: Loss = 0.1225\n",
      "[19 / 32]: Loss = 0.1349\n",
      "[20 / 32]: Loss = 0.1674\n",
      "[21 / 32]: Loss = 0.1476\n",
      "[22 / 32]: Loss = 0.1500\n",
      "[23 / 32]: Loss = 0.1405\n",
      "[24 / 32]: Loss = 0.1505\n",
      "[25 / 32]: Loss = 0.1366\n",
      "[26 / 32]: Loss = 0.1363\n",
      "[27 / 32]: Loss = 0.1466\n",
      "[28 / 32]: Loss = 0.1446\n",
      "[29 / 32]: Loss = 0.1320\n",
      "[30 / 32]: Loss = 0.1501\n",
      "[31 / 32]: Loss = 0.1255\n",
      "Epoch 1 / 5, Epoch Time = 258.19s: Train Loss = 0.2360, Train AUC = 0.6866, Val Loss = 0.1423, Val AUC = 0.8201\n",
      "[0 / 250]: Loss = 0.1416\n",
      "[1 / 250]: Loss = 0.1253\n",
      "[2 / 250]: Loss = 0.1380\n",
      "[3 / 250]: Loss = 0.1359\n",
      "[4 / 250]: Loss = 0.1365\n",
      "[5 / 250]: Loss = 0.1289\n",
      "[6 / 250]: Loss = 0.1301\n",
      "[7 / 250]: Loss = 0.1374\n",
      "[8 / 250]: Loss = 0.1543\n",
      "[9 / 250]: Loss = 0.1124\n",
      "[10 / 250]: Loss = 0.1332\n",
      "[11 / 250]: Loss = 0.1228\n",
      "[12 / 250]: Loss = 0.1436\n",
      "[13 / 250]: Loss = 0.1673\n",
      "[14 / 250]: Loss = 0.1512\n",
      "[15 / 250]: Loss = 0.1612\n",
      "[16 / 250]: Loss = 0.1461\n",
      "[17 / 250]: Loss = 0.1430\n",
      "[18 / 250]: Loss = 0.1204\n",
      "[19 / 250]: Loss = 0.1519\n",
      "[20 / 250]: Loss = 0.1422\n",
      "[21 / 250]: Loss = 0.1604\n",
      "[22 / 250]: Loss = 0.1350\n",
      "[23 / 250]: Loss = 0.1229\n",
      "[24 / 250]: Loss = 0.1468\n",
      "[25 / 250]: Loss = 0.1540\n",
      "[26 / 250]: Loss = 0.1439\n",
      "[27 / 250]: Loss = 0.1369\n",
      "[28 / 250]: Loss = 0.1325\n",
      "[29 / 250]: Loss = 0.1409\n",
      "[30 / 250]: Loss = 0.1496\n",
      "[31 / 250]: Loss = 0.1312\n",
      "[32 / 250]: Loss = 0.1558\n",
      "[33 / 250]: Loss = 0.1566\n",
      "[34 / 250]: Loss = 0.1400\n",
      "[35 / 250]: Loss = 0.1150\n",
      "[36 / 250]: Loss = 0.1550\n",
      "[37 / 250]: Loss = 0.1517\n",
      "[38 / 250]: Loss = 0.1359\n",
      "[39 / 250]: Loss = 0.1285\n",
      "[40 / 250]: Loss = 0.1659\n",
      "[41 / 250]: Loss = 0.1547\n",
      "[42 / 250]: Loss = 0.1338\n",
      "[43 / 250]: Loss = 0.1315\n",
      "[44 / 250]: Loss = 0.1284\n",
      "[45 / 250]: Loss = 0.1285\n",
      "[46 / 250]: Loss = 0.1443\n",
      "[47 / 250]: Loss = 0.1236\n",
      "[48 / 250]: Loss = 0.1749\n",
      "[49 / 250]: Loss = 0.1518\n",
      "[50 / 250]: Loss = 0.1409\n",
      "[51 / 250]: Loss = 0.1673\n",
      "[52 / 250]: Loss = 0.1400\n",
      "[53 / 250]: Loss = 0.1267\n",
      "[54 / 250]: Loss = 0.1361\n",
      "[55 / 250]: Loss = 0.1459\n",
      "[56 / 250]: Loss = 0.1234\n",
      "[57 / 250]: Loss = 0.1396\n",
      "[58 / 250]: Loss = 0.1139\n",
      "[59 / 250]: Loss = 0.1428\n",
      "[60 / 250]: Loss = 0.1195\n",
      "[61 / 250]: Loss = 0.1322\n",
      "[62 / 250]: Loss = 0.1259\n",
      "[63 / 250]: Loss = 0.1387\n",
      "[64 / 250]: Loss = 0.1317\n",
      "[65 / 250]: Loss = 0.1294\n",
      "[66 / 250]: Loss = 0.1463\n",
      "[67 / 250]: Loss = 0.1375\n",
      "[68 / 250]: Loss = 0.1414\n",
      "[69 / 250]: Loss = 0.1400\n",
      "[70 / 250]: Loss = 0.1489\n",
      "[71 / 250]: Loss = 0.1074\n",
      "[72 / 250]: Loss = 0.1268\n",
      "[73 / 250]: Loss = 0.1189\n",
      "[74 / 250]: Loss = 0.1527\n",
      "[75 / 250]: Loss = 0.1377\n",
      "[76 / 250]: Loss = 0.1397\n",
      "[77 / 250]: Loss = 0.1427\n",
      "[78 / 250]: Loss = 0.1507\n",
      "[79 / 250]: Loss = 0.1393\n",
      "[80 / 250]: Loss = 0.1469\n",
      "[81 / 250]: Loss = 0.1251\n",
      "[82 / 250]: Loss = 0.1329\n",
      "[83 / 250]: Loss = 0.1458\n",
      "[84 / 250]: Loss = 0.1022\n",
      "[85 / 250]: Loss = 0.1394\n",
      "[86 / 250]: Loss = 0.1145\n",
      "[87 / 250]: Loss = 0.0987\n",
      "[88 / 250]: Loss = 0.1440\n",
      "[89 / 250]: Loss = 0.1292\n",
      "[90 / 250]: Loss = 0.1559\n",
      "[91 / 250]: Loss = 0.1463\n",
      "[92 / 250]: Loss = 0.1050\n",
      "[93 / 250]: Loss = 0.1470\n",
      "[94 / 250]: Loss = 0.1267\n",
      "[95 / 250]: Loss = 0.1110\n",
      "[96 / 250]: Loss = 0.1310\n",
      "[97 / 250]: Loss = 0.1401\n",
      "[98 / 250]: Loss = 0.1268\n",
      "[99 / 250]: Loss = 0.1408\n",
      "[100 / 250]: Loss = 0.1425\n",
      "[101 / 250]: Loss = 0.1381\n",
      "[102 / 250]: Loss = 0.1464\n",
      "[103 / 250]: Loss = 0.1508\n",
      "[104 / 250]: Loss = 0.1530\n",
      "[105 / 250]: Loss = 0.1340\n",
      "[106 / 250]: Loss = 0.1330\n",
      "[107 / 250]: Loss = 0.1432\n",
      "[108 / 250]: Loss = 0.1414\n",
      "[109 / 250]: Loss = 0.1243\n",
      "[110 / 250]: Loss = 0.1534\n",
      "[111 / 250]: Loss = 0.1282\n",
      "[112 / 250]: Loss = 0.1217\n",
      "[113 / 250]: Loss = 0.1147\n",
      "[114 / 250]: Loss = 0.1500\n",
      "[115 / 250]: Loss = 0.1157\n",
      "[116 / 250]: Loss = 0.1474\n",
      "[117 / 250]: Loss = 0.1314\n",
      "[118 / 250]: Loss = 0.1168\n",
      "[119 / 250]: Loss = 0.1193\n",
      "[120 / 250]: Loss = 0.1340\n",
      "[121 / 250]: Loss = 0.1387\n",
      "[122 / 250]: Loss = 0.1223\n",
      "[123 / 250]: Loss = 0.1567\n",
      "[124 / 250]: Loss = 0.1378\n",
      "[125 / 250]: Loss = 0.1298\n",
      "[126 / 250]: Loss = 0.1273\n",
      "[127 / 250]: Loss = 0.1289\n",
      "[128 / 250]: Loss = 0.1282\n",
      "[129 / 250]: Loss = 0.1717\n",
      "[130 / 250]: Loss = 0.1243\n",
      "[131 / 250]: Loss = 0.1271\n",
      "[132 / 250]: Loss = 0.1290\n",
      "[133 / 250]: Loss = 0.1513\n",
      "[134 / 250]: Loss = 0.1482\n",
      "[135 / 250]: Loss = 0.1188\n",
      "[136 / 250]: Loss = 0.1253\n",
      "[137 / 250]: Loss = 0.1224\n",
      "[138 / 250]: Loss = 0.1469\n",
      "[139 / 250]: Loss = 0.1149\n",
      "[140 / 250]: Loss = 0.1310\n",
      "[141 / 250]: Loss = 0.1483\n",
      "[142 / 250]: Loss = 0.1089\n",
      "[143 / 250]: Loss = 0.1165\n",
      "[144 / 250]: Loss = 0.1605\n",
      "[145 / 250]: Loss = 0.1435\n",
      "[146 / 250]: Loss = 0.1283\n",
      "[147 / 250]: Loss = 0.1303\n",
      "[148 / 250]: Loss = 0.1253\n",
      "[149 / 250]: Loss = 0.1278\n",
      "[150 / 250]: Loss = 0.1316\n",
      "[151 / 250]: Loss = 0.1509\n",
      "[152 / 250]: Loss = 0.1533\n",
      "[153 / 250]: Loss = 0.1338\n",
      "[154 / 250]: Loss = 0.1109\n",
      "[155 / 250]: Loss = 0.1288\n",
      "[156 / 250]: Loss = 0.1230\n",
      "[157 / 250]: Loss = 0.1042\n",
      "[158 / 250]: Loss = 0.1264\n",
      "[159 / 250]: Loss = 0.1117\n",
      "[160 / 250]: Loss = 0.1669\n",
      "[161 / 250]: Loss = 0.1189\n",
      "[162 / 250]: Loss = 0.1591\n",
      "[163 / 250]: Loss = 0.1543\n",
      "[164 / 250]: Loss = 0.1296\n",
      "[165 / 250]: Loss = 0.1191\n",
      "[166 / 250]: Loss = 0.1383\n",
      "[167 / 250]: Loss = 0.1359\n",
      "[168 / 250]: Loss = 0.1262\n",
      "[169 / 250]: Loss = 0.1357\n",
      "[170 / 250]: Loss = 0.1340\n",
      "[171 / 250]: Loss = 0.1597\n",
      "[172 / 250]: Loss = 0.1557\n",
      "[173 / 250]: Loss = 0.1333\n",
      "[174 / 250]: Loss = 0.1209\n",
      "[175 / 250]: Loss = 0.1112\n",
      "[176 / 250]: Loss = 0.1515\n",
      "[177 / 250]: Loss = 0.1270\n",
      "[178 / 250]: Loss = 0.1485\n",
      "[179 / 250]: Loss = 0.1483\n",
      "[180 / 250]: Loss = 0.1395\n",
      "[181 / 250]: Loss = 0.1658\n",
      "[182 / 250]: Loss = 0.1215\n",
      "[183 / 250]: Loss = 0.1415\n",
      "[184 / 250]: Loss = 0.1657\n",
      "[185 / 250]: Loss = 0.1398\n",
      "[186 / 250]: Loss = 0.1256\n",
      "[187 / 250]: Loss = 0.1253\n",
      "[188 / 250]: Loss = 0.1254\n",
      "[189 / 250]: Loss = 0.1337\n",
      "[190 / 250]: Loss = 0.1635\n",
      "[191 / 250]: Loss = 0.1131\n",
      "[192 / 250]: Loss = 0.1221\n",
      "[193 / 250]: Loss = 0.1367\n",
      "[194 / 250]: Loss = 0.1559\n",
      "[195 / 250]: Loss = 0.1445\n",
      "[196 / 250]: Loss = 0.1293\n",
      "[197 / 250]: Loss = 0.1186\n",
      "[198 / 250]: Loss = 0.1549\n",
      "[199 / 250]: Loss = 0.1404\n",
      "[200 / 250]: Loss = 0.1502\n",
      "[201 / 250]: Loss = 0.1328\n",
      "[202 / 250]: Loss = 0.1140\n",
      "[203 / 250]: Loss = 0.1346\n",
      "[204 / 250]: Loss = 0.1387\n",
      "[205 / 250]: Loss = 0.1227\n",
      "[206 / 250]: Loss = 0.1155\n",
      "[207 / 250]: Loss = 0.1261\n",
      "[208 / 250]: Loss = 0.1223\n",
      "[209 / 250]: Loss = 0.1486\n",
      "[210 / 250]: Loss = 0.1285\n",
      "[211 / 250]: Loss = 0.1506\n",
      "[212 / 250]: Loss = 0.1519\n",
      "[213 / 250]: Loss = 0.1222\n",
      "[214 / 250]: Loss = 0.1282\n",
      "[215 / 250]: Loss = 0.1277\n",
      "[216 / 250]: Loss = 0.1269\n",
      "[217 / 250]: Loss = 0.1453\n",
      "[218 / 250]: Loss = 0.1302\n",
      "[219 / 250]: Loss = 0.1301\n",
      "[220 / 250]: Loss = 0.1280\n",
      "[221 / 250]: Loss = 0.1317\n",
      "[222 / 250]: Loss = 0.1507\n",
      "[223 / 250]: Loss = 0.1328\n",
      "[224 / 250]: Loss = 0.1603\n",
      "[225 / 250]: Loss = 0.1388\n",
      "[226 / 250]: Loss = 0.1215\n",
      "[227 / 250]: Loss = 0.1283\n",
      "[228 / 250]: Loss = 0.1270\n",
      "[229 / 250]: Loss = 0.1178\n",
      "[230 / 250]: Loss = 0.1361\n",
      "[231 / 250]: Loss = 0.1284\n",
      "[232 / 250]: Loss = 0.1344\n",
      "[233 / 250]: Loss = 0.1236\n",
      "[234 / 250]: Loss = 0.1414\n",
      "[235 / 250]: Loss = 0.1360\n",
      "[236 / 250]: Loss = 0.1099\n",
      "[237 / 250]: Loss = 0.1320\n",
      "[238 / 250]: Loss = 0.1131\n",
      "[239 / 250]: Loss = 0.1694\n",
      "[240 / 250]: Loss = 0.1169\n",
      "[241 / 250]: Loss = 0.1348\n",
      "[242 / 250]: Loss = 0.1571\n",
      "[243 / 250]: Loss = 0.1502\n",
      "[244 / 250]: Loss = 0.1333\n",
      "[245 / 250]: Loss = 0.1406\n",
      "[246 / 250]: Loss = 0.1433\n",
      "[247 / 250]: Loss = 0.1204\n",
      "[248 / 250]: Loss = 0.1257\n",
      "[249 / 250]: Loss = 0.1483\n",
      "[0 / 32]: Loss = 0.1446\n",
      "[1 / 32]: Loss = 0.1275\n",
      "[2 / 32]: Loss = 0.1346\n",
      "[3 / 32]: Loss = 0.1425\n",
      "[4 / 32]: Loss = 0.1662\n",
      "[5 / 32]: Loss = 0.1572\n",
      "[6 / 32]: Loss = 0.1455\n",
      "[7 / 32]: Loss = 0.1317\n",
      "[8 / 32]: Loss = 0.1410\n",
      "[9 / 32]: Loss = 0.1304\n",
      "[10 / 32]: Loss = 0.1375\n",
      "[11 / 32]: Loss = 0.1409\n",
      "[12 / 32]: Loss = 0.1358\n",
      "[13 / 32]: Loss = 0.1443\n",
      "[14 / 32]: Loss = 0.1370\n",
      "[15 / 32]: Loss = 0.1138\n",
      "[16 / 32]: Loss = 0.1232\n",
      "[17 / 32]: Loss = 0.1490\n",
      "[18 / 32]: Loss = 0.1222\n",
      "[19 / 32]: Loss = 0.1431\n",
      "[20 / 32]: Loss = 0.1366\n",
      "[21 / 32]: Loss = 0.1239\n",
      "[22 / 32]: Loss = 0.1459\n",
      "[23 / 32]: Loss = 0.1238\n",
      "[24 / 32]: Loss = 0.1527\n",
      "[25 / 32]: Loss = 0.1317\n",
      "[26 / 32]: Loss = 0.1338\n",
      "[27 / 32]: Loss = 0.1506\n",
      "[28 / 32]: Loss = 0.1231\n",
      "[29 / 32]: Loss = 0.1208\n",
      "[30 / 32]: Loss = 0.1391\n",
      "[31 / 32]: Loss = 0.1339\n",
      "Epoch 2 / 5, Epoch Time = 258.09s: Train Loss = 0.1358, Train AUC = 0.8405, Val Loss = 0.1370, Val AUC = 0.8402\n",
      "[0 / 250]: Loss = 0.1219\n",
      "[1 / 250]: Loss = 0.1318\n",
      "[2 / 250]: Loss = 0.1323\n",
      "[3 / 250]: Loss = 0.1114\n",
      "[4 / 250]: Loss = 0.1354\n",
      "[5 / 250]: Loss = 0.1415\n",
      "[6 / 250]: Loss = 0.1197\n",
      "[7 / 250]: Loss = 0.1105\n",
      "[8 / 250]: Loss = 0.1337\n",
      "[9 / 250]: Loss = 0.1401\n",
      "[10 / 250]: Loss = 0.1379\n",
      "[11 / 250]: Loss = 0.1505\n",
      "[12 / 250]: Loss = 0.1459\n",
      "[13 / 250]: Loss = 0.1152\n",
      "[14 / 250]: Loss = 0.1281\n",
      "[15 / 250]: Loss = 0.1296\n",
      "[16 / 250]: Loss = 0.1005\n",
      "[17 / 250]: Loss = 0.1242\n",
      "[18 / 250]: Loss = 0.1241\n",
      "[19 / 250]: Loss = 0.1462\n",
      "[20 / 250]: Loss = 0.1431\n",
      "[21 / 250]: Loss = 0.1106\n",
      "[22 / 250]: Loss = 0.1219\n",
      "[23 / 250]: Loss = 0.1545\n",
      "[24 / 250]: Loss = 0.1123\n",
      "[25 / 250]: Loss = 0.1283\n",
      "[26 / 250]: Loss = 0.1239\n",
      "[27 / 250]: Loss = 0.1276\n",
      "[28 / 250]: Loss = 0.1277\n",
      "[29 / 250]: Loss = 0.1394\n",
      "[30 / 250]: Loss = 0.1320\n",
      "[31 / 250]: Loss = 0.1393\n",
      "[32 / 250]: Loss = 0.1202\n",
      "[33 / 250]: Loss = 0.0947\n",
      "[34 / 250]: Loss = 0.1412\n",
      "[35 / 250]: Loss = 0.1328\n",
      "[36 / 250]: Loss = 0.1479\n",
      "[37 / 250]: Loss = 0.1464\n",
      "[38 / 250]: Loss = 0.1395\n",
      "[39 / 250]: Loss = 0.1252\n",
      "[40 / 250]: Loss = 0.1367\n",
      "[41 / 250]: Loss = 0.1081\n",
      "[42 / 250]: Loss = 0.1086\n",
      "[43 / 250]: Loss = 0.1368\n",
      "[44 / 250]: Loss = 0.1392\n",
      "[45 / 250]: Loss = 0.1207\n",
      "[46 / 250]: Loss = 0.1535\n",
      "[47 / 250]: Loss = 0.1499\n",
      "[48 / 250]: Loss = 0.1348\n",
      "[49 / 250]: Loss = 0.1052\n",
      "[50 / 250]: Loss = 0.1534\n",
      "[51 / 250]: Loss = 0.1245\n",
      "[52 / 250]: Loss = 0.1170\n",
      "[53 / 250]: Loss = 0.1316\n",
      "[54 / 250]: Loss = 0.1329\n",
      "[55 / 250]: Loss = 0.1352\n",
      "[56 / 250]: Loss = 0.1271\n",
      "[57 / 250]: Loss = 0.1318\n",
      "[58 / 250]: Loss = 0.1280\n",
      "[59 / 250]: Loss = 0.1175\n",
      "[60 / 250]: Loss = 0.1082\n",
      "[61 / 250]: Loss = 0.1199\n",
      "[62 / 250]: Loss = 0.1434\n",
      "[63 / 250]: Loss = 0.1259\n",
      "[64 / 250]: Loss = 0.1138\n",
      "[65 / 250]: Loss = 0.1377\n",
      "[66 / 250]: Loss = 0.1346\n",
      "[67 / 250]: Loss = 0.1447\n",
      "[68 / 250]: Loss = 0.1476\n",
      "[69 / 250]: Loss = 0.1250\n",
      "[70 / 250]: Loss = 0.1007\n",
      "[71 / 250]: Loss = 0.1097\n",
      "[72 / 250]: Loss = 0.1385\n",
      "[73 / 250]: Loss = 0.1029\n",
      "[74 / 250]: Loss = 0.1493\n",
      "[75 / 250]: Loss = 0.1437\n",
      "[76 / 250]: Loss = 0.1179\n",
      "[77 / 250]: Loss = 0.1335\n",
      "[78 / 250]: Loss = 0.1336\n",
      "[79 / 250]: Loss = 0.1242\n",
      "[80 / 250]: Loss = 0.1361\n",
      "[81 / 250]: Loss = 0.1108\n",
      "[82 / 250]: Loss = 0.0971\n",
      "[83 / 250]: Loss = 0.1818\n",
      "[84 / 250]: Loss = 0.1729\n",
      "[85 / 250]: Loss = 0.1073\n",
      "[86 / 250]: Loss = 0.1365\n",
      "[87 / 250]: Loss = 0.1377\n",
      "[88 / 250]: Loss = 0.1417\n",
      "[89 / 250]: Loss = 0.1111\n",
      "[90 / 250]: Loss = 0.1308\n",
      "[91 / 250]: Loss = 0.1321\n",
      "[92 / 250]: Loss = 0.1379\n",
      "[93 / 250]: Loss = 0.1167\n",
      "[94 / 250]: Loss = 0.1229\n",
      "[95 / 250]: Loss = 0.1232\n",
      "[96 / 250]: Loss = 0.1220\n",
      "[97 / 250]: Loss = 0.1087\n",
      "[98 / 250]: Loss = 0.1178\n",
      "[99 / 250]: Loss = 0.1373\n",
      "[100 / 250]: Loss = 0.1481\n",
      "[101 / 250]: Loss = 0.1601\n",
      "[102 / 250]: Loss = 0.1295\n",
      "[103 / 250]: Loss = 0.1150\n",
      "[104 / 250]: Loss = 0.1428\n",
      "[105 / 250]: Loss = 0.1050\n",
      "[106 / 250]: Loss = 0.1174\n",
      "[107 / 250]: Loss = 0.1073\n",
      "[108 / 250]: Loss = 0.1557\n",
      "[109 / 250]: Loss = 0.1387\n",
      "[110 / 250]: Loss = 0.1173\n",
      "[111 / 250]: Loss = 0.1436\n",
      "[112 / 250]: Loss = 0.1066\n",
      "[113 / 250]: Loss = 0.1135\n",
      "[114 / 250]: Loss = 0.1410\n",
      "[115 / 250]: Loss = 0.1416\n",
      "[116 / 250]: Loss = 0.1002\n",
      "[117 / 250]: Loss = 0.1328\n",
      "[118 / 250]: Loss = 0.0948\n",
      "[119 / 250]: Loss = 0.1203\n",
      "[120 / 250]: Loss = 0.1091\n",
      "[121 / 250]: Loss = 0.1498\n",
      "[122 / 250]: Loss = 0.1290\n",
      "[123 / 250]: Loss = 0.1128\n",
      "[124 / 250]: Loss = 0.1519\n",
      "[125 / 250]: Loss = 0.1277\n",
      "[126 / 250]: Loss = 0.1154\n",
      "[127 / 250]: Loss = 0.1123\n",
      "[128 / 250]: Loss = 0.1466\n",
      "[129 / 250]: Loss = 0.1232\n",
      "[130 / 250]: Loss = 0.1315\n",
      "[131 / 250]: Loss = 0.1466\n",
      "[132 / 250]: Loss = 0.1073\n",
      "[133 / 250]: Loss = 0.1220\n",
      "[134 / 250]: Loss = 0.1234\n",
      "[135 / 250]: Loss = 0.1424\n",
      "[136 / 250]: Loss = 0.1159\n",
      "[137 / 250]: Loss = 0.1196\n",
      "[138 / 250]: Loss = 0.1445\n",
      "[139 / 250]: Loss = 0.1215\n",
      "[140 / 250]: Loss = 0.1249\n",
      "[141 / 250]: Loss = 0.1241\n",
      "[142 / 250]: Loss = 0.1662\n",
      "[143 / 250]: Loss = 0.1242\n",
      "[144 / 250]: Loss = 0.1327\n",
      "[145 / 250]: Loss = 0.1318\n",
      "[146 / 250]: Loss = 0.1432\n",
      "[147 / 250]: Loss = 0.1126\n",
      "[148 / 250]: Loss = 0.1152\n",
      "[149 / 250]: Loss = 0.1459\n",
      "[150 / 250]: Loss = 0.1201\n",
      "[151 / 250]: Loss = 0.1474\n",
      "[152 / 250]: Loss = 0.1089\n",
      "[153 / 250]: Loss = 0.1439\n",
      "[154 / 250]: Loss = 0.1309\n",
      "[155 / 250]: Loss = 0.1427\n",
      "[156 / 250]: Loss = 0.1207\n",
      "[157 / 250]: Loss = 0.1302\n",
      "[158 / 250]: Loss = 0.1505\n",
      "[159 / 250]: Loss = 0.1073\n",
      "[160 / 250]: Loss = 0.1325\n",
      "[161 / 250]: Loss = 0.1404\n",
      "[162 / 250]: Loss = 0.1406\n",
      "[163 / 250]: Loss = 0.1225\n",
      "[164 / 250]: Loss = 0.1166\n",
      "[165 / 250]: Loss = 0.1123\n",
      "[166 / 250]: Loss = 0.1323\n",
      "[167 / 250]: Loss = 0.1317\n",
      "[168 / 250]: Loss = 0.1113\n",
      "[169 / 250]: Loss = 0.1211\n",
      "[170 / 250]: Loss = 0.1214\n",
      "[171 / 250]: Loss = 0.1308\n",
      "[172 / 250]: Loss = 0.1218\n",
      "[173 / 250]: Loss = 0.1604\n",
      "[174 / 250]: Loss = 0.1324\n",
      "[175 / 250]: Loss = 0.1142\n",
      "[176 / 250]: Loss = 0.1125\n",
      "[177 / 250]: Loss = 0.1109\n",
      "[178 / 250]: Loss = 0.1225\n",
      "[179 / 250]: Loss = 0.1429\n",
      "[180 / 250]: Loss = 0.1526\n",
      "[181 / 250]: Loss = 0.1168\n",
      "[182 / 250]: Loss = 0.1150\n",
      "[183 / 250]: Loss = 0.1236\n",
      "[184 / 250]: Loss = 0.1477\n",
      "[185 / 250]: Loss = 0.1344\n",
      "[186 / 250]: Loss = 0.1196\n",
      "[187 / 250]: Loss = 0.1433\n",
      "[188 / 250]: Loss = 0.1293\n",
      "[189 / 250]: Loss = 0.1374\n",
      "[190 / 250]: Loss = 0.1321\n",
      "[191 / 250]: Loss = 0.1431\n",
      "[192 / 250]: Loss = 0.1252\n",
      "[193 / 250]: Loss = 0.1435\n",
      "[194 / 250]: Loss = 0.1441\n",
      "[195 / 250]: Loss = 0.1079\n",
      "[196 / 250]: Loss = 0.1250\n",
      "[197 / 250]: Loss = 0.1179\n",
      "[198 / 250]: Loss = 0.1186\n",
      "[199 / 250]: Loss = 0.1187\n",
      "[200 / 250]: Loss = 0.1160\n",
      "[201 / 250]: Loss = 0.1449\n",
      "[202 / 250]: Loss = 0.1409\n",
      "[203 / 250]: Loss = 0.1121\n",
      "[204 / 250]: Loss = 0.1401\n",
      "[205 / 250]: Loss = 0.1068\n",
      "[206 / 250]: Loss = 0.1319\n",
      "[207 / 250]: Loss = 0.1257\n",
      "[208 / 250]: Loss = 0.1117\n",
      "[209 / 250]: Loss = 0.1134\n",
      "[210 / 250]: Loss = 0.1140\n",
      "[211 / 250]: Loss = 0.1238\n",
      "[212 / 250]: Loss = 0.1581\n",
      "[213 / 250]: Loss = 0.1356\n",
      "[214 / 250]: Loss = 0.1527\n",
      "[215 / 250]: Loss = 0.1254\n",
      "[216 / 250]: Loss = 0.1437\n",
      "[217 / 250]: Loss = 0.1405\n",
      "[218 / 250]: Loss = 0.1263\n",
      "[219 / 250]: Loss = 0.1325\n",
      "[220 / 250]: Loss = 0.1222\n",
      "[221 / 250]: Loss = 0.1361\n",
      "[222 / 250]: Loss = 0.1251\n",
      "[223 / 250]: Loss = 0.1420\n",
      "[224 / 250]: Loss = 0.1257\n",
      "[225 / 250]: Loss = 0.1548\n",
      "[226 / 250]: Loss = 0.1382\n",
      "[227 / 250]: Loss = 0.1211\n",
      "[228 / 250]: Loss = 0.1005\n",
      "[229 / 250]: Loss = 0.1308\n",
      "[230 / 250]: Loss = 0.1526\n",
      "[231 / 250]: Loss = 0.1430\n",
      "[232 / 250]: Loss = 0.1305\n",
      "[233 / 250]: Loss = 0.1221\n",
      "[234 / 250]: Loss = 0.1137\n",
      "[235 / 250]: Loss = 0.1455\n",
      "[236 / 250]: Loss = 0.1419\n",
      "[237 / 250]: Loss = 0.1241\n",
      "[238 / 250]: Loss = 0.1025\n",
      "[239 / 250]: Loss = 0.1255\n",
      "[240 / 250]: Loss = 0.1066\n",
      "[241 / 250]: Loss = 0.1343\n",
      "[242 / 250]: Loss = 0.0995\n",
      "[243 / 250]: Loss = 0.1626\n",
      "[244 / 250]: Loss = 0.1140\n",
      "[245 / 250]: Loss = 0.1332\n",
      "[246 / 250]: Loss = 0.1233\n",
      "[247 / 250]: Loss = 0.1195\n",
      "[248 / 250]: Loss = 0.1250\n",
      "[249 / 250]: Loss = 0.0896\n",
      "[0 / 32]: Loss = 0.1242\n",
      "[1 / 32]: Loss = 0.1181\n",
      "[2 / 32]: Loss = 0.1376\n",
      "[3 / 32]: Loss = 0.1265\n",
      "[4 / 32]: Loss = 0.1244\n",
      "[5 / 32]: Loss = 0.1724\n",
      "[6 / 32]: Loss = 0.1313\n",
      "[7 / 32]: Loss = 0.1344\n",
      "[8 / 32]: Loss = 0.1423\n",
      "[9 / 32]: Loss = 0.1662\n",
      "[10 / 32]: Loss = 0.1304\n",
      "[11 / 32]: Loss = 0.1306\n",
      "[12 / 32]: Loss = 0.1176\n",
      "[13 / 32]: Loss = 0.1323\n",
      "[14 / 32]: Loss = 0.1202\n",
      "[15 / 32]: Loss = 0.1114\n",
      "[16 / 32]: Loss = 0.1407\n",
      "[17 / 32]: Loss = 0.1259\n",
      "[18 / 32]: Loss = 0.1224\n",
      "[19 / 32]: Loss = 0.1299\n",
      "[20 / 32]: Loss = 0.1373\n",
      "[21 / 32]: Loss = 0.1403\n",
      "[22 / 32]: Loss = 0.1521\n",
      "[23 / 32]: Loss = 0.1239\n",
      "[24 / 32]: Loss = 0.1446\n",
      "[25 / 32]: Loss = 0.1542\n",
      "[26 / 32]: Loss = 0.1483\n",
      "[27 / 32]: Loss = 0.1373\n",
      "[28 / 32]: Loss = 0.1412\n",
      "[29 / 32]: Loss = 0.1456\n",
      "[30 / 32]: Loss = 0.1388\n",
      "[31 / 32]: Loss = 0.1123\n",
      "Epoch 3 / 5, Epoch Time = 258.16s: Train Loss = 0.1286, Train AUC = 0.8631, Val Loss = 0.1348, Val AUC = 0.8442\n",
      "[0 / 250]: Loss = 0.1138\n",
      "[1 / 250]: Loss = 0.1292\n",
      "[2 / 250]: Loss = 0.1076\n",
      "[3 / 250]: Loss = 0.1229\n",
      "[4 / 250]: Loss = 0.1372\n",
      "[5 / 250]: Loss = 0.1245\n",
      "[6 / 250]: Loss = 0.1223\n",
      "[7 / 250]: Loss = 0.1105\n",
      "[8 / 250]: Loss = 0.1501\n",
      "[9 / 250]: Loss = 0.1377\n",
      "[10 / 250]: Loss = 0.1441\n",
      "[11 / 250]: Loss = 0.1355\n",
      "[12 / 250]: Loss = 0.0990\n",
      "[13 / 250]: Loss = 0.1371\n",
      "[14 / 250]: Loss = 0.0999\n",
      "[15 / 250]: Loss = 0.1272\n",
      "[16 / 250]: Loss = 0.1233\n",
      "[17 / 250]: Loss = 0.1284\n",
      "[18 / 250]: Loss = 0.1291\n",
      "[19 / 250]: Loss = 0.1415\n",
      "[20 / 250]: Loss = 0.1213\n",
      "[21 / 250]: Loss = 0.1239\n",
      "[22 / 250]: Loss = 0.1416\n",
      "[23 / 250]: Loss = 0.1229\n",
      "[24 / 250]: Loss = 0.1494\n",
      "[25 / 250]: Loss = 0.1180\n",
      "[26 / 250]: Loss = 0.1327\n",
      "[27 / 250]: Loss = 0.1146\n",
      "[28 / 250]: Loss = 0.1359\n",
      "[29 / 250]: Loss = 0.1417\n",
      "[30 / 250]: Loss = 0.1190\n",
      "[31 / 250]: Loss = 0.1545\n",
      "[32 / 250]: Loss = 0.1402\n",
      "[33 / 250]: Loss = 0.1142\n",
      "[34 / 250]: Loss = 0.1272\n",
      "[35 / 250]: Loss = 0.1127\n",
      "[36 / 250]: Loss = 0.1219\n",
      "[37 / 250]: Loss = 0.1413\n",
      "[38 / 250]: Loss = 0.1111\n",
      "[39 / 250]: Loss = 0.1153\n",
      "[40 / 250]: Loss = 0.1140\n",
      "[41 / 250]: Loss = 0.1209\n",
      "[42 / 250]: Loss = 0.1317\n",
      "[43 / 250]: Loss = 0.1177\n",
      "[44 / 250]: Loss = 0.1225\n",
      "[45 / 250]: Loss = 0.1095\n",
      "[46 / 250]: Loss = 0.1118\n",
      "[47 / 250]: Loss = 0.1439\n",
      "[48 / 250]: Loss = 0.1074\n",
      "[49 / 250]: Loss = 0.1352\n",
      "[50 / 250]: Loss = 0.1341\n",
      "[51 / 250]: Loss = 0.0980\n",
      "[52 / 250]: Loss = 0.1248\n",
      "[53 / 250]: Loss = 0.1074\n",
      "[54 / 250]: Loss = 0.1248\n",
      "[55 / 250]: Loss = 0.1351\n",
      "[56 / 250]: Loss = 0.1053\n",
      "[57 / 250]: Loss = 0.1259\n",
      "[58 / 250]: Loss = 0.1305\n",
      "[59 / 250]: Loss = 0.1280\n",
      "[60 / 250]: Loss = 0.1451\n",
      "[61 / 250]: Loss = 0.1550\n",
      "[62 / 250]: Loss = 0.1415\n",
      "[63 / 250]: Loss = 0.1069\n",
      "[64 / 250]: Loss = 0.1210\n",
      "[65 / 250]: Loss = 0.1071\n",
      "[66 / 250]: Loss = 0.1499\n",
      "[67 / 250]: Loss = 0.1201\n",
      "[68 / 250]: Loss = 0.1320\n",
      "[69 / 250]: Loss = 0.1064\n",
      "[70 / 250]: Loss = 0.1583\n",
      "[71 / 250]: Loss = 0.1488\n",
      "[72 / 250]: Loss = 0.1458\n",
      "[73 / 250]: Loss = 0.1057\n",
      "[74 / 250]: Loss = 0.1030\n",
      "[75 / 250]: Loss = 0.1241\n",
      "[76 / 250]: Loss = 0.1029\n",
      "[77 / 250]: Loss = 0.1117\n",
      "[78 / 250]: Loss = 0.1335\n",
      "[79 / 250]: Loss = 0.1251\n",
      "[80 / 250]: Loss = 0.1321\n",
      "[81 / 250]: Loss = 0.1139\n",
      "[82 / 250]: Loss = 0.1182\n",
      "[83 / 250]: Loss = 0.0952\n",
      "[84 / 250]: Loss = 0.1066\n",
      "[85 / 250]: Loss = 0.1251\n",
      "[86 / 250]: Loss = 0.0847\n",
      "[87 / 250]: Loss = 0.1195\n",
      "[88 / 250]: Loss = 0.1358\n",
      "[89 / 250]: Loss = 0.1204\n",
      "[90 / 250]: Loss = 0.1251\n",
      "[91 / 250]: Loss = 0.1154\n",
      "[92 / 250]: Loss = 0.1358\n",
      "[93 / 250]: Loss = 0.1392\n",
      "[94 / 250]: Loss = 0.1239\n",
      "[95 / 250]: Loss = 0.1246\n",
      "[96 / 250]: Loss = 0.1107\n",
      "[97 / 250]: Loss = 0.1160\n",
      "[98 / 250]: Loss = 0.1175\n",
      "[99 / 250]: Loss = 0.1395\n",
      "[100 / 250]: Loss = 0.1122\n",
      "[101 / 250]: Loss = 0.1388\n",
      "[102 / 250]: Loss = 0.1199\n",
      "[103 / 250]: Loss = 0.1340\n",
      "[104 / 250]: Loss = 0.1163\n",
      "[105 / 250]: Loss = 0.1586\n",
      "[106 / 250]: Loss = 0.1273\n",
      "[107 / 250]: Loss = 0.1346\n",
      "[108 / 250]: Loss = 0.1503\n",
      "[109 / 250]: Loss = 0.1326\n",
      "[110 / 250]: Loss = 0.1133\n",
      "[111 / 250]: Loss = 0.1091\n",
      "[112 / 250]: Loss = 0.1145\n",
      "[113 / 250]: Loss = 0.1329\n",
      "[114 / 250]: Loss = 0.1457\n",
      "[115 / 250]: Loss = 0.1166\n",
      "[116 / 250]: Loss = 0.1063\n",
      "[117 / 250]: Loss = 0.1042\n",
      "[118 / 250]: Loss = 0.1180\n",
      "[119 / 250]: Loss = 0.1526\n",
      "[120 / 250]: Loss = 0.1310\n",
      "[121 / 250]: Loss = 0.1065\n",
      "[122 / 250]: Loss = 0.1053\n",
      "[123 / 250]: Loss = 0.1138\n",
      "[124 / 250]: Loss = 0.1134\n",
      "[125 / 250]: Loss = 0.1186\n",
      "[126 / 250]: Loss = 0.1262\n",
      "[127 / 250]: Loss = 0.1086\n",
      "[128 / 250]: Loss = 0.1097\n",
      "[129 / 250]: Loss = 0.0972\n",
      "[130 / 250]: Loss = 0.1267\n",
      "[131 / 250]: Loss = 0.1334\n",
      "[132 / 250]: Loss = 0.1315\n",
      "[133 / 250]: Loss = 0.0923\n",
      "[134 / 250]: Loss = 0.1033\n",
      "[135 / 250]: Loss = 0.1336\n",
      "[136 / 250]: Loss = 0.1573\n",
      "[137 / 250]: Loss = 0.1213\n",
      "[138 / 250]: Loss = 0.0989\n",
      "[139 / 250]: Loss = 0.1201\n",
      "[140 / 250]: Loss = 0.1234\n",
      "[141 / 250]: Loss = 0.1222\n",
      "[142 / 250]: Loss = 0.1422\n",
      "[143 / 250]: Loss = 0.1305\n",
      "[144 / 250]: Loss = 0.1074\n",
      "[145 / 250]: Loss = 0.1230\n",
      "[146 / 250]: Loss = 0.1266\n",
      "[147 / 250]: Loss = 0.1154\n",
      "[148 / 250]: Loss = 0.1026\n",
      "[149 / 250]: Loss = 0.1023\n",
      "[150 / 250]: Loss = 0.1454\n",
      "[151 / 250]: Loss = 0.1514\n",
      "[152 / 250]: Loss = 0.1685\n",
      "[153 / 250]: Loss = 0.0852\n",
      "[154 / 250]: Loss = 0.1371\n",
      "[155 / 250]: Loss = 0.1281\n",
      "[156 / 250]: Loss = 0.1110\n",
      "[157 / 250]: Loss = 0.0981\n",
      "[158 / 250]: Loss = 0.1303\n",
      "[159 / 250]: Loss = 0.1221\n",
      "[160 / 250]: Loss = 0.1106\n",
      "[161 / 250]: Loss = 0.1251\n",
      "[162 / 250]: Loss = 0.1390\n",
      "[163 / 250]: Loss = 0.1273\n",
      "[164 / 250]: Loss = 0.1217\n",
      "[165 / 250]: Loss = 0.1093\n",
      "[166 / 250]: Loss = 0.1244\n",
      "[167 / 250]: Loss = 0.1202\n",
      "[168 / 250]: Loss = 0.1169\n",
      "[169 / 250]: Loss = 0.0940\n",
      "[170 / 250]: Loss = 0.1065\n",
      "[171 / 250]: Loss = 0.1399\n",
      "[172 / 250]: Loss = 0.1217\n",
      "[173 / 250]: Loss = 0.1248\n",
      "[174 / 250]: Loss = 0.1190\n",
      "[175 / 250]: Loss = 0.1015\n",
      "[176 / 250]: Loss = 0.0995\n",
      "[177 / 250]: Loss = 0.1202\n",
      "[178 / 250]: Loss = 0.1200\n",
      "[179 / 250]: Loss = 0.1338\n",
      "[180 / 250]: Loss = 0.1081\n",
      "[181 / 250]: Loss = 0.1331\n",
      "[182 / 250]: Loss = 0.1194\n",
      "[183 / 250]: Loss = 0.1112\n",
      "[184 / 250]: Loss = 0.1066\n",
      "[185 / 250]: Loss = 0.1202\n",
      "[186 / 250]: Loss = 0.1299\n",
      "[187 / 250]: Loss = 0.1143\n",
      "[188 / 250]: Loss = 0.1241\n",
      "[189 / 250]: Loss = 0.1180\n",
      "[190 / 250]: Loss = 0.1369\n",
      "[191 / 250]: Loss = 0.1302\n",
      "[192 / 250]: Loss = 0.1280\n",
      "[193 / 250]: Loss = 0.1216\n",
      "[194 / 250]: Loss = 0.1042\n",
      "[195 / 250]: Loss = 0.1420\n",
      "[196 / 250]: Loss = 0.1114\n",
      "[197 / 250]: Loss = 0.1270\n",
      "[198 / 250]: Loss = 0.1312\n",
      "[199 / 250]: Loss = 0.1294\n",
      "[200 / 250]: Loss = 0.0883\n",
      "[201 / 250]: Loss = 0.0965\n",
      "[202 / 250]: Loss = 0.1160\n",
      "[203 / 250]: Loss = 0.1083\n",
      "[204 / 250]: Loss = 0.1199\n",
      "[205 / 250]: Loss = 0.1160\n",
      "[206 / 250]: Loss = 0.1363\n",
      "[207 / 250]: Loss = 0.1178\n",
      "[208 / 250]: Loss = 0.1197\n",
      "[209 / 250]: Loss = 0.1252\n",
      "[210 / 250]: Loss = 0.1137\n",
      "[211 / 250]: Loss = 0.1301\n",
      "[212 / 250]: Loss = 0.1255\n",
      "[213 / 250]: Loss = 0.1116\n",
      "[214 / 250]: Loss = 0.1222\n",
      "[215 / 250]: Loss = 0.1383\n",
      "[216 / 250]: Loss = 0.1114\n",
      "[217 / 250]: Loss = 0.1129\n",
      "[218 / 250]: Loss = 0.1366\n",
      "[219 / 250]: Loss = 0.1296\n",
      "[220 / 250]: Loss = 0.1124\n",
      "[221 / 250]: Loss = 0.1394\n",
      "[222 / 250]: Loss = 0.1242\n",
      "[223 / 250]: Loss = 0.1218\n",
      "[224 / 250]: Loss = 0.1483\n",
      "[225 / 250]: Loss = 0.1337\n",
      "[226 / 250]: Loss = 0.1503\n",
      "[227 / 250]: Loss = 0.1400\n",
      "[228 / 250]: Loss = 0.1251\n",
      "[229 / 250]: Loss = 0.1065\n",
      "[230 / 250]: Loss = 0.1247\n",
      "[231 / 250]: Loss = 0.1061\n",
      "[232 / 250]: Loss = 0.1182\n",
      "[233 / 250]: Loss = 0.1073\n",
      "[234 / 250]: Loss = 0.0972\n",
      "[235 / 250]: Loss = 0.1256\n",
      "[236 / 250]: Loss = 0.1229\n",
      "[237 / 250]: Loss = 0.1245\n",
      "[238 / 250]: Loss = 0.1161\n",
      "[239 / 250]: Loss = 0.1552\n",
      "[240 / 250]: Loss = 0.1347\n",
      "[241 / 250]: Loss = 0.1324\n",
      "[242 / 250]: Loss = 0.1110\n",
      "[243 / 250]: Loss = 0.1236\n",
      "[244 / 250]: Loss = 0.1446\n",
      "[245 / 250]: Loss = 0.1132\n",
      "[246 / 250]: Loss = 0.1301\n",
      "[247 / 250]: Loss = 0.1318\n",
      "[248 / 250]: Loss = 0.1323\n",
      "[249 / 250]: Loss = 0.1382\n",
      "[0 / 32]: Loss = 0.1307\n",
      "[1 / 32]: Loss = 0.1228\n",
      "[2 / 32]: Loss = 0.1371\n",
      "[3 / 32]: Loss = 0.1304\n",
      "[4 / 32]: Loss = 0.1450\n",
      "[5 / 32]: Loss = 0.1200\n",
      "[6 / 32]: Loss = 0.1352\n",
      "[7 / 32]: Loss = 0.1424\n",
      "[8 / 32]: Loss = 0.1467\n",
      "[9 / 32]: Loss = 0.1245\n",
      "[10 / 32]: Loss = 0.1186\n",
      "[11 / 32]: Loss = 0.1349\n",
      "[12 / 32]: Loss = 0.1356\n",
      "[13 / 32]: Loss = 0.1453\n",
      "[14 / 32]: Loss = 0.1506\n",
      "[15 / 32]: Loss = 0.1307\n",
      "[16 / 32]: Loss = 0.1255\n",
      "[17 / 32]: Loss = 0.1396\n",
      "[18 / 32]: Loss = 0.1511\n",
      "[19 / 32]: Loss = 0.1258\n",
      "[20 / 32]: Loss = 0.1314\n",
      "[21 / 32]: Loss = 0.1302\n",
      "[22 / 32]: Loss = 0.1336\n",
      "[23 / 32]: Loss = 0.1299\n",
      "[24 / 32]: Loss = 0.1291\n",
      "[25 / 32]: Loss = 0.1419\n",
      "[26 / 32]: Loss = 0.1216\n",
      "[27 / 32]: Loss = 0.1436\n",
      "[28 / 32]: Loss = 0.1419\n",
      "[29 / 32]: Loss = 0.1377\n",
      "[30 / 32]: Loss = 0.1386\n",
      "[31 / 32]: Loss = 0.1700\n",
      "Epoch 4 / 5, Epoch Time = 257.88s: Train Loss = 0.1232, Train AUC = 0.8772, Val Loss = 0.1357, Val AUC = 0.8436\n",
      "[0 / 250]: Loss = 0.1358\n",
      "[1 / 250]: Loss = 0.1253\n",
      "[2 / 250]: Loss = 0.1342\n",
      "[3 / 250]: Loss = 0.1132\n",
      "[4 / 250]: Loss = 0.1002\n",
      "[5 / 250]: Loss = 0.1083\n",
      "[6 / 250]: Loss = 0.1110\n",
      "[7 / 250]: Loss = 0.1231\n",
      "[8 / 250]: Loss = 0.1277\n",
      "[9 / 250]: Loss = 0.1195\n",
      "[10 / 250]: Loss = 0.0990\n",
      "[11 / 250]: Loss = 0.1289\n",
      "[12 / 250]: Loss = 0.1379\n",
      "[13 / 250]: Loss = 0.1056\n",
      "[14 / 250]: Loss = 0.1011\n",
      "[15 / 250]: Loss = 0.1175\n",
      "[16 / 250]: Loss = 0.1278\n",
      "[17 / 250]: Loss = 0.1181\n",
      "[18 / 250]: Loss = 0.1205\n",
      "[19 / 250]: Loss = 0.0973\n",
      "[20 / 250]: Loss = 0.1195\n",
      "[21 / 250]: Loss = 0.1478\n",
      "[22 / 250]: Loss = 0.1033\n",
      "[23 / 250]: Loss = 0.1471\n",
      "[24 / 250]: Loss = 0.1044\n",
      "[25 / 250]: Loss = 0.1137\n",
      "[26 / 250]: Loss = 0.1255\n",
      "[27 / 250]: Loss = 0.1160\n",
      "[28 / 250]: Loss = 0.0904\n",
      "[29 / 250]: Loss = 0.1140\n",
      "[30 / 250]: Loss = 0.1184\n",
      "[31 / 250]: Loss = 0.0857\n",
      "[32 / 250]: Loss = 0.1201\n",
      "[33 / 250]: Loss = 0.1307\n",
      "[34 / 250]: Loss = 0.1300\n",
      "[35 / 250]: Loss = 0.1027\n",
      "[36 / 250]: Loss = 0.1138\n",
      "[37 / 250]: Loss = 0.1166\n",
      "[38 / 250]: Loss = 0.0977\n",
      "[39 / 250]: Loss = 0.1206\n",
      "[40 / 250]: Loss = 0.1128\n",
      "[41 / 250]: Loss = 0.1190\n",
      "[42 / 250]: Loss = 0.1256\n",
      "[43 / 250]: Loss = 0.1075\n",
      "[44 / 250]: Loss = 0.1179\n",
      "[45 / 250]: Loss = 0.1138\n",
      "[46 / 250]: Loss = 0.1051\n",
      "[47 / 250]: Loss = 0.1399\n",
      "[48 / 250]: Loss = 0.1416\n",
      "[49 / 250]: Loss = 0.1113\n",
      "[50 / 250]: Loss = 0.1060\n",
      "[51 / 250]: Loss = 0.1446\n",
      "[52 / 250]: Loss = 0.1087\n",
      "[53 / 250]: Loss = 0.1062\n",
      "[54 / 250]: Loss = 0.1280\n",
      "[55 / 250]: Loss = 0.0982\n",
      "[56 / 250]: Loss = 0.1287\n",
      "[57 / 250]: Loss = 0.1116\n",
      "[58 / 250]: Loss = 0.1353\n",
      "[59 / 250]: Loss = 0.0956\n",
      "[60 / 250]: Loss = 0.1225\n",
      "[61 / 250]: Loss = 0.1162\n",
      "[62 / 250]: Loss = 0.1390\n",
      "[63 / 250]: Loss = 0.1348\n",
      "[64 / 250]: Loss = 0.0987\n",
      "[65 / 250]: Loss = 0.1204\n",
      "[66 / 250]: Loss = 0.1173\n",
      "[67 / 250]: Loss = 0.1318\n",
      "[68 / 250]: Loss = 0.1103\n",
      "[69 / 250]: Loss = 0.1095\n",
      "[70 / 250]: Loss = 0.1282\n",
      "[71 / 250]: Loss = 0.0884\n",
      "[72 / 250]: Loss = 0.1041\n",
      "[73 / 250]: Loss = 0.1011\n",
      "[74 / 250]: Loss = 0.1211\n",
      "[75 / 250]: Loss = 0.1062\n",
      "[76 / 250]: Loss = 0.1270\n",
      "[77 / 250]: Loss = 0.1210\n",
      "[78 / 250]: Loss = 0.1315\n",
      "[79 / 250]: Loss = 0.1199\n",
      "[80 / 250]: Loss = 0.1265\n",
      "[81 / 250]: Loss = 0.1164\n",
      "[82 / 250]: Loss = 0.1107\n",
      "[83 / 250]: Loss = 0.1083\n",
      "[84 / 250]: Loss = 0.1239\n",
      "[85 / 250]: Loss = 0.1246\n",
      "[86 / 250]: Loss = 0.1011\n",
      "[87 / 250]: Loss = 0.1057\n",
      "[88 / 250]: Loss = 0.1236\n",
      "[89 / 250]: Loss = 0.1340\n",
      "[90 / 250]: Loss = 0.1166\n",
      "[91 / 250]: Loss = 0.1231\n",
      "[92 / 250]: Loss = 0.1147\n",
      "[93 / 250]: Loss = 0.1108\n",
      "[94 / 250]: Loss = 0.1470\n",
      "[95 / 250]: Loss = 0.1234\n",
      "[96 / 250]: Loss = 0.1127\n",
      "[97 / 250]: Loss = 0.1162\n",
      "[98 / 250]: Loss = 0.1128\n",
      "[99 / 250]: Loss = 0.1131\n",
      "[100 / 250]: Loss = 0.1268\n",
      "[101 / 250]: Loss = 0.1493\n",
      "[102 / 250]: Loss = 0.1210\n",
      "[103 / 250]: Loss = 0.1045\n",
      "[104 / 250]: Loss = 0.0898\n",
      "[105 / 250]: Loss = 0.1401\n",
      "[106 / 250]: Loss = 0.1258\n",
      "[107 / 250]: Loss = 0.1163\n",
      "[108 / 250]: Loss = 0.1366\n",
      "[109 / 250]: Loss = 0.1187\n",
      "[110 / 250]: Loss = 0.1270\n",
      "[111 / 250]: Loss = 0.1249\n",
      "[112 / 250]: Loss = 0.1072\n",
      "[113 / 250]: Loss = 0.1169\n",
      "[114 / 250]: Loss = 0.1352\n",
      "[115 / 250]: Loss = 0.1241\n",
      "[116 / 250]: Loss = 0.1308\n",
      "[117 / 250]: Loss = 0.1604\n",
      "[118 / 250]: Loss = 0.1173\n",
      "[119 / 250]: Loss = 0.1254\n",
      "[120 / 250]: Loss = 0.0943\n",
      "[121 / 250]: Loss = 0.1100\n",
      "[122 / 250]: Loss = 0.1091\n",
      "[123 / 250]: Loss = 0.1210\n",
      "[124 / 250]: Loss = 0.1332\n",
      "[125 / 250]: Loss = 0.1290\n",
      "[126 / 250]: Loss = 0.1018\n",
      "[127 / 250]: Loss = 0.1155\n",
      "[128 / 250]: Loss = 0.1096\n",
      "[129 / 250]: Loss = 0.1154\n",
      "[130 / 250]: Loss = 0.0957\n",
      "[131 / 250]: Loss = 0.0898\n",
      "[132 / 250]: Loss = 0.1370\n",
      "[133 / 250]: Loss = 0.1015\n",
      "[134 / 250]: Loss = 0.1040\n",
      "[135 / 250]: Loss = 0.1127\n",
      "[136 / 250]: Loss = 0.1421\n",
      "[137 / 250]: Loss = 0.1322\n",
      "[138 / 250]: Loss = 0.1129\n",
      "[139 / 250]: Loss = 0.1262\n",
      "[140 / 250]: Loss = 0.1342\n",
      "[141 / 250]: Loss = 0.1220\n",
      "[142 / 250]: Loss = 0.0984\n",
      "[143 / 250]: Loss = 0.1098\n",
      "[144 / 250]: Loss = 0.1429\n",
      "[145 / 250]: Loss = 0.1148\n",
      "[146 / 250]: Loss = 0.1283\n",
      "[147 / 250]: Loss = 0.1197\n",
      "[148 / 250]: Loss = 0.1096\n",
      "[149 / 250]: Loss = 0.1034\n",
      "[150 / 250]: Loss = 0.0807\n",
      "[151 / 250]: Loss = 0.1007\n",
      "[152 / 250]: Loss = 0.1391\n",
      "[153 / 250]: Loss = 0.1158\n",
      "[154 / 250]: Loss = 0.1231\n",
      "[155 / 250]: Loss = 0.1191\n",
      "[156 / 250]: Loss = 0.1082\n",
      "[157 / 250]: Loss = 0.1361\n",
      "[158 / 250]: Loss = 0.1026\n",
      "[159 / 250]: Loss = 0.1252\n",
      "[160 / 250]: Loss = 0.0939\n",
      "[161 / 250]: Loss = 0.1165\n",
      "[162 / 250]: Loss = 0.1301\n",
      "[163 / 250]: Loss = 0.1358\n",
      "[164 / 250]: Loss = 0.1223\n",
      "[165 / 250]: Loss = 0.1263\n",
      "[166 / 250]: Loss = 0.1119\n",
      "[167 / 250]: Loss = 0.1393\n",
      "[168 / 250]: Loss = 0.1049\n",
      "[169 / 250]: Loss = 0.1125\n",
      "[170 / 250]: Loss = 0.1377\n",
      "[171 / 250]: Loss = 0.1212\n",
      "[172 / 250]: Loss = 0.1172\n",
      "[173 / 250]: Loss = 0.1164\n",
      "[174 / 250]: Loss = 0.1227\n",
      "[175 / 250]: Loss = 0.1332\n",
      "[176 / 250]: Loss = 0.1119\n",
      "[177 / 250]: Loss = 0.1088\n",
      "[178 / 250]: Loss = 0.1252\n",
      "[179 / 250]: Loss = 0.1196\n",
      "[180 / 250]: Loss = 0.1309\n",
      "[181 / 250]: Loss = 0.1281\n",
      "[182 / 250]: Loss = 0.1116\n",
      "[183 / 250]: Loss = 0.1054\n",
      "[184 / 250]: Loss = 0.1384\n",
      "[185 / 250]: Loss = 0.1320\n",
      "[186 / 250]: Loss = 0.1231\n",
      "[187 / 250]: Loss = 0.1332\n",
      "[188 / 250]: Loss = 0.1415\n",
      "[189 / 250]: Loss = 0.1136\n",
      "[190 / 250]: Loss = 0.1063\n",
      "[191 / 250]: Loss = 0.1191\n",
      "[192 / 250]: Loss = 0.1295\n",
      "[193 / 250]: Loss = 0.1341\n",
      "[194 / 250]: Loss = 0.0931\n",
      "[195 / 250]: Loss = 0.1015\n",
      "[196 / 250]: Loss = 0.1077\n",
      "[197 / 250]: Loss = 0.1238\n",
      "[198 / 250]: Loss = 0.1042\n",
      "[199 / 250]: Loss = 0.0745\n",
      "[200 / 250]: Loss = 0.1200\n",
      "[201 / 250]: Loss = 0.1031\n",
      "[202 / 250]: Loss = 0.1036\n",
      "[203 / 250]: Loss = 0.1093\n",
      "[204 / 250]: Loss = 0.0978\n",
      "[205 / 250]: Loss = 0.1331\n",
      "[206 / 250]: Loss = 0.1361\n",
      "[207 / 250]: Loss = 0.1173\n",
      "[208 / 250]: Loss = 0.1120\n",
      "[209 / 250]: Loss = 0.1109\n",
      "[210 / 250]: Loss = 0.1352\n",
      "[211 / 250]: Loss = 0.1169\n",
      "[212 / 250]: Loss = 0.1205\n",
      "[213 / 250]: Loss = 0.1007\n",
      "[214 / 250]: Loss = 0.1123\n",
      "[215 / 250]: Loss = 0.1297\n",
      "[216 / 250]: Loss = 0.1230\n",
      "[217 / 250]: Loss = 0.1192\n",
      "[218 / 250]: Loss = 0.1408\n",
      "[219 / 250]: Loss = 0.0936\n",
      "[220 / 250]: Loss = 0.1242\n",
      "[221 / 250]: Loss = 0.1255\n",
      "[222 / 250]: Loss = 0.1344\n",
      "[223 / 250]: Loss = 0.1356\n",
      "[224 / 250]: Loss = 0.1186\n",
      "[225 / 250]: Loss = 0.1133\n",
      "[226 / 250]: Loss = 0.1264\n",
      "[227 / 250]: Loss = 0.1264\n",
      "[228 / 250]: Loss = 0.1131\n",
      "[229 / 250]: Loss = 0.1204\n",
      "[230 / 250]: Loss = 0.1471\n",
      "[231 / 250]: Loss = 0.1296\n",
      "[232 / 250]: Loss = 0.1429\n",
      "[233 / 250]: Loss = 0.1477\n",
      "[234 / 250]: Loss = 0.1406\n",
      "[235 / 250]: Loss = 0.1150\n",
      "[236 / 250]: Loss = 0.1304\n",
      "[237 / 250]: Loss = 0.1312\n",
      "[238 / 250]: Loss = 0.1235\n",
      "[239 / 250]: Loss = 0.1295\n",
      "[240 / 250]: Loss = 0.1241\n",
      "[241 / 250]: Loss = 0.0952\n",
      "[242 / 250]: Loss = 0.1125\n",
      "[243 / 250]: Loss = 0.1473\n",
      "[244 / 250]: Loss = 0.1062\n",
      "[245 / 250]: Loss = 0.1267\n",
      "[246 / 250]: Loss = 0.1087\n",
      "[247 / 250]: Loss = 0.1184\n",
      "[248 / 250]: Loss = 0.1195\n",
      "[249 / 250]: Loss = 0.1202\n",
      "[0 / 32]: Loss = 0.1322\n",
      "[1 / 32]: Loss = 0.1473\n",
      "[2 / 32]: Loss = 0.1460\n",
      "[3 / 32]: Loss = 0.1325\n",
      "[4 / 32]: Loss = 0.1304\n",
      "[5 / 32]: Loss = 0.1347\n",
      "[6 / 32]: Loss = 0.1301\n",
      "[7 / 32]: Loss = 0.1354\n",
      "[8 / 32]: Loss = 0.1502\n",
      "[9 / 32]: Loss = 0.1669\n",
      "[10 / 32]: Loss = 0.1368\n",
      "[11 / 32]: Loss = 0.1561\n",
      "[12 / 32]: Loss = 0.1327\n",
      "[13 / 32]: Loss = 0.1295\n",
      "[14 / 32]: Loss = 0.1383\n",
      "[15 / 32]: Loss = 0.1237\n",
      "[16 / 32]: Loss = 0.1106\n",
      "[17 / 32]: Loss = 0.1218\n",
      "[18 / 32]: Loss = 0.1237\n",
      "[19 / 32]: Loss = 0.1585\n",
      "[20 / 32]: Loss = 0.1228\n",
      "[21 / 32]: Loss = 0.1160\n",
      "[22 / 32]: Loss = 0.1183\n",
      "[23 / 32]: Loss = 0.1247\n",
      "[24 / 32]: Loss = 0.1505\n",
      "[25 / 32]: Loss = 0.1387\n",
      "[26 / 32]: Loss = 0.1347\n",
      "[27 / 32]: Loss = 0.1384\n",
      "[28 / 32]: Loss = 0.1356\n",
      "[29 / 32]: Loss = 0.1451\n",
      "[30 / 32]: Loss = 0.1227\n",
      "[31 / 32]: Loss = 0.1596\n",
      "Epoch 5 / 5, Epoch Time = 257.92s: Train Loss = 0.1189, Train AUC = 0.8884, Val Loss = 0.1358, Val AUC = 0.8438\n"
     ]
    }
   ],
   "source": [
    "model        = TConvolution(len(token_to_id), hidden_dim=64, num_classes=7).cuda()\n",
    "criterion    = nn.BCEWithLogitsLoss().cuda()\n",
    "optimizer    = optim.Adam([param for param in model.parameters() if param.requires_grad], lr=0.01)\n",
    "\n",
    "X_train      = as_matrix(data_train['tokenized_comments'])\n",
    "train_labels = data_train.loc[:, TARGET_COLS].values \n",
    "\n",
    "X_test       = as_matrix(data_val['tokenized_comments'])\n",
    "test_labels  = data_val.loc[:, TARGET_COLS].values\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, train_labels), epochs_count=5, \n",
    "    batch_size=512, val_data=(X_test, test_labels), val_batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Full Training\n",
    "\n",
    "a) Train model for 3 epochs using our Convolutional Model.\n",
    "b) Store all the hyper-parameters used for the experiment.\n",
    "c) Store model to disk using Pytorch best practices.\n",
    "d) Load model from disk in the prediction phase ( full test set )\n",
    "e) Write a method that takes in model and the test dataset and returns back all the predictions in the same\n",
    "   order.\n",
    "f) Generate logits for every label ( we would generate logits for all the 7 labels but we need sigmoid\n",
    "   for only 6 labels ).\n",
    "g) Apply sigmoid across all rows for all the 6 not 7 labels.\n",
    "h) Using Kaggle API, submit predictions to kaggle and note down the public and private evaluation score.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data, batch_size):\n",
    "    is_train = False\n",
    "    model.train(is_train)\n",
    "    \n",
    "    batchs_count = math.ceil(data.shape[0] / batch_size)\n",
    "    preds        = []\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        for i, X_batch in enumerate(iterate_batches(data, labels=[], batch_size=batch_size, predict_mode='test')):\n",
    "            X_batch = torch.cuda.LongTensor(X_batch)\n",
    "            logits  = model(X_batch)\n",
    "            p       = torch.sigmoid(logits).cpu().detach().numpy()\n",
    "            \n",
    "            preds.append(p)\n",
    "    \n",
    "    return np.vstack(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 / 312]: Loss = 0.9834\n",
      "[1 / 312]: Loss = 0.8984\n",
      "[2 / 312]: Loss = 0.8541\n",
      "[3 / 312]: Loss = 0.8202\n",
      "[4 / 312]: Loss = 0.7915\n",
      "[5 / 312]: Loss = 0.7702\n",
      "[6 / 312]: Loss = 0.7529\n",
      "[7 / 312]: Loss = 0.7377\n",
      "[8 / 312]: Loss = 0.7260\n",
      "[9 / 312]: Loss = 0.7161\n",
      "[10 / 312]: Loss = 0.7075\n",
      "[11 / 312]: Loss = 0.6983\n",
      "[12 / 312]: Loss = 0.6914\n",
      "[13 / 312]: Loss = 0.6839\n",
      "[14 / 312]: Loss = 0.6769\n",
      "[15 / 312]: Loss = 0.6702\n",
      "[16 / 312]: Loss = 0.6587\n",
      "[17 / 312]: Loss = 0.6497\n",
      "[18 / 312]: Loss = 0.6413\n",
      "[19 / 312]: Loss = 0.6313\n",
      "[20 / 312]: Loss = 0.6199\n",
      "[21 / 312]: Loss = 0.6081\n",
      "[22 / 312]: Loss = 0.5976\n",
      "[23 / 312]: Loss = 0.5861\n",
      "[24 / 312]: Loss = 0.5755\n",
      "[25 / 312]: Loss = 0.5579\n",
      "[26 / 312]: Loss = 0.5468\n",
      "[27 / 312]: Loss = 0.5372\n",
      "[28 / 312]: Loss = 0.5193\n",
      "[29 / 312]: Loss = 0.4926\n",
      "[30 / 312]: Loss = 0.4867\n",
      "[31 / 312]: Loss = 0.4672\n",
      "[32 / 312]: Loss = 0.4685\n",
      "[33 / 312]: Loss = 0.4414\n",
      "[34 / 312]: Loss = 0.4333\n",
      "[35 / 312]: Loss = 0.4315\n",
      "[36 / 312]: Loss = 0.4149\n",
      "[37 / 312]: Loss = 0.3900\n",
      "[38 / 312]: Loss = 0.3739\n",
      "[39 / 312]: Loss = 0.3866\n",
      "[40 / 312]: Loss = 0.3376\n",
      "[41 / 312]: Loss = 0.3442\n",
      "[42 / 312]: Loss = 0.3232\n",
      "[43 / 312]: Loss = 0.3217\n",
      "[44 / 312]: Loss = 0.3063\n",
      "[45 / 312]: Loss = 0.2652\n",
      "[46 / 312]: Loss = 0.2420\n",
      "[47 / 312]: Loss = 0.2908\n",
      "[48 / 312]: Loss = 0.2479\n",
      "[49 / 312]: Loss = 0.2371\n",
      "[50 / 312]: Loss = 0.2418\n",
      "[51 / 312]: Loss = 0.1842\n",
      "[52 / 312]: Loss = 0.2188\n",
      "[53 / 312]: Loss = 0.2267\n",
      "[54 / 312]: Loss = 0.2254\n",
      "[55 / 312]: Loss = 0.2230\n",
      "[56 / 312]: Loss = 0.2245\n",
      "[57 / 312]: Loss = 0.1928\n",
      "[58 / 312]: Loss = 0.2274\n",
      "[59 / 312]: Loss = 0.1726\n",
      "[60 / 312]: Loss = 0.2203\n",
      "[61 / 312]: Loss = 0.2045\n",
      "[62 / 312]: Loss = 0.1993\n",
      "[63 / 312]: Loss = 0.2294\n",
      "[64 / 312]: Loss = 0.1983\n",
      "[65 / 312]: Loss = 0.1757\n",
      "[66 / 312]: Loss = 0.1749\n",
      "[67 / 312]: Loss = 0.2317\n",
      "[68 / 312]: Loss = 0.2242\n",
      "[69 / 312]: Loss = 0.2080\n",
      "[70 / 312]: Loss = 0.1378\n",
      "[71 / 312]: Loss = 0.1830\n",
      "[72 / 312]: Loss = 0.1792\n",
      "[73 / 312]: Loss = 0.1816\n",
      "[74 / 312]: Loss = 0.1819\n",
      "[75 / 312]: Loss = 0.1589\n",
      "[76 / 312]: Loss = 0.1779\n",
      "[77 / 312]: Loss = 0.1805\n",
      "[78 / 312]: Loss = 0.1703\n",
      "[79 / 312]: Loss = 0.1791\n",
      "[80 / 312]: Loss = 0.1975\n",
      "[81 / 312]: Loss = 0.1929\n",
      "[82 / 312]: Loss = 0.1742\n",
      "[83 / 312]: Loss = 0.1609\n",
      "[84 / 312]: Loss = 0.1581\n",
      "[85 / 312]: Loss = 0.1381\n",
      "[86 / 312]: Loss = 0.1609\n",
      "[87 / 312]: Loss = 0.1561\n",
      "[88 / 312]: Loss = 0.1656\n",
      "[89 / 312]: Loss = 0.1639\n",
      "[90 / 312]: Loss = 0.1544\n",
      "[91 / 312]: Loss = 0.1761\n",
      "[92 / 312]: Loss = 0.1581\n",
      "[93 / 312]: Loss = 0.1680\n",
      "[94 / 312]: Loss = 0.1743\n",
      "[95 / 312]: Loss = 0.1660\n",
      "[96 / 312]: Loss = 0.1693\n",
      "[97 / 312]: Loss = 0.1609\n",
      "[98 / 312]: Loss = 0.1756\n",
      "[99 / 312]: Loss = 0.1259\n",
      "[100 / 312]: Loss = 0.1411\n",
      "[101 / 312]: Loss = 0.1559\n",
      "[102 / 312]: Loss = 0.1553\n",
      "[103 / 312]: Loss = 0.1595\n",
      "[104 / 312]: Loss = 0.1470\n",
      "[105 / 312]: Loss = 0.1662\n",
      "[106 / 312]: Loss = 0.1484\n",
      "[107 / 312]: Loss = 0.1717\n",
      "[108 / 312]: Loss = 0.1271\n",
      "[109 / 312]: Loss = 0.1557\n",
      "[110 / 312]: Loss = 0.1646\n",
      "[111 / 312]: Loss = 0.1235\n",
      "[112 / 312]: Loss = 0.1414\n",
      "[113 / 312]: Loss = 0.1552\n",
      "[114 / 312]: Loss = 0.1691\n",
      "[115 / 312]: Loss = 0.1768\n",
      "[116 / 312]: Loss = 0.1348\n",
      "[117 / 312]: Loss = 0.1602\n",
      "[118 / 312]: Loss = 0.1488\n",
      "[119 / 312]: Loss = 0.1302\n",
      "[120 / 312]: Loss = 0.1164\n",
      "[121 / 312]: Loss = 0.1547\n",
      "[122 / 312]: Loss = 0.1530\n",
      "[123 / 312]: Loss = 0.1487\n",
      "[124 / 312]: Loss = 0.1408\n",
      "[125 / 312]: Loss = 0.1471\n",
      "[126 / 312]: Loss = 0.1683\n",
      "[127 / 312]: Loss = 0.1681\n",
      "[128 / 312]: Loss = 0.1614\n",
      "[129 / 312]: Loss = 0.1528\n",
      "[130 / 312]: Loss = 0.1565\n",
      "[131 / 312]: Loss = 0.1368\n",
      "[132 / 312]: Loss = 0.1613\n",
      "[133 / 312]: Loss = 0.1353\n",
      "[134 / 312]: Loss = 0.1332\n",
      "[135 / 312]: Loss = 0.1532\n",
      "[136 / 312]: Loss = 0.1407\n",
      "[137 / 312]: Loss = 0.1642\n",
      "[138 / 312]: Loss = 0.1462\n",
      "[139 / 312]: Loss = 0.1599\n",
      "[140 / 312]: Loss = 0.1591\n",
      "[141 / 312]: Loss = 0.1602\n",
      "[142 / 312]: Loss = 0.1276\n",
      "[143 / 312]: Loss = 0.1734\n",
      "[144 / 312]: Loss = 0.1385\n",
      "[145 / 312]: Loss = 0.1356\n",
      "[146 / 312]: Loss = 0.1329\n",
      "[147 / 312]: Loss = 0.1729\n",
      "[148 / 312]: Loss = 0.1679\n",
      "[149 / 312]: Loss = 0.1850\n",
      "[150 / 312]: Loss = 0.1544\n",
      "[151 / 312]: Loss = 0.1335\n",
      "[152 / 312]: Loss = 0.1535\n",
      "[153 / 312]: Loss = 0.1599\n",
      "[154 / 312]: Loss = 0.1398\n",
      "[155 / 312]: Loss = 0.1538\n",
      "[156 / 312]: Loss = 0.1180\n",
      "[157 / 312]: Loss = 0.1569\n",
      "[158 / 312]: Loss = 0.1560\n",
      "[159 / 312]: Loss = 0.1152\n",
      "[160 / 312]: Loss = 0.1792\n",
      "[161 / 312]: Loss = 0.1571\n",
      "[162 / 312]: Loss = 0.1468\n",
      "[163 / 312]: Loss = 0.1489\n",
      "[164 / 312]: Loss = 0.1479\n",
      "[165 / 312]: Loss = 0.1460\n",
      "[166 / 312]: Loss = 0.1637\n",
      "[167 / 312]: Loss = 0.1266\n",
      "[168 / 312]: Loss = 0.1460\n",
      "[169 / 312]: Loss = 0.1656\n",
      "[170 / 312]: Loss = 0.1577\n",
      "[171 / 312]: Loss = 0.1502\n",
      "[172 / 312]: Loss = 0.1407\n",
      "[173 / 312]: Loss = 0.1304\n",
      "[174 / 312]: Loss = 0.1678\n",
      "[175 / 312]: Loss = 0.1361\n",
      "[176 / 312]: Loss = 0.1324\n",
      "[177 / 312]: Loss = 0.1451\n",
      "[178 / 312]: Loss = 0.1549\n",
      "[179 / 312]: Loss = 0.1587\n",
      "[180 / 312]: Loss = 0.1720\n",
      "[181 / 312]: Loss = 0.1687\n",
      "[182 / 312]: Loss = 0.1518\n",
      "[183 / 312]: Loss = 0.1496\n",
      "[184 / 312]: Loss = 0.1332\n",
      "[185 / 312]: Loss = 0.1425\n",
      "[186 / 312]: Loss = 0.1529\n",
      "[187 / 312]: Loss = 0.1418\n",
      "[188 / 312]: Loss = 0.1503\n",
      "[189 / 312]: Loss = 0.1394\n",
      "[190 / 312]: Loss = 0.1508\n",
      "[191 / 312]: Loss = 0.1193\n",
      "[192 / 312]: Loss = 0.1221\n",
      "[193 / 312]: Loss = 0.1785\n",
      "[194 / 312]: Loss = 0.1619\n",
      "[195 / 312]: Loss = 0.1585\n",
      "[196 / 312]: Loss = 0.2004\n",
      "[197 / 312]: Loss = 0.1641\n",
      "[198 / 312]: Loss = 0.1298\n",
      "[199 / 312]: Loss = 0.1336\n",
      "[200 / 312]: Loss = 0.1610\n",
      "[201 / 312]: Loss = 0.1682\n",
      "[202 / 312]: Loss = 0.1321\n",
      "[203 / 312]: Loss = 0.1682\n",
      "[204 / 312]: Loss = 0.1587\n",
      "[205 / 312]: Loss = 0.1770\n",
      "[206 / 312]: Loss = 0.1344\n",
      "[207 / 312]: Loss = 0.1508\n",
      "[208 / 312]: Loss = 0.1603\n",
      "[209 / 312]: Loss = 0.1471\n",
      "[210 / 312]: Loss = 0.1468\n",
      "[211 / 312]: Loss = 0.1660\n",
      "[212 / 312]: Loss = 0.1302\n",
      "[213 / 312]: Loss = 0.1492\n",
      "[214 / 312]: Loss = 0.1393\n",
      "[215 / 312]: Loss = 0.1619\n",
      "[216 / 312]: Loss = 0.1410\n",
      "[217 / 312]: Loss = 0.1465\n",
      "[218 / 312]: Loss = 0.1542\n",
      "[219 / 312]: Loss = 0.1413\n",
      "[220 / 312]: Loss = 0.1346\n",
      "[221 / 312]: Loss = 0.1428\n",
      "[222 / 312]: Loss = 0.1385\n",
      "[223 / 312]: Loss = 0.1545\n",
      "[224 / 312]: Loss = 0.1655\n",
      "[225 / 312]: Loss = 0.1386\n",
      "[226 / 312]: Loss = 0.1578\n",
      "[227 / 312]: Loss = 0.1663\n",
      "[228 / 312]: Loss = 0.1365\n",
      "[229 / 312]: Loss = 0.1298\n",
      "[230 / 312]: Loss = 0.1647\n",
      "[231 / 312]: Loss = 0.1602\n",
      "[232 / 312]: Loss = 0.1372\n",
      "[233 / 312]: Loss = 0.1380\n",
      "[234 / 312]: Loss = 0.1508\n",
      "[235 / 312]: Loss = 0.1361\n",
      "[236 / 312]: Loss = 0.1625\n",
      "[237 / 312]: Loss = 0.1480\n",
      "[238 / 312]: Loss = 0.1522\n",
      "[239 / 312]: Loss = 0.1447\n",
      "[240 / 312]: Loss = 0.1266\n",
      "[241 / 312]: Loss = 0.1664\n",
      "[242 / 312]: Loss = 0.1565\n",
      "[243 / 312]: Loss = 0.1329\n",
      "[244 / 312]: Loss = 0.1566\n",
      "[245 / 312]: Loss = 0.1798\n",
      "[246 / 312]: Loss = 0.1421\n",
      "[247 / 312]: Loss = 0.1590\n",
      "[248 / 312]: Loss = 0.1583\n",
      "[249 / 312]: Loss = 0.1230\n",
      "[250 / 312]: Loss = 0.1658\n",
      "[251 / 312]: Loss = 0.1798\n",
      "[252 / 312]: Loss = 0.1553\n",
      "[253 / 312]: Loss = 0.1390\n",
      "[254 / 312]: Loss = 0.1428\n",
      "[255 / 312]: Loss = 0.1290\n",
      "[256 / 312]: Loss = 0.1391\n",
      "[257 / 312]: Loss = 0.1541\n",
      "[258 / 312]: Loss = 0.1194\n",
      "[259 / 312]: Loss = 0.1498\n",
      "[260 / 312]: Loss = 0.1580\n",
      "[261 / 312]: Loss = 0.1409\n",
      "[262 / 312]: Loss = 0.1465\n",
      "[263 / 312]: Loss = 0.1306\n",
      "[264 / 312]: Loss = 0.1323\n",
      "[265 / 312]: Loss = 0.1715\n",
      "[266 / 312]: Loss = 0.1390\n",
      "[267 / 312]: Loss = 0.1346\n",
      "[268 / 312]: Loss = 0.1848\n",
      "[269 / 312]: Loss = 0.1529\n",
      "[270 / 312]: Loss = 0.1662\n",
      "[271 / 312]: Loss = 0.1514\n",
      "[272 / 312]: Loss = 0.1628\n",
      "[273 / 312]: Loss = 0.1612\n",
      "[274 / 312]: Loss = 0.1267\n",
      "[275 / 312]: Loss = 0.1382\n",
      "[276 / 312]: Loss = 0.1668\n",
      "[277 / 312]: Loss = 0.1496\n",
      "[278 / 312]: Loss = 0.1457\n",
      "[279 / 312]: Loss = 0.1230\n",
      "[280 / 312]: Loss = 0.1371\n",
      "[281 / 312]: Loss = 0.1648\n",
      "[282 / 312]: Loss = 0.1615\n",
      "[283 / 312]: Loss = 0.1576\n",
      "[284 / 312]: Loss = 0.1422\n",
      "[285 / 312]: Loss = 0.1199\n",
      "[286 / 312]: Loss = 0.1484\n",
      "[287 / 312]: Loss = 0.1349\n",
      "[288 / 312]: Loss = 0.1539\n",
      "[289 / 312]: Loss = 0.1401\n",
      "[290 / 312]: Loss = 0.1461\n",
      "[291 / 312]: Loss = 0.1445\n",
      "[292 / 312]: Loss = 0.1577\n",
      "[293 / 312]: Loss = 0.1661\n",
      "[294 / 312]: Loss = 0.1221\n",
      "[295 / 312]: Loss = 0.1649\n",
      "[296 / 312]: Loss = 0.1521\n",
      "[297 / 312]: Loss = 0.1320\n",
      "[298 / 312]: Loss = 0.1209\n",
      "[299 / 312]: Loss = 0.1645\n",
      "[300 / 312]: Loss = 0.1464\n",
      "[301 / 312]: Loss = 0.1556\n",
      "[302 / 312]: Loss = 0.1322\n",
      "[303 / 312]: Loss = 0.1669\n",
      "[304 / 312]: Loss = 0.1689\n",
      "[305 / 312]: Loss = 0.1364\n",
      "[306 / 312]: Loss = 0.1389\n",
      "[307 / 312]: Loss = 0.1549\n",
      "[308 / 312]: Loss = 0.1395\n",
      "[309 / 312]: Loss = 0.1372\n",
      "[310 / 312]: Loss = 0.1643\n",
      "[311 / 312]: Loss = 0.1567\n",
      "Epoch 1 / 3, Epoch Time = 20.95s: Train Loss = 0.2198, Train AUC = 0.6857\n",
      "[0 / 312]: Loss = 0.1250\n",
      "[1 / 312]: Loss = 0.1603\n",
      "[2 / 312]: Loss = 0.1480\n",
      "[3 / 312]: Loss = 0.1441\n",
      "[4 / 312]: Loss = 0.1271\n",
      "[5 / 312]: Loss = 0.1737\n",
      "[6 / 312]: Loss = 0.1476\n",
      "[7 / 312]: Loss = 0.1549\n",
      "[8 / 312]: Loss = 0.1329\n",
      "[9 / 312]: Loss = 0.1387\n",
      "[10 / 312]: Loss = 0.1251\n",
      "[11 / 312]: Loss = 0.1548\n",
      "[12 / 312]: Loss = 0.1505\n",
      "[13 / 312]: Loss = 0.1773\n",
      "[14 / 312]: Loss = 0.1351\n",
      "[15 / 312]: Loss = 0.1552\n",
      "[16 / 312]: Loss = 0.1206\n",
      "[17 / 312]: Loss = 0.1727\n",
      "[18 / 312]: Loss = 0.1205\n",
      "[19 / 312]: Loss = 0.1680\n",
      "[20 / 312]: Loss = 0.1430\n",
      "[21 / 312]: Loss = 0.1588\n",
      "[22 / 312]: Loss = 0.1223\n",
      "[23 / 312]: Loss = 0.1409\n",
      "[24 / 312]: Loss = 0.1301\n",
      "[25 / 312]: Loss = 0.1473\n",
      "[26 / 312]: Loss = 0.1315\n",
      "[27 / 312]: Loss = 0.1319\n",
      "[28 / 312]: Loss = 0.1697\n",
      "[29 / 312]: Loss = 0.1325\n",
      "[30 / 312]: Loss = 0.1283\n",
      "[31 / 312]: Loss = 0.1544\n",
      "[32 / 312]: Loss = 0.1566\n",
      "[33 / 312]: Loss = 0.1618\n",
      "[34 / 312]: Loss = 0.1386\n",
      "[35 / 312]: Loss = 0.1436\n",
      "[36 / 312]: Loss = 0.1423\n",
      "[37 / 312]: Loss = 0.1447\n",
      "[38 / 312]: Loss = 0.1470\n",
      "[39 / 312]: Loss = 0.1616\n",
      "[40 / 312]: Loss = 0.1275\n",
      "[41 / 312]: Loss = 0.1412\n",
      "[42 / 312]: Loss = 0.1517\n",
      "[43 / 312]: Loss = 0.1343\n",
      "[44 / 312]: Loss = 0.1214\n",
      "[45 / 312]: Loss = 0.1488\n",
      "[46 / 312]: Loss = 0.1619\n",
      "[47 / 312]: Loss = 0.1471\n",
      "[48 / 312]: Loss = 0.1285\n",
      "[49 / 312]: Loss = 0.1790\n",
      "[50 / 312]: Loss = 0.1082\n",
      "[51 / 312]: Loss = 0.1449\n",
      "[52 / 312]: Loss = 0.1422\n",
      "[53 / 312]: Loss = 0.1515\n",
      "[54 / 312]: Loss = 0.1345\n",
      "[55 / 312]: Loss = 0.1365\n",
      "[56 / 312]: Loss = 0.1505\n",
      "[57 / 312]: Loss = 0.1410\n",
      "[58 / 312]: Loss = 0.1792\n",
      "[59 / 312]: Loss = 0.1167\n",
      "[60 / 312]: Loss = 0.1430\n",
      "[61 / 312]: Loss = 0.1523\n",
      "[62 / 312]: Loss = 0.1324\n",
      "[63 / 312]: Loss = 0.1456\n",
      "[64 / 312]: Loss = 0.1493\n",
      "[65 / 312]: Loss = 0.1322\n",
      "[66 / 312]: Loss = 0.1599\n",
      "[67 / 312]: Loss = 0.1742\n",
      "[68 / 312]: Loss = 0.1581\n",
      "[69 / 312]: Loss = 0.1570\n",
      "[70 / 312]: Loss = 0.1328\n",
      "[71 / 312]: Loss = 0.1380\n",
      "[72 / 312]: Loss = 0.1451\n",
      "[73 / 312]: Loss = 0.1335\n",
      "[74 / 312]: Loss = 0.1392\n",
      "[75 / 312]: Loss = 0.1337\n",
      "[76 / 312]: Loss = 0.1554\n",
      "[77 / 312]: Loss = 0.1341\n",
      "[78 / 312]: Loss = 0.1156\n",
      "[79 / 312]: Loss = 0.1434\n",
      "[80 / 312]: Loss = 0.1628\n",
      "[81 / 312]: Loss = 0.1512\n",
      "[82 / 312]: Loss = 0.1550\n",
      "[83 / 312]: Loss = 0.1470\n",
      "[84 / 312]: Loss = 0.1451\n",
      "[85 / 312]: Loss = 0.1340\n",
      "[86 / 312]: Loss = 0.1437\n",
      "[87 / 312]: Loss = 0.1126\n",
      "[88 / 312]: Loss = 0.1316\n",
      "[89 / 312]: Loss = 0.1209\n",
      "[90 / 312]: Loss = 0.1173\n",
      "[91 / 312]: Loss = 0.1572\n",
      "[92 / 312]: Loss = 0.1476\n",
      "[93 / 312]: Loss = 0.1394\n",
      "[94 / 312]: Loss = 0.1320\n",
      "[95 / 312]: Loss = 0.1541\n",
      "[96 / 312]: Loss = 0.1501\n",
      "[97 / 312]: Loss = 0.1146\n",
      "[98 / 312]: Loss = 0.1273\n",
      "[99 / 312]: Loss = 0.1505\n",
      "[100 / 312]: Loss = 0.1205\n",
      "[101 / 312]: Loss = 0.1397\n",
      "[102 / 312]: Loss = 0.1596\n",
      "[103 / 312]: Loss = 0.1346\n",
      "[104 / 312]: Loss = 0.1328\n",
      "[105 / 312]: Loss = 0.1365\n",
      "[106 / 312]: Loss = 0.1590\n",
      "[107 / 312]: Loss = 0.1427\n",
      "[108 / 312]: Loss = 0.1550\n",
      "[109 / 312]: Loss = 0.1655\n",
      "[110 / 312]: Loss = 0.1515\n",
      "[111 / 312]: Loss = 0.1213\n",
      "[112 / 312]: Loss = 0.1539\n",
      "[113 / 312]: Loss = 0.1276\n",
      "[114 / 312]: Loss = 0.1384\n",
      "[115 / 312]: Loss = 0.1275\n",
      "[116 / 312]: Loss = 0.1302\n",
      "[117 / 312]: Loss = 0.1072\n",
      "[118 / 312]: Loss = 0.1508\n",
      "[119 / 312]: Loss = 0.1165\n",
      "[120 / 312]: Loss = 0.1563\n",
      "[121 / 312]: Loss = 0.1449\n",
      "[122 / 312]: Loss = 0.1325\n",
      "[123 / 312]: Loss = 0.1254\n",
      "[124 / 312]: Loss = 0.1513\n",
      "[125 / 312]: Loss = 0.1367\n",
      "[126 / 312]: Loss = 0.1162\n",
      "[127 / 312]: Loss = 0.1251\n",
      "[128 / 312]: Loss = 0.1504\n",
      "[129 / 312]: Loss = 0.1506\n",
      "[130 / 312]: Loss = 0.1660\n",
      "[131 / 312]: Loss = 0.1600\n",
      "[132 / 312]: Loss = 0.1549\n",
      "[133 / 312]: Loss = 0.1474\n",
      "[134 / 312]: Loss = 0.1240\n",
      "[135 / 312]: Loss = 0.1540\n",
      "[136 / 312]: Loss = 0.1391\n",
      "[137 / 312]: Loss = 0.1373\n",
      "[138 / 312]: Loss = 0.1398\n",
      "[139 / 312]: Loss = 0.1495\n",
      "[140 / 312]: Loss = 0.1244\n",
      "[141 / 312]: Loss = 0.1406\n",
      "[142 / 312]: Loss = 0.1359\n",
      "[143 / 312]: Loss = 0.1539\n",
      "[144 / 312]: Loss = 0.1476\n",
      "[145 / 312]: Loss = 0.1456\n",
      "[146 / 312]: Loss = 0.1508\n",
      "[147 / 312]: Loss = 0.1282\n",
      "[148 / 312]: Loss = 0.1079\n",
      "[149 / 312]: Loss = 0.1545\n",
      "[150 / 312]: Loss = 0.1402\n",
      "[151 / 312]: Loss = 0.1567\n",
      "[152 / 312]: Loss = 0.1690\n",
      "[153 / 312]: Loss = 0.1285\n",
      "[154 / 312]: Loss = 0.1450\n",
      "[155 / 312]: Loss = 0.1280\n",
      "[156 / 312]: Loss = 0.1562\n",
      "[157 / 312]: Loss = 0.1424\n",
      "[158 / 312]: Loss = 0.1086\n",
      "[159 / 312]: Loss = 0.1554\n",
      "[160 / 312]: Loss = 0.1453\n",
      "[161 / 312]: Loss = 0.1414\n",
      "[162 / 312]: Loss = 0.1589\n",
      "[163 / 312]: Loss = 0.1592\n",
      "[164 / 312]: Loss = 0.1425\n",
      "[165 / 312]: Loss = 0.1421\n",
      "[166 / 312]: Loss = 0.1687\n",
      "[167 / 312]: Loss = 0.1352\n",
      "[168 / 312]: Loss = 0.1246\n",
      "[169 / 312]: Loss = 0.1214\n",
      "[170 / 312]: Loss = 0.1568\n",
      "[171 / 312]: Loss = 0.1228\n",
      "[172 / 312]: Loss = 0.1446\n",
      "[173 / 312]: Loss = 0.1300\n",
      "[174 / 312]: Loss = 0.1250\n",
      "[175 / 312]: Loss = 0.1346\n",
      "[176 / 312]: Loss = 0.1468\n",
      "[177 / 312]: Loss = 0.1394\n",
      "[178 / 312]: Loss = 0.1200\n",
      "[179 / 312]: Loss = 0.1384\n",
      "[180 / 312]: Loss = 0.1381\n",
      "[181 / 312]: Loss = 0.1631\n",
      "[182 / 312]: Loss = 0.1264\n",
      "[183 / 312]: Loss = 0.1623\n",
      "[184 / 312]: Loss = 0.1534\n",
      "[185 / 312]: Loss = 0.1402\n",
      "[186 / 312]: Loss = 0.1460\n",
      "[187 / 312]: Loss = 0.1617\n",
      "[188 / 312]: Loss = 0.1326\n",
      "[189 / 312]: Loss = 0.1335\n",
      "[190 / 312]: Loss = 0.1112\n",
      "[191 / 312]: Loss = 0.1594\n",
      "[192 / 312]: Loss = 0.1407\n",
      "[193 / 312]: Loss = 0.1302\n",
      "[194 / 312]: Loss = 0.1170\n",
      "[195 / 312]: Loss = 0.1358\n",
      "[196 / 312]: Loss = 0.1473\n",
      "[197 / 312]: Loss = 0.1203\n",
      "[198 / 312]: Loss = 0.1480\n",
      "[199 / 312]: Loss = 0.1497\n",
      "[200 / 312]: Loss = 0.1269\n",
      "[201 / 312]: Loss = 0.1314\n",
      "[202 / 312]: Loss = 0.1673\n",
      "[203 / 312]: Loss = 0.1326\n",
      "[204 / 312]: Loss = 0.1393\n",
      "[205 / 312]: Loss = 0.1456\n",
      "[206 / 312]: Loss = 0.1243\n",
      "[207 / 312]: Loss = 0.1365\n",
      "[208 / 312]: Loss = 0.1377\n",
      "[209 / 312]: Loss = 0.1501\n",
      "[210 / 312]: Loss = 0.1255\n",
      "[211 / 312]: Loss = 0.1210\n",
      "[212 / 312]: Loss = 0.1361\n",
      "[213 / 312]: Loss = 0.1172\n",
      "[214 / 312]: Loss = 0.1693\n",
      "[215 / 312]: Loss = 0.1240\n",
      "[216 / 312]: Loss = 0.1152\n",
      "[217 / 312]: Loss = 0.1392\n",
      "[218 / 312]: Loss = 0.1227\n",
      "[219 / 312]: Loss = 0.1307\n",
      "[220 / 312]: Loss = 0.1337\n",
      "[221 / 312]: Loss = 0.1379\n",
      "[222 / 312]: Loss = 0.1533\n",
      "[223 / 312]: Loss = 0.1320\n",
      "[224 / 312]: Loss = 0.1391\n",
      "[225 / 312]: Loss = 0.1408\n",
      "[226 / 312]: Loss = 0.1385\n",
      "[227 / 312]: Loss = 0.1133\n",
      "[228 / 312]: Loss = 0.1451\n",
      "[229 / 312]: Loss = 0.1838\n",
      "[230 / 312]: Loss = 0.1098\n",
      "[231 / 312]: Loss = 0.1196\n",
      "[232 / 312]: Loss = 0.1050\n",
      "[233 / 312]: Loss = 0.1178\n",
      "[234 / 312]: Loss = 0.1221\n",
      "[235 / 312]: Loss = 0.1396\n",
      "[236 / 312]: Loss = 0.1251\n",
      "[237 / 312]: Loss = 0.1228\n",
      "[238 / 312]: Loss = 0.1308\n",
      "[239 / 312]: Loss = 0.1522\n",
      "[240 / 312]: Loss = 0.1465\n",
      "[241 / 312]: Loss = 0.1424\n",
      "[242 / 312]: Loss = 0.1266\n",
      "[243 / 312]: Loss = 0.1246\n",
      "[244 / 312]: Loss = 0.1462\n",
      "[245 / 312]: Loss = 0.1373\n",
      "[246 / 312]: Loss = 0.1285\n",
      "[247 / 312]: Loss = 0.1427\n",
      "[248 / 312]: Loss = 0.1639\n",
      "[249 / 312]: Loss = 0.1270\n",
      "[250 / 312]: Loss = 0.1665\n",
      "[251 / 312]: Loss = 0.1327\n",
      "[252 / 312]: Loss = 0.1318\n",
      "[253 / 312]: Loss = 0.1282\n",
      "[254 / 312]: Loss = 0.1280\n",
      "[255 / 312]: Loss = 0.1269\n",
      "[256 / 312]: Loss = 0.1032\n",
      "[257 / 312]: Loss = 0.1662\n",
      "[258 / 312]: Loss = 0.0968\n",
      "[259 / 312]: Loss = 0.1483\n",
      "[260 / 312]: Loss = 0.1212\n",
      "[261 / 312]: Loss = 0.1043\n",
      "[262 / 312]: Loss = 0.1362\n",
      "[263 / 312]: Loss = 0.1286\n",
      "[264 / 312]: Loss = 0.1312\n",
      "[265 / 312]: Loss = 0.1070\n",
      "[266 / 312]: Loss = 0.1239\n",
      "[267 / 312]: Loss = 0.1567\n",
      "[268 / 312]: Loss = 0.1380\n",
      "[269 / 312]: Loss = 0.1427\n",
      "[270 / 312]: Loss = 0.1625\n",
      "[271 / 312]: Loss = 0.1195\n",
      "[272 / 312]: Loss = 0.1200\n",
      "[273 / 312]: Loss = 0.1250\n",
      "[274 / 312]: Loss = 0.1212\n",
      "[275 / 312]: Loss = 0.1253\n",
      "[276 / 312]: Loss = 0.1318\n",
      "[277 / 312]: Loss = 0.1293\n",
      "[278 / 312]: Loss = 0.1457\n",
      "[279 / 312]: Loss = 0.1259\n",
      "[280 / 312]: Loss = 0.1778\n",
      "[281 / 312]: Loss = 0.1153\n",
      "[282 / 312]: Loss = 0.1714\n",
      "[283 / 312]: Loss = 0.1280\n",
      "[284 / 312]: Loss = 0.1286\n",
      "[285 / 312]: Loss = 0.1363\n",
      "[286 / 312]: Loss = 0.1502\n",
      "[287 / 312]: Loss = 0.1442\n",
      "[288 / 312]: Loss = 0.1288\n",
      "[289 / 312]: Loss = 0.1380\n",
      "[290 / 312]: Loss = 0.1196\n",
      "[291 / 312]: Loss = 0.1431\n",
      "[292 / 312]: Loss = 0.1372\n",
      "[293 / 312]: Loss = 0.1395\n",
      "[294 / 312]: Loss = 0.1408\n",
      "[295 / 312]: Loss = 0.0911\n",
      "[296 / 312]: Loss = 0.1166\n",
      "[297 / 312]: Loss = 0.1482\n",
      "[298 / 312]: Loss = 0.1638\n",
      "[299 / 312]: Loss = 0.1410\n",
      "[300 / 312]: Loss = 0.1456\n",
      "[301 / 312]: Loss = 0.1212\n",
      "[302 / 312]: Loss = 0.1227\n",
      "[303 / 312]: Loss = 0.1484\n",
      "[304 / 312]: Loss = 0.1572\n",
      "[305 / 312]: Loss = 0.1302\n",
      "[306 / 312]: Loss = 0.1338\n",
      "[307 / 312]: Loss = 0.1316\n",
      "[308 / 312]: Loss = 0.1402\n",
      "[309 / 312]: Loss = 0.1541\n",
      "[310 / 312]: Loss = 0.1082\n",
      "[311 / 312]: Loss = 0.1474\n",
      "Epoch 2 / 3, Epoch Time = 20.94s: Train Loss = 0.1393, Train AUC = 0.8159\n",
      "[0 / 312]: Loss = 0.1156\n",
      "[1 / 312]: Loss = 0.1383\n",
      "[2 / 312]: Loss = 0.1231\n",
      "[3 / 312]: Loss = 0.1313\n",
      "[4 / 312]: Loss = 0.1362\n",
      "[5 / 312]: Loss = 0.1171\n",
      "[6 / 312]: Loss = 0.1095\n",
      "[7 / 312]: Loss = 0.1159\n",
      "[8 / 312]: Loss = 0.1120\n",
      "[9 / 312]: Loss = 0.1252\n",
      "[10 / 312]: Loss = 0.1303\n",
      "[11 / 312]: Loss = 0.1356\n",
      "[12 / 312]: Loss = 0.1374\n",
      "[13 / 312]: Loss = 0.1284\n",
      "[14 / 312]: Loss = 0.1273\n",
      "[15 / 312]: Loss = 0.1313\n",
      "[16 / 312]: Loss = 0.1594\n",
      "[17 / 312]: Loss = 0.1273\n",
      "[18 / 312]: Loss = 0.1331\n",
      "[19 / 312]: Loss = 0.1383\n",
      "[20 / 312]: Loss = 0.1270\n",
      "[21 / 312]: Loss = 0.1244\n",
      "[22 / 312]: Loss = 0.1208\n",
      "[23 / 312]: Loss = 0.1170\n",
      "[24 / 312]: Loss = 0.1290\n",
      "[25 / 312]: Loss = 0.1460\n",
      "[26 / 312]: Loss = 0.1298\n",
      "[27 / 312]: Loss = 0.1371\n",
      "[28 / 312]: Loss = 0.1161\n",
      "[29 / 312]: Loss = 0.1183\n",
      "[30 / 312]: Loss = 0.1541\n",
      "[31 / 312]: Loss = 0.1401\n",
      "[32 / 312]: Loss = 0.1405\n",
      "[33 / 312]: Loss = 0.1383\n",
      "[34 / 312]: Loss = 0.1295\n",
      "[35 / 312]: Loss = 0.1129\n",
      "[36 / 312]: Loss = 0.1442\n",
      "[37 / 312]: Loss = 0.1201\n",
      "[38 / 312]: Loss = 0.1428\n",
      "[39 / 312]: Loss = 0.1245\n",
      "[40 / 312]: Loss = 0.1257\n",
      "[41 / 312]: Loss = 0.1422\n",
      "[42 / 312]: Loss = 0.1182\n",
      "[43 / 312]: Loss = 0.1828\n",
      "[44 / 312]: Loss = 0.1366\n",
      "[45 / 312]: Loss = 0.1465\n",
      "[46 / 312]: Loss = 0.1224\n",
      "[47 / 312]: Loss = 0.1181\n",
      "[48 / 312]: Loss = 0.1308\n",
      "[49 / 312]: Loss = 0.1024\n",
      "[50 / 312]: Loss = 0.1374\n",
      "[51 / 312]: Loss = 0.1433\n",
      "[52 / 312]: Loss = 0.1162\n",
      "[53 / 312]: Loss = 0.1305\n",
      "[54 / 312]: Loss = 0.1254\n",
      "[55 / 312]: Loss = 0.1510\n",
      "[56 / 312]: Loss = 0.1523\n",
      "[57 / 312]: Loss = 0.1543\n",
      "[58 / 312]: Loss = 0.1219\n",
      "[59 / 312]: Loss = 0.1142\n",
      "[60 / 312]: Loss = 0.1289\n",
      "[61 / 312]: Loss = 0.1400\n",
      "[62 / 312]: Loss = 0.1488\n",
      "[63 / 312]: Loss = 0.1278\n",
      "[64 / 312]: Loss = 0.1295\n",
      "[65 / 312]: Loss = 0.1407\n",
      "[66 / 312]: Loss = 0.1480\n",
      "[67 / 312]: Loss = 0.1494\n",
      "[68 / 312]: Loss = 0.1409\n",
      "[69 / 312]: Loss = 0.1203\n",
      "[70 / 312]: Loss = 0.1291\n",
      "[71 / 312]: Loss = 0.1447\n",
      "[72 / 312]: Loss = 0.1225\n",
      "[73 / 312]: Loss = 0.1394\n",
      "[74 / 312]: Loss = 0.1206\n",
      "[75 / 312]: Loss = 0.1125\n",
      "[76 / 312]: Loss = 0.1536\n",
      "[77 / 312]: Loss = 0.1156\n",
      "[78 / 312]: Loss = 0.1283\n",
      "[79 / 312]: Loss = 0.1277\n",
      "[80 / 312]: Loss = 0.1150\n",
      "[81 / 312]: Loss = 0.1399\n",
      "[82 / 312]: Loss = 0.1175\n",
      "[83 / 312]: Loss = 0.1398\n",
      "[84 / 312]: Loss = 0.1208\n",
      "[85 / 312]: Loss = 0.1458\n",
      "[86 / 312]: Loss = 0.1363\n",
      "[87 / 312]: Loss = 0.1047\n",
      "[88 / 312]: Loss = 0.1280\n",
      "[89 / 312]: Loss = 0.1095\n",
      "[90 / 312]: Loss = 0.1039\n",
      "[91 / 312]: Loss = 0.1150\n",
      "[92 / 312]: Loss = 0.1534\n",
      "[93 / 312]: Loss = 0.1247\n",
      "[94 / 312]: Loss = 0.1532\n",
      "[95 / 312]: Loss = 0.1263\n",
      "[96 / 312]: Loss = 0.1397\n",
      "[97 / 312]: Loss = 0.1256\n",
      "[98 / 312]: Loss = 0.1325\n",
      "[99 / 312]: Loss = 0.1220\n",
      "[100 / 312]: Loss = 0.1160\n",
      "[101 / 312]: Loss = 0.1213\n",
      "[102 / 312]: Loss = 0.1232\n",
      "[103 / 312]: Loss = 0.1511\n",
      "[104 / 312]: Loss = 0.1373\n",
      "[105 / 312]: Loss = 0.1293\n",
      "[106 / 312]: Loss = 0.1256\n",
      "[107 / 312]: Loss = 0.1441\n",
      "[108 / 312]: Loss = 0.1418\n",
      "[109 / 312]: Loss = 0.1143\n",
      "[110 / 312]: Loss = 0.1346\n",
      "[111 / 312]: Loss = 0.1274\n",
      "[112 / 312]: Loss = 0.1250\n",
      "[113 / 312]: Loss = 0.1450\n",
      "[114 / 312]: Loss = 0.1432\n",
      "[115 / 312]: Loss = 0.1000\n",
      "[116 / 312]: Loss = 0.1265\n",
      "[117 / 312]: Loss = 0.1257\n",
      "[118 / 312]: Loss = 0.1305\n",
      "[119 / 312]: Loss = 0.1300\n",
      "[120 / 312]: Loss = 0.1231\n",
      "[121 / 312]: Loss = 0.1203\n",
      "[122 / 312]: Loss = 0.1072\n",
      "[123 / 312]: Loss = 0.1384\n",
      "[124 / 312]: Loss = 0.1362\n",
      "[125 / 312]: Loss = 0.1323\n",
      "[126 / 312]: Loss = 0.1247\n",
      "[127 / 312]: Loss = 0.1309\n",
      "[128 / 312]: Loss = 0.1505\n",
      "[129 / 312]: Loss = 0.1314\n",
      "[130 / 312]: Loss = 0.1365\n",
      "[131 / 312]: Loss = 0.1321\n",
      "[132 / 312]: Loss = 0.1246\n",
      "[133 / 312]: Loss = 0.1250\n",
      "[134 / 312]: Loss = 0.1430\n",
      "[135 / 312]: Loss = 0.1023\n",
      "[136 / 312]: Loss = 0.1335\n",
      "[137 / 312]: Loss = 0.1233\n",
      "[138 / 312]: Loss = 0.1275\n",
      "[139 / 312]: Loss = 0.1441\n",
      "[140 / 312]: Loss = 0.1339\n",
      "[141 / 312]: Loss = 0.1283\n",
      "[142 / 312]: Loss = 0.1014\n",
      "[143 / 312]: Loss = 0.1222\n",
      "[144 / 312]: Loss = 0.1427\n",
      "[145 / 312]: Loss = 0.1137\n",
      "[146 / 312]: Loss = 0.1218\n",
      "[147 / 312]: Loss = 0.1170\n",
      "[148 / 312]: Loss = 0.1506\n",
      "[149 / 312]: Loss = 0.0976\n",
      "[150 / 312]: Loss = 0.1342\n",
      "[151 / 312]: Loss = 0.1393\n",
      "[152 / 312]: Loss = 0.1404\n",
      "[153 / 312]: Loss = 0.1364\n",
      "[154 / 312]: Loss = 0.1143\n",
      "[155 / 312]: Loss = 0.1380\n",
      "[156 / 312]: Loss = 0.1289\n",
      "[157 / 312]: Loss = 0.1074\n",
      "[158 / 312]: Loss = 0.1212\n",
      "[159 / 312]: Loss = 0.1542\n",
      "[160 / 312]: Loss = 0.1151\n",
      "[161 / 312]: Loss = 0.1244\n",
      "[162 / 312]: Loss = 0.1155\n",
      "[163 / 312]: Loss = 0.1392\n",
      "[164 / 312]: Loss = 0.1315\n",
      "[165 / 312]: Loss = 0.1400\n",
      "[166 / 312]: Loss = 0.1399\n",
      "[167 / 312]: Loss = 0.1187\n",
      "[168 / 312]: Loss = 0.1282\n",
      "[169 / 312]: Loss = 0.1298\n",
      "[170 / 312]: Loss = 0.1345\n",
      "[171 / 312]: Loss = 0.1095\n",
      "[172 / 312]: Loss = 0.1506\n",
      "[173 / 312]: Loss = 0.1438\n",
      "[174 / 312]: Loss = 0.1339\n",
      "[175 / 312]: Loss = 0.1157\n",
      "[176 / 312]: Loss = 0.1224\n",
      "[177 / 312]: Loss = 0.1245\n",
      "[178 / 312]: Loss = 0.1407\n",
      "[179 / 312]: Loss = 0.1419\n",
      "[180 / 312]: Loss = 0.1595\n",
      "[181 / 312]: Loss = 0.1300\n",
      "[182 / 312]: Loss = 0.1057\n",
      "[183 / 312]: Loss = 0.1268\n",
      "[184 / 312]: Loss = 0.1327\n",
      "[185 / 312]: Loss = 0.1278\n",
      "[186 / 312]: Loss = 0.1459\n",
      "[187 / 312]: Loss = 0.1225\n",
      "[188 / 312]: Loss = 0.1527\n",
      "[189 / 312]: Loss = 0.1207\n",
      "[190 / 312]: Loss = 0.1389\n",
      "[191 / 312]: Loss = 0.1512\n",
      "[192 / 312]: Loss = 0.1383\n",
      "[193 / 312]: Loss = 0.1200\n",
      "[194 / 312]: Loss = 0.1295\n",
      "[195 / 312]: Loss = 0.1241\n",
      "[196 / 312]: Loss = 0.1235\n",
      "[197 / 312]: Loss = 0.1321\n",
      "[198 / 312]: Loss = 0.1232\n",
      "[199 / 312]: Loss = 0.1224\n",
      "[200 / 312]: Loss = 0.1175\n",
      "[201 / 312]: Loss = 0.1324\n",
      "[202 / 312]: Loss = 0.1302\n",
      "[203 / 312]: Loss = 0.1522\n",
      "[204 / 312]: Loss = 0.1573\n",
      "[205 / 312]: Loss = 0.0983\n",
      "[206 / 312]: Loss = 0.1240\n",
      "[207 / 312]: Loss = 0.1241\n",
      "[208 / 312]: Loss = 0.0980\n",
      "[209 / 312]: Loss = 0.1047\n",
      "[210 / 312]: Loss = 0.1217\n",
      "[211 / 312]: Loss = 0.1385\n",
      "[212 / 312]: Loss = 0.1255\n",
      "[213 / 312]: Loss = 0.1080\n",
      "[214 / 312]: Loss = 0.1341\n",
      "[215 / 312]: Loss = 0.1180\n",
      "[216 / 312]: Loss = 0.1351\n",
      "[217 / 312]: Loss = 0.1230\n",
      "[218 / 312]: Loss = 0.1323\n",
      "[219 / 312]: Loss = 0.1367\n",
      "[220 / 312]: Loss = 0.1361\n",
      "[221 / 312]: Loss = 0.1506\n",
      "[222 / 312]: Loss = 0.1393\n",
      "[223 / 312]: Loss = 0.1166\n",
      "[224 / 312]: Loss = 0.1337\n",
      "[225 / 312]: Loss = 0.1129\n",
      "[226 / 312]: Loss = 0.1337\n",
      "[227 / 312]: Loss = 0.1392\n",
      "[228 / 312]: Loss = 0.1054\n",
      "[229 / 312]: Loss = 0.1100\n",
      "[230 / 312]: Loss = 0.1325\n",
      "[231 / 312]: Loss = 0.1133\n",
      "[232 / 312]: Loss = 0.1454\n",
      "[233 / 312]: Loss = 0.1441\n",
      "[234 / 312]: Loss = 0.1706\n",
      "[235 / 312]: Loss = 0.1112\n",
      "[236 / 312]: Loss = 0.1423\n",
      "[237 / 312]: Loss = 0.1263\n",
      "[238 / 312]: Loss = 0.1402\n",
      "[239 / 312]: Loss = 0.1462\n",
      "[240 / 312]: Loss = 0.1171\n",
      "[241 / 312]: Loss = 0.1174\n",
      "[242 / 312]: Loss = 0.1050\n",
      "[243 / 312]: Loss = 0.1350\n",
      "[244 / 312]: Loss = 0.1359\n",
      "[245 / 312]: Loss = 0.1317\n",
      "[246 / 312]: Loss = 0.1177\n",
      "[247 / 312]: Loss = 0.1331\n",
      "[248 / 312]: Loss = 0.1191\n",
      "[249 / 312]: Loss = 0.1447\n",
      "[250 / 312]: Loss = 0.1274\n",
      "[251 / 312]: Loss = 0.1375\n",
      "[252 / 312]: Loss = 0.1302\n",
      "[253 / 312]: Loss = 0.1234\n",
      "[254 / 312]: Loss = 0.1470\n",
      "[255 / 312]: Loss = 0.1071\n",
      "[256 / 312]: Loss = 0.1610\n",
      "[257 / 312]: Loss = 0.1039\n",
      "[258 / 312]: Loss = 0.1211\n",
      "[259 / 312]: Loss = 0.1189\n",
      "[260 / 312]: Loss = 0.1223\n",
      "[261 / 312]: Loss = 0.1500\n",
      "[262 / 312]: Loss = 0.1423\n",
      "[263 / 312]: Loss = 0.1210\n",
      "[264 / 312]: Loss = 0.1184\n",
      "[265 / 312]: Loss = 0.1310\n",
      "[266 / 312]: Loss = 0.1343\n",
      "[267 / 312]: Loss = 0.1097\n",
      "[268 / 312]: Loss = 0.1282\n",
      "[269 / 312]: Loss = 0.1385\n",
      "[270 / 312]: Loss = 0.1366\n",
      "[271 / 312]: Loss = 0.1193\n",
      "[272 / 312]: Loss = 0.1337\n",
      "[273 / 312]: Loss = 0.1326\n",
      "[274 / 312]: Loss = 0.1106\n",
      "[275 / 312]: Loss = 0.1103\n",
      "[276 / 312]: Loss = 0.1223\n",
      "[277 / 312]: Loss = 0.1613\n",
      "[278 / 312]: Loss = 0.1270\n",
      "[279 / 312]: Loss = 0.1327\n",
      "[280 / 312]: Loss = 0.1239\n",
      "[281 / 312]: Loss = 0.1260\n",
      "[282 / 312]: Loss = 0.1373\n",
      "[283 / 312]: Loss = 0.1419\n",
      "[284 / 312]: Loss = 0.1497\n",
      "[285 / 312]: Loss = 0.1100\n",
      "[286 / 312]: Loss = 0.1037\n",
      "[287 / 312]: Loss = 0.1179\n",
      "[288 / 312]: Loss = 0.1325\n",
      "[289 / 312]: Loss = 0.1488\n",
      "[290 / 312]: Loss = 0.1293\n",
      "[291 / 312]: Loss = 0.1131\n",
      "[292 / 312]: Loss = 0.1252\n",
      "[293 / 312]: Loss = 0.1036\n",
      "[294 / 312]: Loss = 0.1315\n",
      "[295 / 312]: Loss = 0.1308\n",
      "[296 / 312]: Loss = 0.1447\n",
      "[297 / 312]: Loss = 0.1348\n",
      "[298 / 312]: Loss = 0.1105\n",
      "[299 / 312]: Loss = 0.1208\n",
      "[300 / 312]: Loss = 0.1261\n",
      "[301 / 312]: Loss = 0.1329\n",
      "[302 / 312]: Loss = 0.1376\n",
      "[303 / 312]: Loss = 0.1275\n",
      "[304 / 312]: Loss = 0.1319\n",
      "[305 / 312]: Loss = 0.1063\n",
      "[306 / 312]: Loss = 0.1169\n",
      "[307 / 312]: Loss = 0.1332\n",
      "[308 / 312]: Loss = 0.1257\n",
      "[309 / 312]: Loss = 0.1416\n",
      "[310 / 312]: Loss = 0.1299\n",
      "[311 / 312]: Loss = 0.1337\n",
      "Epoch 3 / 3, Epoch Time = 20.83s: Train Loss = 0.1294, Train AUC = 0.8545\n"
     ]
    }
   ],
   "source": [
    "model        = TConvolution(len(token_to_id), hidden_dim=64, num_classes=7, PAD_IX=PAD_IX).cuda()\n",
    "criterion    = nn.BCEWithLogitsLoss().cuda()\n",
    "optimizer    = optim.Adam([param for param in model.parameters() if param.requires_grad], lr=0.01)\n",
    "\n",
    "X_train      = as_matrix(train['tokenized_comments'])\n",
    "train_labels = train.loc[:, TARGET_COLS].values\n",
    "\n",
    "# train model\n",
    "fit(model, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    train_data=(X_train, train_labels), \n",
    "    epochs_count=3, \n",
    "    batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate Xtest matrix\n",
    "X_test = as_matrix(test['tokenized_comments'])\n",
    "preds  = predict(model, X_test, batch_size=512)\n",
    "# create generator for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 7)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>decent</th>\n",
       "      <th>tokenized_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Explanation Why the edits made under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>D ' aww ! He matches this background colour I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Hey man , I ' m really not trying to edit war ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\" More I can ' t make any real suggestions on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>You , sir , are my hero . Any chance you remem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  decent  \\\n",
       "0             0        0       0       0              0       1   \n",
       "1             0        0       0       0              0       1   \n",
       "2             0        0       0       0              0       1   \n",
       "3             0        0       0       0              0       1   \n",
       "4             0        0       0       0              0       1   \n",
       "\n",
       "                                  tokenized_comments  \n",
       "0  Explanation Why the edits made under my userna...  \n",
       "1  D ' aww ! He matches this background colour I ...  \n",
       "2  Hey man , I ' m really not trying to edit war ...  \n",
       "3  \" More I can ' t make any real suggestions on ...  \n",
       "4  You , sir , are my hero . Any chance you remem...  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels.iloc[:, 1:] = preds[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels.to_csv('./conv1d_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/home/jupyter/.local/bin/kaggle competitions submit jigsaw-toxic-comment-classification-challenge -f ./conv1.csv -m 'Baseline sub'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
