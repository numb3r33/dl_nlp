{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from collections import Counter\n",
    "\n",
    "SEED = 41\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_PATH        = '../../dl_nlp/data/jigsaw_toxic/raw/'\n",
    "PROCESSED_DATA_PATH  = '../../dl_nlp/data/jigsaw_toxic/processed/' \n",
    "\n",
    "MAX_LEN = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample():\n",
    "    return pd.read_csv(os.path.join(PROCESSED_DATA_PATH, 'train_sample.csv'))\n",
    "\n",
    "def load_full():\n",
    "    train       = pd.read_csv(os.path.join(RAW_DATA_PATH, 'train.csv'))\n",
    "    test        = pd.read_csv(os.path.join(RAW_DATA_PATH, 'test.csv'))\n",
    "    test_labels = pd.read_csv(os.path.join(RAW_DATA_PATH, 'test_labels.csv'))\n",
    "    \n",
    "    return train, test, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.42 s, sys: 332 ms, total: 1.75 s\n",
      "Wall time: 2.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train, _, _ = load_full()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define target columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLS = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tokenizer\n",
    "tokenizer = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.8 s, sys: 336 ms, total: 5.14 s\n",
      "Wall time: 5.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_tokenized_comments = list(map(tokenizer.tokenize, train.comment_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.25 s, sys: 4.38 s, total: 9.63 s\n",
      "Wall time: 9.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train.loc[:, 'tokenized_comments'] = list(map(' '.join, map(tokenizer.tokenize, train.comment_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create word freq mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = Counter()\n",
    "\n",
    "for tok_comments in train_tokenized_comments:\n",
    "    token_counts.update(tok_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary: 32838\n"
     ]
    }
   ],
   "source": [
    "# we can put a threshold on the token frequency to reduce the vocabulary\n",
    "tokens    = {}\n",
    "min_count = 10\n",
    "\n",
    "for token, freq in token_counts.items():\n",
    "    if freq >= min_count:\n",
    "        tokens[token] = freq\n",
    "        \n",
    "print('Size of the vocabulary: {}'.format(len(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wv_embedding_matrix(words):\n",
    "    word2vec_dict = word2vec.KeyedVectors.load_word2vec_format('../../dl_nlp/data/jigsaw_toxic/processed/word2vec.bin.gz', binary=True)\n",
    "    embed_size    = 300\n",
    "\n",
    "    embedding_index = dict()\n",
    "    for word in word2vec_dict.wv.vocab:\n",
    "        embedding_index[word] = word2vec_dict.word_vec(word)\n",
    "\n",
    "    print('Loaded %d word vectors'%(len(embedding_index)))\n",
    "\n",
    "    all_embs          = np.stack(list(embedding_index.values()))\n",
    "    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "    UNK, PAD       = 'UNK', 'PAD'\n",
    "    UNK_IX, PAD_IX = len(words), len(words) + 1\n",
    "\n",
    "    nb_words = len(words) + 2\n",
    "\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "\n",
    "    embed_cnt = 0\n",
    "    for i, word in enumerate(list(words.keys()) + [UNK, PAD]):\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            embed_cnt +=1\n",
    "\n",
    "    print('total embedded ', embed_cnt, ' common words')\n",
    "    del embedding_index\n",
    "    gc.collect()\n",
    "\n",
    "    return embedding_matrix, UNK, PAD, UNK_IX, PAD_IX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token to ID mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token to index (manual)\n",
    "# UNK, PAD       = 'UNK', 'PAD'\n",
    "# UNK_IX, PAD_IX =  0, 1\n",
    "\n",
    "# token_to_id = {UNK: UNK_IX,\n",
    "#                PAD: PAD_IX\n",
    "#               }\n",
    "\n",
    "# for token in tokens.keys():\n",
    "#     token_to_id[token] = len(token_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3000000 word vectors\n",
      "total embedded  29714  common words\n",
      "CPU times: user 1min 59s, sys: 8.26 s, total: 2min 7s\n",
      "Wall time: 2min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# token to index ( word2vec embeddings )\n",
    "embedding_matrix, UNK, PAD, UNK_IX, PAD_IX = load_wv_embedding_matrix(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id      = {word: index for index, word in enumerate(tokens.keys())}\n",
    "token_to_id[UNK] = UNK_IX\n",
    "token_to_id[PAD] = PAD_IX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad Sequences and convert map tokens to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IX, PAD_IX = map(token_to_id.get, [UNK, PAD])\n",
    "\n",
    "def as_matrix(sequences, token_to_id, word_dropout, UNK_IX, PAD_IX, max_len=None):\n",
    "    \"\"\" Convert a list of tokens into a matrix with padding \"\"\"\n",
    "\n",
    "    if isinstance(sequences[0], str):\n",
    "        sequences = list(map(str.split, sequences))\n",
    "\n",
    "    max_len = min(max(map(len, sequences)), max_len or float('inf'))\n",
    "    matrix = np.full((len(sequences), max_len), np.int32(PAD_IX))\n",
    "\n",
    "    for i,seq in enumerate(sequences):\n",
    "        row_ix = [token_to_id.get(word, UNK_IX) for word in seq[:max_len]]\n",
    "        matrix[i, :len(row_ix)] = row_ix\n",
    "\n",
    "    if word_dropout != 0:\n",
    "        matrix = apply_word_dropout(matrix, 1 - word_dropout, replace_with=UNK_IX, pad_ix=PAD_IX)\n",
    "\n",
    "    return matrix\n",
    "\n",
    "def apply_word_dropout(matrix, keep_prop, replace_with, pad_ix):\n",
    "    dropout_mask = np.random.choice(2, np.shape(matrix), p=[keep_prop, 1-keep_prop])\n",
    "    dropout_mask &= matrix != pad_ix\n",
    "    return np.choose(dropout_mask, [matrix, np.full_like(matrix, replace_with)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size =  127656\n",
      "Validation size =  31915\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_val = train_test_split(train, test_size=0.2, random_state=42)\n",
    "data_train.index     = range(len(data_train))\n",
    "data_val.index       = range(len(data_val))\n",
    "\n",
    "print(\"Train size = \", len(data_train))\n",
    "print(\"Validation size = \", len(data_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(matrix, labels, batch_size, predict_mode='train'):\n",
    "    indices = np.arange(len(matrix))\n",
    "    if predict_mode == 'train':\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, len(matrix), batch_size):\n",
    "        end = min(start + batch_size, len(matrix))\n",
    "        \n",
    "        batch_indices = indices[start: end]\n",
    "        X = matrix[batch_indices]\n",
    "        \n",
    "        if predict_mode != 'train': yield X\n",
    "        else: yield X, labels[batch_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "#     def __init__(self, vocab_size, hidden_dim, PAD_IX):\n",
    "#         super(Net, self).__init__()\n",
    "        \n",
    "#         # num feature maps: It is referred to as the number of output channels\n",
    "#         # to be produced by the output kernel.\n",
    "#         self.num_feature_maps = 3 \n",
    "        \n",
    "#         # kernel_size = refers to the height of the kernel\n",
    "#         # by default it the width covers the entire dimension of the word in question\n",
    "#         # height refers to the number of words are convolving over.\n",
    "#         # for example if k = 2, it means we are considering bigrams\n",
    "#         # and k = 3 would indicate that we are moving over trigrams and so on.\n",
    "        \n",
    "#         self.kernel_size = 3\n",
    "        \n",
    "#         self.embedding = nn.Embedding(vocab_size, hidden_dim, padding_idx=PAD_IX)\n",
    "#         self.conv1     = nn.Conv1d(hidden_dim, self.num_feature_maps, self.kernel_size)\n",
    "#         self.relu      = nn.ReLU()\n",
    "#         self.fc        = nn.Linear(3, 6)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         out = self.embedding(x)\n",
    "        \n",
    "#         # shape of the output after passing it through embedding layer is\n",
    "#         # (batch, seq len, channels)\n",
    "#         # channels here refer to the hidden dim\n",
    "#         # but we wan't to convert it into (batch, channels, seq len)\n",
    "#         # so as to make sure our convolution operation works correctly.\n",
    "        \n",
    "#         out = torch.transpose(out, 1, 2)\n",
    "#         out = self.conv1(out)\n",
    "#         out = self.relu(out)\n",
    "        \n",
    "#         # before taking max let's transpose dimension 1 with 2\n",
    "#         out = torch.transpose(out, 1, 2)\n",
    "        \n",
    "#         # take max pooling over time\n",
    "#         out = out.max(dim=1)[0]\n",
    "        \n",
    "#         # pass it to fully connected layer\n",
    "#         out = self.fc(out)\n",
    "        \n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self, weights, vocab_size, hidden_dim, PAD_IX):\n",
    "        super(Net2, self).__init__()\n",
    "        \n",
    "        # num feature maps: It is referred to as the number of output channels\n",
    "        # to be produced by the output kernel.\n",
    "        self.nfms = [100, 100]\n",
    "        \n",
    "        # kernel_size = refers to the height of the kernel\n",
    "        # by default it the width covers the entire dimension of the word in question\n",
    "        # height refers to the number of words are convolving over.\n",
    "        # for example if k = 2, it means we are considering bigrams\n",
    "        # and k = 3 would indicate that we are moving over trigrams and so on.\n",
    "        \n",
    "        self.ks = [4, 5]\n",
    "        \n",
    "        self.embedding        = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.embedding.weight = nn.Parameter(weights)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.conv1     = nn.Conv1d(hidden_dim, self.nfms[0], self.ks[0])\n",
    "        self.conv2     = nn.Conv1d(hidden_dim, self.nfms[1], self.ks[1])\n",
    "#         self.conv3     = nn.Conv1d(hidden_dim, self.nfms[2], self.ks[2])\n",
    "        \n",
    "        \n",
    "        self.relu      = nn.ReLU()\n",
    "        self.fc        = nn.Linear(self.nfms[0] + self.nfms[1], \n",
    "                                   6)\n",
    "        self.dropout   = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        \n",
    "        # shape of the output after passing it through embedding layer is\n",
    "        # (batch, seq len, channels)\n",
    "        # channels here refer to the hidden dim\n",
    "        # but we wan't to convert it into (batch, channels, seq len)\n",
    "        # so as to make sure our convolution operation works correctly.\n",
    "        \n",
    "        out = torch.transpose(out, 1, 2)\n",
    "        \n",
    "        out1 = self.conv1(out)\n",
    "        out1 = self.relu(out1)\n",
    "        \n",
    "        out2 = self.conv2(out)\n",
    "        out2 = self.relu(out2)\n",
    "        \n",
    "#         out3 = self.conv2(out)\n",
    "#         out3 = self.relu(out3)\n",
    "        \n",
    "        # before taking max let's transpose dimension 1 with 2\n",
    "        out1 = torch.transpose(out1, 1, 2)\n",
    "        out2 = torch.transpose(out2, 1, 2)\n",
    "#         out3 = torch.transpose(out3, 1, 2)\n",
    "        \n",
    "        \n",
    "        # take max pooling over time\n",
    "        out1 = out1.max(dim=1)[0]\n",
    "        out2 = out2.max(dim=1)[0]\n",
    "#         out3 = out3.max(dim=1)[0]\n",
    "        \n",
    "        # concatenate outputs from multiple convolutional layers\n",
    "        out = torch.cat((out1, out2), dim=1)\n",
    "        \n",
    "        # pass it through dropout layer as well\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # pass it to fully connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, criterion, data, batch_size, optimizer=None):\n",
    "    epoch_loss, total_size = 0, 0\n",
    "    per_label_preds = [[], [], [], [], [], []]\n",
    "    per_label_true  = [[], [], [], [], [], []]\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    model.train(is_train)\n",
    "    \n",
    "    data, labels = data\n",
    "    batchs_count = math.ceil(data.shape[0] / batch_size)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        for i, (X_batch, y_batch) in enumerate(iterate_batches(data, labels, batch_size)):\n",
    "            X_batch, y_batch = torch.cuda.LongTensor(X_batch), torch.cuda.FloatTensor(y_batch)\n",
    "\n",
    "            logits = model(X_batch)\n",
    "            loss   = criterion(logits, y_batch)\n",
    "            \n",
    "            if is_train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            # convert true target\n",
    "            batch_target = y_batch.cpu().detach().numpy()\n",
    "            logits_cpu   = logits.cpu().detach().numpy()\n",
    "            \n",
    "            # per_label_preds\n",
    "            for j in range(6):\n",
    "                label_preds     = logits_cpu[:, j]\n",
    "                per_label_preds[j].extend(label_preds)\n",
    "                per_label_true[j].extend(batch_target[:, j])\n",
    "                            \n",
    "            # calculate log loss\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            print('\\r[{} / {}]: Loss = {:.4f}'.format(\n",
    "                  i, batchs_count, loss.item(), end=''))\n",
    "    \n",
    "    label_auc = []\n",
    "    \n",
    "    for i in range(6):\n",
    "        label_auc.append(roc_auc_score(per_label_true[i], per_label_preds[i]))\n",
    "    \n",
    "    return epoch_loss / batchs_count, np.mean(label_auc)\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, \n",
    "        batch_size=32, val_data=None, val_batch_size=None):\n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        start_time = time.time()\n",
    "        train_loss, train_auc = do_epoch(\n",
    "            model, criterion, train_data, batch_size, optimizer\n",
    "        )\n",
    "        \n",
    "        output_info = '\\rEpoch {} / {}, Epoch Time = {:.2f}s: Train Loss = {:.4f}, Train AUC = {:.4f}'\n",
    "        if not val_data is None:\n",
    "            val_loss, val_auc   = do_epoch(model, criterion, val_data, val_batch_size, None)\n",
    "            \n",
    "            epoch_time   = time.time() - start_time\n",
    "            output_info += ', Val Loss = {:.4f}, Val AUC = {:.4f}'\n",
    "            print(output_info.format(epoch+1, epochs_count, epoch_time, \n",
    "                                     train_loss,\n",
    "                                     train_auc,\n",
    "                                     val_loss,\n",
    "                                     val_auc\n",
    "                                    ))\n",
    "        else:\n",
    "            epoch_time = time.time() - start_time\n",
    "            print(output_info.format(epoch+1, epochs_count, epoch_time, train_loss, train_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on a single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix = as_matrix(data_train['tokenized_comments'], \n",
    "#                    token_to_id, \n",
    "#                    word_dropout=0, \n",
    "#                    UNK_IX=UNK_IX, \n",
    "#                    PAD_IX=PAD_IX\n",
    "#                   )\n",
    "\n",
    "# labels = data_train.loc[:, TARGET_COLS].values\n",
    "# X, y   = next(iterate_batches(matrix, labels, batch_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = torch.cuda.LongTensor(X)\n",
    "# y = torch.cuda.LongTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size = len(token_to_id)\n",
    "# hidden_dim = 300\n",
    "\n",
    "# model = Net2(torch.FloatTensor(embedding_matrix), \n",
    "#              vocab_size, \n",
    "#              hidden_dim, \n",
    "#              PAD_IX).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5611,  0.0108, -0.4054, -0.6412, -0.1093,  0.0689],\n",
      "        [ 0.5287,  0.0313, -0.4272, -0.6379, -0.0615,  0.0176]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# logits = model(X)\n",
    "# print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on full batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 / 250]: Loss = 0.7095\n",
      "[1 / 250]: Loss = 0.6211\n",
      "[2 / 250]: Loss = 0.5331\n",
      "[3 / 250]: Loss = 0.4665\n",
      "[4 / 250]: Loss = 0.3823\n",
      "[5 / 250]: Loss = 0.3307\n",
      "[6 / 250]: Loss = 0.2959\n",
      "[7 / 250]: Loss = 0.2467\n",
      "[8 / 250]: Loss = 0.2100\n",
      "[9 / 250]: Loss = 0.1956\n",
      "[10 / 250]: Loss = 0.1514\n",
      "[11 / 250]: Loss = 0.1775\n",
      "[12 / 250]: Loss = 0.1930\n",
      "[13 / 250]: Loss = 0.1665\n",
      "[14 / 250]: Loss = 0.1472\n",
      "[15 / 250]: Loss = 0.1498\n",
      "[16 / 250]: Loss = 0.1461\n",
      "[17 / 250]: Loss = 0.1749\n",
      "[18 / 250]: Loss = 0.1723\n",
      "[19 / 250]: Loss = 0.1508\n",
      "[20 / 250]: Loss = 0.2100\n",
      "[21 / 250]: Loss = 0.1978\n",
      "[22 / 250]: Loss = 0.1897\n",
      "[23 / 250]: Loss = 0.1356\n",
      "[24 / 250]: Loss = 0.1435\n",
      "[25 / 250]: Loss = 0.1727\n",
      "[26 / 250]: Loss = 0.1372\n",
      "[27 / 250]: Loss = 0.1774\n",
      "[28 / 250]: Loss = 0.1718\n",
      "[29 / 250]: Loss = 0.1323\n",
      "[30 / 250]: Loss = 0.1235\n",
      "[31 / 250]: Loss = 0.1527\n",
      "[32 / 250]: Loss = 0.1413\n",
      "[33 / 250]: Loss = 0.1606\n",
      "[34 / 250]: Loss = 0.1312\n",
      "[35 / 250]: Loss = 0.1230\n",
      "[36 / 250]: Loss = 0.1231\n",
      "[37 / 250]: Loss = 0.1394\n",
      "[38 / 250]: Loss = 0.1369\n",
      "[39 / 250]: Loss = 0.1682\n",
      "[40 / 250]: Loss = 0.1064\n",
      "[41 / 250]: Loss = 0.1391\n",
      "[42 / 250]: Loss = 0.1509\n",
      "[43 / 250]: Loss = 0.1412\n",
      "[44 / 250]: Loss = 0.1301\n",
      "[45 / 250]: Loss = 0.1413\n",
      "[46 / 250]: Loss = 0.1165\n",
      "[47 / 250]: Loss = 0.1326\n",
      "[48 / 250]: Loss = 0.1522\n",
      "[49 / 250]: Loss = 0.1368\n",
      "[50 / 250]: Loss = 0.1201\n",
      "[51 / 250]: Loss = 0.1227\n",
      "[52 / 250]: Loss = 0.1107\n",
      "[53 / 250]: Loss = 0.1372\n",
      "[54 / 250]: Loss = 0.1403\n",
      "[55 / 250]: Loss = 0.1441\n",
      "[56 / 250]: Loss = 0.1403\n",
      "[57 / 250]: Loss = 0.1283\n",
      "[58 / 250]: Loss = 0.1200\n",
      "[59 / 250]: Loss = 0.1399\n",
      "[60 / 250]: Loss = 0.1412\n",
      "[61 / 250]: Loss = 0.1692\n",
      "[62 / 250]: Loss = 0.1437\n",
      "[63 / 250]: Loss = 0.1220\n",
      "[64 / 250]: Loss = 0.1044\n",
      "[65 / 250]: Loss = 0.1081\n",
      "[66 / 250]: Loss = 0.1227\n",
      "[67 / 250]: Loss = 0.1135\n",
      "[68 / 250]: Loss = 0.1070\n",
      "[69 / 250]: Loss = 0.1405\n",
      "[70 / 250]: Loss = 0.1319\n",
      "[71 / 250]: Loss = 0.1065\n",
      "[72 / 250]: Loss = 0.1110\n",
      "[73 / 250]: Loss = 0.1285\n",
      "[74 / 250]: Loss = 0.1214\n",
      "[75 / 250]: Loss = 0.1179\n",
      "[76 / 250]: Loss = 0.1220\n",
      "[77 / 250]: Loss = 0.1125\n",
      "[78 / 250]: Loss = 0.1168\n",
      "[79 / 250]: Loss = 0.1255\n",
      "[80 / 250]: Loss = 0.1168\n",
      "[81 / 250]: Loss = 0.1269\n",
      "[82 / 250]: Loss = 0.1227\n",
      "[83 / 250]: Loss = 0.0976\n",
      "[84 / 250]: Loss = 0.1390\n",
      "[85 / 250]: Loss = 0.1206\n",
      "[86 / 250]: Loss = 0.1033\n",
      "[87 / 250]: Loss = 0.1112\n",
      "[88 / 250]: Loss = 0.1086\n",
      "[89 / 250]: Loss = 0.0992\n",
      "[90 / 250]: Loss = 0.1156\n",
      "[91 / 250]: Loss = 0.1040\n",
      "[92 / 250]: Loss = 0.1179\n",
      "[93 / 250]: Loss = 0.1075\n",
      "[94 / 250]: Loss = 0.1016\n",
      "[95 / 250]: Loss = 0.0850\n",
      "[96 / 250]: Loss = 0.0885\n",
      "[97 / 250]: Loss = 0.1046\n",
      "[98 / 250]: Loss = 0.1032\n",
      "[99 / 250]: Loss = 0.0907\n",
      "[100 / 250]: Loss = 0.0981\n",
      "[101 / 250]: Loss = 0.0896\n",
      "[102 / 250]: Loss = 0.0706\n",
      "[103 / 250]: Loss = 0.0863\n",
      "[104 / 250]: Loss = 0.0897\n",
      "[105 / 250]: Loss = 0.0896\n",
      "[106 / 250]: Loss = 0.1029\n",
      "[107 / 250]: Loss = 0.0882\n",
      "[108 / 250]: Loss = 0.0850\n",
      "[109 / 250]: Loss = 0.0942\n",
      "[110 / 250]: Loss = 0.0764\n",
      "[111 / 250]: Loss = 0.0903\n",
      "[112 / 250]: Loss = 0.0989\n",
      "[113 / 250]: Loss = 0.0613\n",
      "[114 / 250]: Loss = 0.0845\n",
      "[115 / 250]: Loss = 0.0934\n",
      "[116 / 250]: Loss = 0.0923\n",
      "[117 / 250]: Loss = 0.1023\n",
      "[118 / 250]: Loss = 0.0904\n",
      "[119 / 250]: Loss = 0.0861\n",
      "[120 / 250]: Loss = 0.0921\n",
      "[121 / 250]: Loss = 0.0693\n",
      "[122 / 250]: Loss = 0.1011\n",
      "[123 / 250]: Loss = 0.0794\n",
      "[124 / 250]: Loss = 0.0798\n",
      "[125 / 250]: Loss = 0.0901\n",
      "[126 / 250]: Loss = 0.0661\n",
      "[127 / 250]: Loss = 0.0697\n",
      "[128 / 250]: Loss = 0.0773\n",
      "[129 / 250]: Loss = 0.0852\n",
      "[130 / 250]: Loss = 0.0767\n",
      "[131 / 250]: Loss = 0.0953\n",
      "[132 / 250]: Loss = 0.1001\n",
      "[133 / 250]: Loss = 0.0682\n",
      "[134 / 250]: Loss = 0.0694\n",
      "[135 / 250]: Loss = 0.0731\n",
      "[136 / 250]: Loss = 0.0850\n",
      "[137 / 250]: Loss = 0.0791\n",
      "[138 / 250]: Loss = 0.0759\n",
      "[139 / 250]: Loss = 0.0637\n",
      "[140 / 250]: Loss = 0.0819\n",
      "[141 / 250]: Loss = 0.0605\n",
      "[142 / 250]: Loss = 0.0595\n",
      "[143 / 250]: Loss = 0.0666\n",
      "[144 / 250]: Loss = 0.0891\n",
      "[145 / 250]: Loss = 0.0842\n",
      "[146 / 250]: Loss = 0.0636\n",
      "[147 / 250]: Loss = 0.0701\n",
      "[148 / 250]: Loss = 0.0821\n",
      "[149 / 250]: Loss = 0.0762\n",
      "[150 / 250]: Loss = 0.0655\n",
      "[151 / 250]: Loss = 0.0710\n",
      "[152 / 250]: Loss = 0.0675\n",
      "[153 / 250]: Loss = 0.0632\n",
      "[154 / 250]: Loss = 0.0773\n",
      "[155 / 250]: Loss = 0.0805\n",
      "[156 / 250]: Loss = 0.0606\n",
      "[157 / 250]: Loss = 0.0836\n",
      "[158 / 250]: Loss = 0.0651\n",
      "[159 / 250]: Loss = 0.0687\n",
      "[160 / 250]: Loss = 0.0688\n",
      "[161 / 250]: Loss = 0.0711\n",
      "[162 / 250]: Loss = 0.0724\n",
      "[163 / 250]: Loss = 0.0726\n",
      "[164 / 250]: Loss = 0.0640\n",
      "[165 / 250]: Loss = 0.0562\n",
      "[166 / 250]: Loss = 0.0689\n",
      "[167 / 250]: Loss = 0.0724\n",
      "[168 / 250]: Loss = 0.0868\n",
      "[169 / 250]: Loss = 0.0653\n",
      "[170 / 250]: Loss = 0.0733\n",
      "[171 / 250]: Loss = 0.0528\n",
      "[172 / 250]: Loss = 0.0787\n",
      "[173 / 250]: Loss = 0.0632\n",
      "[174 / 250]: Loss = 0.0753\n",
      "[175 / 250]: Loss = 0.0595\n",
      "[176 / 250]: Loss = 0.0608\n",
      "[177 / 250]: Loss = 0.0654\n",
      "[178 / 250]: Loss = 0.0839\n",
      "[179 / 250]: Loss = 0.0669\n",
      "[180 / 250]: Loss = 0.0594\n",
      "[181 / 250]: Loss = 0.0674\n",
      "[182 / 250]: Loss = 0.0688\n",
      "[183 / 250]: Loss = 0.0686\n",
      "[184 / 250]: Loss = 0.0656\n",
      "[185 / 250]: Loss = 0.0757\n",
      "[186 / 250]: Loss = 0.0612\n",
      "[187 / 250]: Loss = 0.0837\n",
      "[188 / 250]: Loss = 0.0725\n",
      "[189 / 250]: Loss = 0.0615\n",
      "[190 / 250]: Loss = 0.0765\n",
      "[191 / 250]: Loss = 0.0638\n",
      "[192 / 250]: Loss = 0.0601\n",
      "[193 / 250]: Loss = 0.0773\n",
      "[194 / 250]: Loss = 0.0686\n",
      "[195 / 250]: Loss = 0.0588\n",
      "[196 / 250]: Loss = 0.0736\n",
      "[197 / 250]: Loss = 0.0600\n",
      "[198 / 250]: Loss = 0.0584\n",
      "[199 / 250]: Loss = 0.0600\n",
      "[200 / 250]: Loss = 0.0455\n",
      "[201 / 250]: Loss = 0.0492\n",
      "[202 / 250]: Loss = 0.0780\n",
      "[203 / 250]: Loss = 0.0568\n",
      "[204 / 250]: Loss = 0.0707\n",
      "[205 / 250]: Loss = 0.0595\n",
      "[206 / 250]: Loss = 0.0663\n",
      "[207 / 250]: Loss = 0.0674\n",
      "[208 / 250]: Loss = 0.0626\n",
      "[209 / 250]: Loss = 0.0638\n",
      "[210 / 250]: Loss = 0.0615\n",
      "[211 / 250]: Loss = 0.0615\n",
      "[212 / 250]: Loss = 0.0683\n",
      "[213 / 250]: Loss = 0.0931\n",
      "[214 / 250]: Loss = 0.0799\n",
      "[215 / 250]: Loss = 0.0719\n",
      "[216 / 250]: Loss = 0.0886\n",
      "[217 / 250]: Loss = 0.0606\n",
      "[218 / 250]: Loss = 0.0687\n",
      "[219 / 250]: Loss = 0.0745\n",
      "[220 / 250]: Loss = 0.0742\n",
      "[221 / 250]: Loss = 0.0583\n",
      "[222 / 250]: Loss = 0.0620\n",
      "[223 / 250]: Loss = 0.0886\n",
      "[224 / 250]: Loss = 0.0577\n",
      "[225 / 250]: Loss = 0.0747\n",
      "[226 / 250]: Loss = 0.0749\n",
      "[227 / 250]: Loss = 0.0737\n",
      "[228 / 250]: Loss = 0.0632\n",
      "[229 / 250]: Loss = 0.0558\n",
      "[230 / 250]: Loss = 0.0691\n",
      "[231 / 250]: Loss = 0.0787\n",
      "[232 / 250]: Loss = 0.0624\n",
      "[233 / 250]: Loss = 0.0620\n",
      "[234 / 250]: Loss = 0.0715\n",
      "[235 / 250]: Loss = 0.0646\n",
      "[236 / 250]: Loss = 0.0809\n",
      "[237 / 250]: Loss = 0.0573\n",
      "[238 / 250]: Loss = 0.0653\n",
      "[239 / 250]: Loss = 0.0574\n",
      "[240 / 250]: Loss = 0.0821\n",
      "[241 / 250]: Loss = 0.0806\n",
      "[242 / 250]: Loss = 0.0602\n",
      "[243 / 250]: Loss = 0.0575\n",
      "[244 / 250]: Loss = 0.0696\n",
      "[245 / 250]: Loss = 0.0634\n",
      "[246 / 250]: Loss = 0.0705\n",
      "[247 / 250]: Loss = 0.0578\n",
      "[248 / 250]: Loss = 0.0547\n",
      "[249 / 250]: Loss = 0.0465\n",
      "[0 / 32]: Loss = 0.0618\n",
      "[1 / 32]: Loss = 0.0615\n",
      "[2 / 32]: Loss = 0.0517\n",
      "[3 / 32]: Loss = 0.0605\n",
      "[4 / 32]: Loss = 0.0577\n",
      "[5 / 32]: Loss = 0.0705\n",
      "[6 / 32]: Loss = 0.0512\n",
      "[7 / 32]: Loss = 0.0652\n",
      "[8 / 32]: Loss = 0.0546\n",
      "[9 / 32]: Loss = 0.0587\n",
      "[10 / 32]: Loss = 0.0626\n",
      "[11 / 32]: Loss = 0.0657\n",
      "[12 / 32]: Loss = 0.0705\n",
      "[13 / 32]: Loss = 0.0715\n",
      "[14 / 32]: Loss = 0.0639\n",
      "[15 / 32]: Loss = 0.0671\n",
      "[16 / 32]: Loss = 0.0597\n",
      "[17 / 32]: Loss = 0.0678\n",
      "[18 / 32]: Loss = 0.0490\n",
      "[19 / 32]: Loss = 0.0569\n",
      "[20 / 32]: Loss = 0.0504\n",
      "[21 / 32]: Loss = 0.0618\n",
      "[22 / 32]: Loss = 0.0649\n",
      "[23 / 32]: Loss = 0.0627\n",
      "[24 / 32]: Loss = 0.0453\n",
      "[25 / 32]: Loss = 0.0519\n",
      "[26 / 32]: Loss = 0.0683\n",
      "[27 / 32]: Loss = 0.0591\n",
      "[28 / 32]: Loss = 0.0626\n",
      "[29 / 32]: Loss = 0.0576\n",
      "[30 / 32]: Loss = 0.0730\n",
      "[31 / 32]: Loss = 0.0528\n",
      "Epoch 1 / 5, Epoch Time = 9.27s: Train Loss = 0.1073, Train AUC = 0.7867, Val Loss = 0.0606, Val AUC = 0.9549\n",
      "[0 / 250]: Loss = 0.0699\n",
      "[1 / 250]: Loss = 0.0589\n",
      "[2 / 250]: Loss = 0.0681\n",
      "[3 / 250]: Loss = 0.0592\n",
      "[4 / 250]: Loss = 0.0576\n",
      "[5 / 250]: Loss = 0.0637\n",
      "[6 / 250]: Loss = 0.0658\n",
      "[7 / 250]: Loss = 0.0577\n",
      "[8 / 250]: Loss = 0.0543\n",
      "[9 / 250]: Loss = 0.0454\n",
      "[10 / 250]: Loss = 0.0583\n",
      "[11 / 250]: Loss = 0.0547\n",
      "[12 / 250]: Loss = 0.0570\n",
      "[13 / 250]: Loss = 0.0641\n",
      "[14 / 250]: Loss = 0.0778\n",
      "[15 / 250]: Loss = 0.0605\n",
      "[16 / 250]: Loss = 0.0565\n",
      "[17 / 250]: Loss = 0.0506\n",
      "[18 / 250]: Loss = 0.0593\n",
      "[19 / 250]: Loss = 0.0657\n",
      "[20 / 250]: Loss = 0.0507\n",
      "[21 / 250]: Loss = 0.0636\n",
      "[22 / 250]: Loss = 0.0541\n",
      "[23 / 250]: Loss = 0.0550\n",
      "[24 / 250]: Loss = 0.0570\n",
      "[25 / 250]: Loss = 0.0557\n",
      "[26 / 250]: Loss = 0.0594\n",
      "[27 / 250]: Loss = 0.0665\n",
      "[28 / 250]: Loss = 0.0650\n",
      "[29 / 250]: Loss = 0.0784\n",
      "[30 / 250]: Loss = 0.0586\n",
      "[31 / 250]: Loss = 0.0526\n",
      "[32 / 250]: Loss = 0.0622\n",
      "[33 / 250]: Loss = 0.0598\n",
      "[34 / 250]: Loss = 0.0502\n",
      "[35 / 250]: Loss = 0.0748\n",
      "[36 / 250]: Loss = 0.0557\n",
      "[37 / 250]: Loss = 0.0546\n",
      "[38 / 250]: Loss = 0.0540\n",
      "[39 / 250]: Loss = 0.0725\n",
      "[40 / 250]: Loss = 0.0642\n",
      "[41 / 250]: Loss = 0.0618\n",
      "[42 / 250]: Loss = 0.0571\n",
      "[43 / 250]: Loss = 0.0760\n",
      "[44 / 250]: Loss = 0.0551\n",
      "[45 / 250]: Loss = 0.0576\n",
      "[46 / 250]: Loss = 0.0420\n",
      "[47 / 250]: Loss = 0.0692\n",
      "[48 / 250]: Loss = 0.0488\n",
      "[49 / 250]: Loss = 0.0497\n",
      "[50 / 250]: Loss = 0.0617\n",
      "[51 / 250]: Loss = 0.0580\n",
      "[52 / 250]: Loss = 0.0681\n",
      "[53 / 250]: Loss = 0.0559\n",
      "[54 / 250]: Loss = 0.0636\n",
      "[55 / 250]: Loss = 0.0545\n",
      "[56 / 250]: Loss = 0.0564\n",
      "[57 / 250]: Loss = 0.0608\n",
      "[58 / 250]: Loss = 0.0484\n",
      "[59 / 250]: Loss = 0.0621\n",
      "[60 / 250]: Loss = 0.0592\n",
      "[61 / 250]: Loss = 0.0579\n",
      "[62 / 250]: Loss = 0.0564\n",
      "[63 / 250]: Loss = 0.0823\n",
      "[64 / 250]: Loss = 0.0582\n",
      "[65 / 250]: Loss = 0.0656\n",
      "[66 / 250]: Loss = 0.0543\n",
      "[67 / 250]: Loss = 0.0722\n",
      "[68 / 250]: Loss = 0.0582\n",
      "[69 / 250]: Loss = 0.0450\n",
      "[70 / 250]: Loss = 0.0455\n",
      "[71 / 250]: Loss = 0.0717\n",
      "[72 / 250]: Loss = 0.0614\n",
      "[73 / 250]: Loss = 0.0626\n",
      "[74 / 250]: Loss = 0.0681\n",
      "[75 / 250]: Loss = 0.0665\n",
      "[76 / 250]: Loss = 0.0491\n",
      "[77 / 250]: Loss = 0.0591\n",
      "[78 / 250]: Loss = 0.0543\n",
      "[79 / 250]: Loss = 0.0522\n",
      "[80 / 250]: Loss = 0.0551\n",
      "[81 / 250]: Loss = 0.0631\n",
      "[82 / 250]: Loss = 0.0583\n",
      "[83 / 250]: Loss = 0.0495\n",
      "[84 / 250]: Loss = 0.0629\n",
      "[85 / 250]: Loss = 0.0604\n",
      "[86 / 250]: Loss = 0.0648\n",
      "[87 / 250]: Loss = 0.0479\n",
      "[88 / 250]: Loss = 0.0436\n",
      "[89 / 250]: Loss = 0.0556\n",
      "[90 / 250]: Loss = 0.0670\n",
      "[91 / 250]: Loss = 0.0513\n",
      "[92 / 250]: Loss = 0.0593\n",
      "[93 / 250]: Loss = 0.0548\n",
      "[94 / 250]: Loss = 0.0617\n",
      "[95 / 250]: Loss = 0.0618\n",
      "[96 / 250]: Loss = 0.0616\n",
      "[97 / 250]: Loss = 0.0522\n",
      "[98 / 250]: Loss = 0.0506\n",
      "[99 / 250]: Loss = 0.0537\n",
      "[100 / 250]: Loss = 0.0772\n",
      "[101 / 250]: Loss = 0.0736\n",
      "[102 / 250]: Loss = 0.0576\n",
      "[103 / 250]: Loss = 0.0547\n",
      "[104 / 250]: Loss = 0.0576\n",
      "[105 / 250]: Loss = 0.0641\n",
      "[106 / 250]: Loss = 0.0639\n",
      "[107 / 250]: Loss = 0.0507\n",
      "[108 / 250]: Loss = 0.0504\n",
      "[109 / 250]: Loss = 0.0588\n",
      "[110 / 250]: Loss = 0.0555\n",
      "[111 / 250]: Loss = 0.0703\n",
      "[112 / 250]: Loss = 0.0670\n",
      "[113 / 250]: Loss = 0.0688\n",
      "[114 / 250]: Loss = 0.0625\n",
      "[115 / 250]: Loss = 0.0601\n",
      "[116 / 250]: Loss = 0.0519\n",
      "[117 / 250]: Loss = 0.0530\n",
      "[118 / 250]: Loss = 0.0609\n",
      "[119 / 250]: Loss = 0.0557\n",
      "[120 / 250]: Loss = 0.0641\n",
      "[121 / 250]: Loss = 0.0489\n",
      "[122 / 250]: Loss = 0.0542\n",
      "[123 / 250]: Loss = 0.0672\n",
      "[124 / 250]: Loss = 0.0701\n",
      "[125 / 250]: Loss = 0.0727\n",
      "[126 / 250]: Loss = 0.0654\n",
      "[127 / 250]: Loss = 0.0493\n",
      "[128 / 250]: Loss = 0.0609\n",
      "[129 / 250]: Loss = 0.0525\n",
      "[130 / 250]: Loss = 0.0555\n",
      "[131 / 250]: Loss = 0.0578\n",
      "[132 / 250]: Loss = 0.0638\n",
      "[133 / 250]: Loss = 0.0595\n",
      "[134 / 250]: Loss = 0.0604\n",
      "[135 / 250]: Loss = 0.0557\n",
      "[136 / 250]: Loss = 0.0670\n",
      "[137 / 250]: Loss = 0.0433\n",
      "[138 / 250]: Loss = 0.0527\n",
      "[139 / 250]: Loss = 0.0622\n",
      "[140 / 250]: Loss = 0.0658\n",
      "[141 / 250]: Loss = 0.0604\n",
      "[142 / 250]: Loss = 0.0484\n",
      "[143 / 250]: Loss = 0.0565\n",
      "[144 / 250]: Loss = 0.0535\n",
      "[145 / 250]: Loss = 0.0705\n",
      "[146 / 250]: Loss = 0.0509\n",
      "[147 / 250]: Loss = 0.0378\n",
      "[148 / 250]: Loss = 0.0723\n",
      "[149 / 250]: Loss = 0.0427\n",
      "[150 / 250]: Loss = 0.0502\n",
      "[151 / 250]: Loss = 0.0522\n",
      "[152 / 250]: Loss = 0.0532\n",
      "[153 / 250]: Loss = 0.0611\n",
      "[154 / 250]: Loss = 0.0478\n",
      "[155 / 250]: Loss = 0.0581\n",
      "[156 / 250]: Loss = 0.0664\n",
      "[157 / 250]: Loss = 0.0553\n",
      "[158 / 250]: Loss = 0.0706\n",
      "[159 / 250]: Loss = 0.0636\n",
      "[160 / 250]: Loss = 0.0605\n",
      "[161 / 250]: Loss = 0.0711\n",
      "[162 / 250]: Loss = 0.0490\n",
      "[163 / 250]: Loss = 0.0588\n",
      "[164 / 250]: Loss = 0.0347\n",
      "[165 / 250]: Loss = 0.0541\n",
      "[166 / 250]: Loss = 0.0519\n",
      "[167 / 250]: Loss = 0.0503\n",
      "[168 / 250]: Loss = 0.0702\n",
      "[169 / 250]: Loss = 0.0630\n",
      "[170 / 250]: Loss = 0.0580\n",
      "[171 / 250]: Loss = 0.0554\n",
      "[172 / 250]: Loss = 0.0430\n",
      "[173 / 250]: Loss = 0.0608\n",
      "[174 / 250]: Loss = 0.0508\n",
      "[175 / 250]: Loss = 0.0495\n",
      "[176 / 250]: Loss = 0.0571\n",
      "[177 / 250]: Loss = 0.0774\n",
      "[178 / 250]: Loss = 0.0627\n",
      "[179 / 250]: Loss = 0.0591\n",
      "[180 / 250]: Loss = 0.0502\n",
      "[181 / 250]: Loss = 0.0667\n",
      "[182 / 250]: Loss = 0.0609\n",
      "[183 / 250]: Loss = 0.0534\n",
      "[184 / 250]: Loss = 0.0531\n",
      "[185 / 250]: Loss = 0.0592\n",
      "[186 / 250]: Loss = 0.0607\n",
      "[187 / 250]: Loss = 0.0544\n",
      "[188 / 250]: Loss = 0.0624\n",
      "[189 / 250]: Loss = 0.0558\n",
      "[190 / 250]: Loss = 0.0470\n",
      "[191 / 250]: Loss = 0.0574\n",
      "[192 / 250]: Loss = 0.0631\n",
      "[193 / 250]: Loss = 0.0559\n",
      "[194 / 250]: Loss = 0.0664\n",
      "[195 / 250]: Loss = 0.0485\n",
      "[196 / 250]: Loss = 0.0557\n",
      "[197 / 250]: Loss = 0.0535\n",
      "[198 / 250]: Loss = 0.0671\n",
      "[199 / 250]: Loss = 0.0413\n",
      "[200 / 250]: Loss = 0.0606\n",
      "[201 / 250]: Loss = 0.0598\n",
      "[202 / 250]: Loss = 0.0461\n",
      "[203 / 250]: Loss = 0.0547\n",
      "[204 / 250]: Loss = 0.0714\n",
      "[205 / 250]: Loss = 0.0637\n",
      "[206 / 250]: Loss = 0.0588\n",
      "[207 / 250]: Loss = 0.0577\n",
      "[208 / 250]: Loss = 0.0588\n",
      "[209 / 250]: Loss = 0.0458\n",
      "[210 / 250]: Loss = 0.0592\n",
      "[211 / 250]: Loss = 0.0784\n",
      "[212 / 250]: Loss = 0.0526\n",
      "[213 / 250]: Loss = 0.0593\n",
      "[214 / 250]: Loss = 0.0585\n",
      "[215 / 250]: Loss = 0.0681\n",
      "[216 / 250]: Loss = 0.0611\n",
      "[217 / 250]: Loss = 0.0477\n",
      "[218 / 250]: Loss = 0.0511\n",
      "[219 / 250]: Loss = 0.0725\n",
      "[220 / 250]: Loss = 0.0722\n",
      "[221 / 250]: Loss = 0.0529\n",
      "[222 / 250]: Loss = 0.0689\n",
      "[223 / 250]: Loss = 0.0728\n",
      "[224 / 250]: Loss = 0.0530\n",
      "[225 / 250]: Loss = 0.0476\n",
      "[226 / 250]: Loss = 0.0555\n",
      "[227 / 250]: Loss = 0.0377\n",
      "[228 / 250]: Loss = 0.0755\n",
      "[229 / 250]: Loss = 0.0577\n",
      "[230 / 250]: Loss = 0.0682\n",
      "[231 / 250]: Loss = 0.0564\n",
      "[232 / 250]: Loss = 0.0500\n",
      "[233 / 250]: Loss = 0.0635\n",
      "[234 / 250]: Loss = 0.0544\n",
      "[235 / 250]: Loss = 0.0826\n",
      "[236 / 250]: Loss = 0.0530\n",
      "[237 / 250]: Loss = 0.0612\n",
      "[238 / 250]: Loss = 0.0717\n",
      "[239 / 250]: Loss = 0.0660\n",
      "[240 / 250]: Loss = 0.0505\n",
      "[241 / 250]: Loss = 0.0452\n",
      "[242 / 250]: Loss = 0.0601\n",
      "[243 / 250]: Loss = 0.0635\n",
      "[244 / 250]: Loss = 0.0729\n",
      "[245 / 250]: Loss = 0.0453\n",
      "[246 / 250]: Loss = 0.0645\n",
      "[247 / 250]: Loss = 0.0686\n",
      "[248 / 250]: Loss = 0.0568\n",
      "[249 / 250]: Loss = 0.0668\n",
      "[0 / 32]: Loss = 0.0453\n",
      "[1 / 32]: Loss = 0.0533\n",
      "[2 / 32]: Loss = 0.0522\n",
      "[3 / 32]: Loss = 0.0461\n",
      "[4 / 32]: Loss = 0.0598\n",
      "[5 / 32]: Loss = 0.0515\n",
      "[6 / 32]: Loss = 0.0566\n",
      "[7 / 32]: Loss = 0.0485\n",
      "[8 / 32]: Loss = 0.0610\n",
      "[9 / 32]: Loss = 0.0585\n",
      "[10 / 32]: Loss = 0.0622\n",
      "[11 / 32]: Loss = 0.0586\n",
      "[12 / 32]: Loss = 0.0503\n",
      "[13 / 32]: Loss = 0.0587\n",
      "[14 / 32]: Loss = 0.0531\n",
      "[15 / 32]: Loss = 0.0565\n",
      "[16 / 32]: Loss = 0.0570\n",
      "[17 / 32]: Loss = 0.0494\n",
      "[18 / 32]: Loss = 0.0547\n",
      "[19 / 32]: Loss = 0.0597\n",
      "[20 / 32]: Loss = 0.0573\n",
      "[21 / 32]: Loss = 0.0548\n",
      "[22 / 32]: Loss = 0.0582\n",
      "[23 / 32]: Loss = 0.0488\n",
      "[24 / 32]: Loss = 0.0520\n",
      "[25 / 32]: Loss = 0.0616\n",
      "[26 / 32]: Loss = 0.0559\n",
      "[27 / 32]: Loss = 0.0531\n",
      "[28 / 32]: Loss = 0.0543\n",
      "[29 / 32]: Loss = 0.0520\n",
      "[30 / 32]: Loss = 0.0531\n",
      "[31 / 32]: Loss = 0.0771\n",
      "Epoch 2 / 5, Epoch Time = 9.27s: Train Loss = 0.0589, Train AUC = 0.9620, Val Loss = 0.0554, Val AUC = 0.9702\n",
      "[0 / 250]: Loss = 0.0585\n",
      "[1 / 250]: Loss = 0.0555\n",
      "[2 / 250]: Loss = 0.0422\n",
      "[3 / 250]: Loss = 0.0508\n",
      "[4 / 250]: Loss = 0.0441\n",
      "[5 / 250]: Loss = 0.0532\n",
      "[6 / 250]: Loss = 0.0430\n",
      "[7 / 250]: Loss = 0.0653\n",
      "[8 / 250]: Loss = 0.0532\n",
      "[9 / 250]: Loss = 0.0419\n",
      "[10 / 250]: Loss = 0.0413\n",
      "[11 / 250]: Loss = 0.0553\n",
      "[12 / 250]: Loss = 0.0766\n",
      "[13 / 250]: Loss = 0.0556\n",
      "[14 / 250]: Loss = 0.0562\n",
      "[15 / 250]: Loss = 0.0700\n",
      "[16 / 250]: Loss = 0.0564\n",
      "[17 / 250]: Loss = 0.0579\n",
      "[18 / 250]: Loss = 0.0682\n",
      "[19 / 250]: Loss = 0.0657\n",
      "[20 / 250]: Loss = 0.0681\n",
      "[21 / 250]: Loss = 0.0674\n",
      "[22 / 250]: Loss = 0.0461\n",
      "[23 / 250]: Loss = 0.0473\n",
      "[24 / 250]: Loss = 0.0494\n",
      "[25 / 250]: Loss = 0.0627\n",
      "[26 / 250]: Loss = 0.0435\n",
      "[27 / 250]: Loss = 0.0541\n",
      "[28 / 250]: Loss = 0.0470\n",
      "[29 / 250]: Loss = 0.0708\n",
      "[30 / 250]: Loss = 0.0519\n",
      "[31 / 250]: Loss = 0.0487\n",
      "[32 / 250]: Loss = 0.0657\n",
      "[33 / 250]: Loss = 0.0495\n",
      "[34 / 250]: Loss = 0.0519\n",
      "[35 / 250]: Loss = 0.0481\n",
      "[36 / 250]: Loss = 0.0465\n",
      "[37 / 250]: Loss = 0.0624\n",
      "[38 / 250]: Loss = 0.0552\n",
      "[39 / 250]: Loss = 0.0578\n",
      "[40 / 250]: Loss = 0.0581\n",
      "[41 / 250]: Loss = 0.0592\n",
      "[42 / 250]: Loss = 0.0649\n",
      "[43 / 250]: Loss = 0.0566\n",
      "[44 / 250]: Loss = 0.0484\n",
      "[45 / 250]: Loss = 0.0452\n",
      "[46 / 250]: Loss = 0.0534\n",
      "[47 / 250]: Loss = 0.0521\n",
      "[48 / 250]: Loss = 0.0631\n",
      "[49 / 250]: Loss = 0.0482\n",
      "[50 / 250]: Loss = 0.0510\n",
      "[51 / 250]: Loss = 0.0577\n",
      "[52 / 250]: Loss = 0.0617\n",
      "[53 / 250]: Loss = 0.0561\n",
      "[54 / 250]: Loss = 0.0590\n",
      "[55 / 250]: Loss = 0.0545\n",
      "[56 / 250]: Loss = 0.0582\n",
      "[57 / 250]: Loss = 0.0532\n",
      "[58 / 250]: Loss = 0.0396\n",
      "[59 / 250]: Loss = 0.0573\n",
      "[60 / 250]: Loss = 0.0574\n",
      "[61 / 250]: Loss = 0.0593\n",
      "[62 / 250]: Loss = 0.0501\n",
      "[63 / 250]: Loss = 0.0456\n",
      "[64 / 250]: Loss = 0.0521\n",
      "[65 / 250]: Loss = 0.0591\n",
      "[66 / 250]: Loss = 0.0643\n",
      "[67 / 250]: Loss = 0.0516\n",
      "[68 / 250]: Loss = 0.0567\n",
      "[69 / 250]: Loss = 0.0543\n",
      "[70 / 250]: Loss = 0.0410\n",
      "[71 / 250]: Loss = 0.0541\n",
      "[72 / 250]: Loss = 0.0444\n",
      "[73 / 250]: Loss = 0.0528\n",
      "[74 / 250]: Loss = 0.0691\n",
      "[75 / 250]: Loss = 0.0493\n",
      "[76 / 250]: Loss = 0.0421\n",
      "[77 / 250]: Loss = 0.0493\n",
      "[78 / 250]: Loss = 0.0392\n",
      "[79 / 250]: Loss = 0.0725\n",
      "[80 / 250]: Loss = 0.0518\n",
      "[81 / 250]: Loss = 0.0493\n",
      "[82 / 250]: Loss = 0.0564\n",
      "[83 / 250]: Loss = 0.0506\n",
      "[84 / 250]: Loss = 0.0618\n",
      "[85 / 250]: Loss = 0.0554\n",
      "[86 / 250]: Loss = 0.0556\n",
      "[87 / 250]: Loss = 0.0495\n",
      "[88 / 250]: Loss = 0.0517\n",
      "[89 / 250]: Loss = 0.0529\n",
      "[90 / 250]: Loss = 0.0624\n",
      "[91 / 250]: Loss = 0.0561\n",
      "[92 / 250]: Loss = 0.0408\n",
      "[93 / 250]: Loss = 0.0589\n",
      "[94 / 250]: Loss = 0.0542\n",
      "[95 / 250]: Loss = 0.0581\n",
      "[96 / 250]: Loss = 0.0349\n",
      "[97 / 250]: Loss = 0.0534\n",
      "[98 / 250]: Loss = 0.0440\n",
      "[99 / 250]: Loss = 0.0441\n",
      "[100 / 250]: Loss = 0.0520\n",
      "[101 / 250]: Loss = 0.0522\n",
      "[102 / 250]: Loss = 0.0528\n",
      "[103 / 250]: Loss = 0.0530\n",
      "[104 / 250]: Loss = 0.0454\n",
      "[105 / 250]: Loss = 0.0613\n",
      "[106 / 250]: Loss = 0.0555\n",
      "[107 / 250]: Loss = 0.0620\n",
      "[108 / 250]: Loss = 0.0510\n",
      "[109 / 250]: Loss = 0.0569\n",
      "[110 / 250]: Loss = 0.0576\n",
      "[111 / 250]: Loss = 0.0427\n",
      "[112 / 250]: Loss = 0.0631\n",
      "[113 / 250]: Loss = 0.0529\n",
      "[114 / 250]: Loss = 0.0626\n",
      "[115 / 250]: Loss = 0.0496\n",
      "[116 / 250]: Loss = 0.0525\n",
      "[117 / 250]: Loss = 0.0586\n",
      "[118 / 250]: Loss = 0.0548\n",
      "[119 / 250]: Loss = 0.0596\n",
      "[120 / 250]: Loss = 0.0532\n",
      "[121 / 250]: Loss = 0.0503\n",
      "[122 / 250]: Loss = 0.0606\n",
      "[123 / 250]: Loss = 0.0530\n",
      "[124 / 250]: Loss = 0.0692\n",
      "[125 / 250]: Loss = 0.0505\n",
      "[126 / 250]: Loss = 0.0428\n",
      "[127 / 250]: Loss = 0.0575\n",
      "[128 / 250]: Loss = 0.0478\n",
      "[129 / 250]: Loss = 0.0474\n",
      "[130 / 250]: Loss = 0.0544\n",
      "[131 / 250]: Loss = 0.0434\n",
      "[132 / 250]: Loss = 0.0487\n",
      "[133 / 250]: Loss = 0.0621\n",
      "[134 / 250]: Loss = 0.0463\n",
      "[135 / 250]: Loss = 0.0706\n",
      "[136 / 250]: Loss = 0.0513\n",
      "[137 / 250]: Loss = 0.0515\n",
      "[138 / 250]: Loss = 0.0554\n",
      "[139 / 250]: Loss = 0.0482\n",
      "[140 / 250]: Loss = 0.0363\n",
      "[141 / 250]: Loss = 0.0571\n",
      "[142 / 250]: Loss = 0.0529\n",
      "[143 / 250]: Loss = 0.0421\n",
      "[144 / 250]: Loss = 0.0467\n",
      "[145 / 250]: Loss = 0.0528\n",
      "[146 / 250]: Loss = 0.0520\n",
      "[147 / 250]: Loss = 0.0440\n",
      "[148 / 250]: Loss = 0.0602\n",
      "[149 / 250]: Loss = 0.0531\n",
      "[150 / 250]: Loss = 0.0524\n",
      "[151 / 250]: Loss = 0.0611\n",
      "[152 / 250]: Loss = 0.0426\n",
      "[153 / 250]: Loss = 0.0576\n",
      "[154 / 250]: Loss = 0.0606\n",
      "[155 / 250]: Loss = 0.0529\n",
      "[156 / 250]: Loss = 0.0554\n",
      "[157 / 250]: Loss = 0.0506\n",
      "[158 / 250]: Loss = 0.0443\n",
      "[159 / 250]: Loss = 0.0434\n",
      "[160 / 250]: Loss = 0.0557\n",
      "[161 / 250]: Loss = 0.0511\n",
      "[162 / 250]: Loss = 0.0404\n",
      "[163 / 250]: Loss = 0.0628\n",
      "[164 / 250]: Loss = 0.0616\n",
      "[165 / 250]: Loss = 0.0608\n",
      "[166 / 250]: Loss = 0.0298\n",
      "[167 / 250]: Loss = 0.0482\n",
      "[168 / 250]: Loss = 0.0484\n",
      "[169 / 250]: Loss = 0.0400\n",
      "[170 / 250]: Loss = 0.0423\n",
      "[171 / 250]: Loss = 0.0676\n",
      "[172 / 250]: Loss = 0.0491\n",
      "[173 / 250]: Loss = 0.0527\n",
      "[174 / 250]: Loss = 0.0455\n",
      "[175 / 250]: Loss = 0.0493\n",
      "[176 / 250]: Loss = 0.0534\n",
      "[177 / 250]: Loss = 0.0560\n",
      "[178 / 250]: Loss = 0.0541\n",
      "[179 / 250]: Loss = 0.0543\n",
      "[180 / 250]: Loss = 0.0537\n",
      "[181 / 250]: Loss = 0.0469\n",
      "[182 / 250]: Loss = 0.0478\n",
      "[183 / 250]: Loss = 0.0484\n",
      "[184 / 250]: Loss = 0.0481\n",
      "[185 / 250]: Loss = 0.0501\n",
      "[186 / 250]: Loss = 0.0649\n",
      "[187 / 250]: Loss = 0.0506\n",
      "[188 / 250]: Loss = 0.0478\n",
      "[189 / 250]: Loss = 0.0605\n",
      "[190 / 250]: Loss = 0.0704\n",
      "[191 / 250]: Loss = 0.0524\n",
      "[192 / 250]: Loss = 0.0475\n",
      "[193 / 250]: Loss = 0.0545\n",
      "[194 / 250]: Loss = 0.0449\n",
      "[195 / 250]: Loss = 0.0504\n",
      "[196 / 250]: Loss = 0.0483\n",
      "[197 / 250]: Loss = 0.0554\n",
      "[198 / 250]: Loss = 0.0348\n",
      "[199 / 250]: Loss = 0.0450\n",
      "[200 / 250]: Loss = 0.0546\n",
      "[201 / 250]: Loss = 0.0647\n",
      "[202 / 250]: Loss = 0.0622\n",
      "[203 / 250]: Loss = 0.0518\n",
      "[204 / 250]: Loss = 0.0408\n",
      "[205 / 250]: Loss = 0.0582\n",
      "[206 / 250]: Loss = 0.0445\n",
      "[207 / 250]: Loss = 0.0550\n",
      "[208 / 250]: Loss = 0.0551\n",
      "[209 / 250]: Loss = 0.0452\n",
      "[210 / 250]: Loss = 0.0505\n",
      "[211 / 250]: Loss = 0.0455\n",
      "[212 / 250]: Loss = 0.0395\n",
      "[213 / 250]: Loss = 0.0534\n",
      "[214 / 250]: Loss = 0.0509\n",
      "[215 / 250]: Loss = 0.0517\n",
      "[216 / 250]: Loss = 0.0566\n",
      "[217 / 250]: Loss = 0.0482\n",
      "[218 / 250]: Loss = 0.0592\n",
      "[219 / 250]: Loss = 0.0638\n",
      "[220 / 250]: Loss = 0.0565\n",
      "[221 / 250]: Loss = 0.0607\n",
      "[222 / 250]: Loss = 0.0555\n",
      "[223 / 250]: Loss = 0.0534\n",
      "[224 / 250]: Loss = 0.0532\n",
      "[225 / 250]: Loss = 0.0489\n",
      "[226 / 250]: Loss = 0.0569\n",
      "[227 / 250]: Loss = 0.0628\n",
      "[228 / 250]: Loss = 0.0550\n",
      "[229 / 250]: Loss = 0.0511\n",
      "[230 / 250]: Loss = 0.0567\n",
      "[231 / 250]: Loss = 0.0566\n",
      "[232 / 250]: Loss = 0.0568\n",
      "[233 / 250]: Loss = 0.0547\n",
      "[234 / 250]: Loss = 0.0560\n",
      "[235 / 250]: Loss = 0.0560\n",
      "[236 / 250]: Loss = 0.0626\n",
      "[237 / 250]: Loss = 0.0398\n",
      "[238 / 250]: Loss = 0.0388\n",
      "[239 / 250]: Loss = 0.0485\n",
      "[240 / 250]: Loss = 0.0567\n",
      "[241 / 250]: Loss = 0.0593\n",
      "[242 / 250]: Loss = 0.0568\n",
      "[243 / 250]: Loss = 0.0712\n",
      "[244 / 250]: Loss = 0.0562\n",
      "[245 / 250]: Loss = 0.0489\n",
      "[246 / 250]: Loss = 0.0607\n",
      "[247 / 250]: Loss = 0.0514\n",
      "[248 / 250]: Loss = 0.0510\n",
      "[249 / 250]: Loss = 0.0651\n",
      "[0 / 32]: Loss = 0.0425\n",
      "[1 / 32]: Loss = 0.0395\n",
      "[2 / 32]: Loss = 0.0494\n",
      "[3 / 32]: Loss = 0.0615\n",
      "[4 / 32]: Loss = 0.0542\n",
      "[5 / 32]: Loss = 0.0533\n",
      "[6 / 32]: Loss = 0.0547\n",
      "[7 / 32]: Loss = 0.0517\n",
      "[8 / 32]: Loss = 0.0551\n",
      "[9 / 32]: Loss = 0.0556\n",
      "[10 / 32]: Loss = 0.0556\n",
      "[11 / 32]: Loss = 0.0579\n",
      "[12 / 32]: Loss = 0.0454\n",
      "[13 / 32]: Loss = 0.0504\n",
      "[14 / 32]: Loss = 0.0519\n",
      "[15 / 32]: Loss = 0.0492\n",
      "[16 / 32]: Loss = 0.0525\n",
      "[17 / 32]: Loss = 0.0584\n",
      "[18 / 32]: Loss = 0.0534\n",
      "[19 / 32]: Loss = 0.0439\n",
      "[20 / 32]: Loss = 0.0483\n",
      "[21 / 32]: Loss = 0.0509\n",
      "[22 / 32]: Loss = 0.0520\n",
      "[23 / 32]: Loss = 0.0446\n",
      "[24 / 32]: Loss = 0.0436\n",
      "[25 / 32]: Loss = 0.0639\n",
      "[26 / 32]: Loss = 0.0608\n",
      "[27 / 32]: Loss = 0.0512\n",
      "[28 / 32]: Loss = 0.0499\n",
      "[29 / 32]: Loss = 0.0492\n",
      "[30 / 32]: Loss = 0.0486\n",
      "[31 / 32]: Loss = 0.0372\n",
      "Epoch 3 / 5, Epoch Time = 9.19s: Train Loss = 0.0534, Train AUC = 0.9716, Val Loss = 0.0511, Val AUC = 0.9752\n",
      "[0 / 250]: Loss = 0.0502\n",
      "[1 / 250]: Loss = 0.0500\n",
      "[2 / 250]: Loss = 0.0438\n",
      "[3 / 250]: Loss = 0.0457\n",
      "[4 / 250]: Loss = 0.0470\n",
      "[5 / 250]: Loss = 0.0608\n",
      "[6 / 250]: Loss = 0.0368\n",
      "[7 / 250]: Loss = 0.0628\n",
      "[8 / 250]: Loss = 0.0503\n",
      "[9 / 250]: Loss = 0.0632\n",
      "[10 / 250]: Loss = 0.0700\n",
      "[11 / 250]: Loss = 0.0554\n",
      "[12 / 250]: Loss = 0.0322\n",
      "[13 / 250]: Loss = 0.0561\n",
      "[14 / 250]: Loss = 0.0569\n",
      "[15 / 250]: Loss = 0.0363\n",
      "[16 / 250]: Loss = 0.0471\n",
      "[17 / 250]: Loss = 0.0451\n",
      "[18 / 250]: Loss = 0.0547\n",
      "[19 / 250]: Loss = 0.0593\n",
      "[20 / 250]: Loss = 0.0473\n",
      "[21 / 250]: Loss = 0.0416\n",
      "[22 / 250]: Loss = 0.0503\n",
      "[23 / 250]: Loss = 0.0544\n",
      "[24 / 250]: Loss = 0.0447\n",
      "[25 / 250]: Loss = 0.0522\n",
      "[26 / 250]: Loss = 0.0473\n",
      "[27 / 250]: Loss = 0.0364\n",
      "[28 / 250]: Loss = 0.0543\n",
      "[29 / 250]: Loss = 0.0710\n",
      "[30 / 250]: Loss = 0.0648\n",
      "[31 / 250]: Loss = 0.0450\n",
      "[32 / 250]: Loss = 0.0520\n",
      "[33 / 250]: Loss = 0.0508\n",
      "[34 / 250]: Loss = 0.0647\n",
      "[35 / 250]: Loss = 0.0587\n",
      "[36 / 250]: Loss = 0.0450\n",
      "[37 / 250]: Loss = 0.0379\n",
      "[38 / 250]: Loss = 0.0402\n",
      "[39 / 250]: Loss = 0.0416\n",
      "[40 / 250]: Loss = 0.0601\n",
      "[41 / 250]: Loss = 0.0540\n",
      "[42 / 250]: Loss = 0.0517\n",
      "[43 / 250]: Loss = 0.0547\n",
      "[44 / 250]: Loss = 0.0489\n",
      "[45 / 250]: Loss = 0.0398\n",
      "[46 / 250]: Loss = 0.0563\n",
      "[47 / 250]: Loss = 0.0578\n",
      "[48 / 250]: Loss = 0.0516\n",
      "[49 / 250]: Loss = 0.0533\n",
      "[50 / 250]: Loss = 0.0452\n",
      "[51 / 250]: Loss = 0.0535\n",
      "[52 / 250]: Loss = 0.0463\n",
      "[53 / 250]: Loss = 0.0461\n",
      "[54 / 250]: Loss = 0.0611\n",
      "[55 / 250]: Loss = 0.0506\n",
      "[56 / 250]: Loss = 0.0523\n",
      "[57 / 250]: Loss = 0.0519\n",
      "[58 / 250]: Loss = 0.0507\n",
      "[59 / 250]: Loss = 0.0464\n",
      "[60 / 250]: Loss = 0.0486\n",
      "[61 / 250]: Loss = 0.0430\n",
      "[62 / 250]: Loss = 0.0420\n",
      "[63 / 250]: Loss = 0.0611\n",
      "[64 / 250]: Loss = 0.0468\n",
      "[65 / 250]: Loss = 0.0461\n",
      "[66 / 250]: Loss = 0.0513\n",
      "[67 / 250]: Loss = 0.0462\n",
      "[68 / 250]: Loss = 0.0449\n",
      "[69 / 250]: Loss = 0.0512\n",
      "[70 / 250]: Loss = 0.0427\n",
      "[71 / 250]: Loss = 0.0401\n",
      "[72 / 250]: Loss = 0.0541\n",
      "[73 / 250]: Loss = 0.0537\n",
      "[74 / 250]: Loss = 0.0519\n",
      "[75 / 250]: Loss = 0.0469\n",
      "[76 / 250]: Loss = 0.0506\n",
      "[77 / 250]: Loss = 0.0527\n",
      "[78 / 250]: Loss = 0.0418\n",
      "[79 / 250]: Loss = 0.0389\n",
      "[80 / 250]: Loss = 0.0574\n",
      "[81 / 250]: Loss = 0.0465\n",
      "[82 / 250]: Loss = 0.0612\n",
      "[83 / 250]: Loss = 0.0646\n",
      "[84 / 250]: Loss = 0.0500\n",
      "[85 / 250]: Loss = 0.0409\n",
      "[86 / 250]: Loss = 0.0441\n",
      "[87 / 250]: Loss = 0.0588\n",
      "[88 / 250]: Loss = 0.0571\n",
      "[89 / 250]: Loss = 0.0440\n",
      "[90 / 250]: Loss = 0.0604\n",
      "[91 / 250]: Loss = 0.0541\n",
      "[92 / 250]: Loss = 0.0504\n",
      "[93 / 250]: Loss = 0.0372\n",
      "[94 / 250]: Loss = 0.0426\n",
      "[95 / 250]: Loss = 0.0624\n",
      "[96 / 250]: Loss = 0.0486\n",
      "[97 / 250]: Loss = 0.0404\n",
      "[98 / 250]: Loss = 0.0646\n",
      "[99 / 250]: Loss = 0.0478\n",
      "[100 / 250]: Loss = 0.0436\n",
      "[101 / 250]: Loss = 0.0580\n",
      "[102 / 250]: Loss = 0.0444\n",
      "[103 / 250]: Loss = 0.0554\n",
      "[104 / 250]: Loss = 0.0573\n",
      "[105 / 250]: Loss = 0.0452\n",
      "[106 / 250]: Loss = 0.0673\n",
      "[107 / 250]: Loss = 0.0525\n",
      "[108 / 250]: Loss = 0.0388\n",
      "[109 / 250]: Loss = 0.0547\n",
      "[110 / 250]: Loss = 0.0499\n",
      "[111 / 250]: Loss = 0.0468\n",
      "[112 / 250]: Loss = 0.0481\n",
      "[113 / 250]: Loss = 0.0534\n",
      "[114 / 250]: Loss = 0.0533\n",
      "[115 / 250]: Loss = 0.0670\n",
      "[116 / 250]: Loss = 0.0433\n",
      "[117 / 250]: Loss = 0.0531\n",
      "[118 / 250]: Loss = 0.0601\n",
      "[119 / 250]: Loss = 0.0616\n",
      "[120 / 250]: Loss = 0.0490\n",
      "[121 / 250]: Loss = 0.0446\n",
      "[122 / 250]: Loss = 0.0542\n",
      "[123 / 250]: Loss = 0.0652\n",
      "[124 / 250]: Loss = 0.0431\n",
      "[125 / 250]: Loss = 0.0535\n",
      "[126 / 250]: Loss = 0.0473\n",
      "[127 / 250]: Loss = 0.0533\n",
      "[128 / 250]: Loss = 0.0527\n",
      "[129 / 250]: Loss = 0.0457\n",
      "[130 / 250]: Loss = 0.0404\n",
      "[131 / 250]: Loss = 0.0626\n",
      "[132 / 250]: Loss = 0.0441\n",
      "[133 / 250]: Loss = 0.0409\n",
      "[134 / 250]: Loss = 0.0454\n",
      "[135 / 250]: Loss = 0.0519\n",
      "[136 / 250]: Loss = 0.0558\n",
      "[137 / 250]: Loss = 0.0460\n",
      "[138 / 250]: Loss = 0.0366\n",
      "[139 / 250]: Loss = 0.0423\n",
      "[140 / 250]: Loss = 0.0581\n",
      "[141 / 250]: Loss = 0.0419\n",
      "[142 / 250]: Loss = 0.0484\n",
      "[143 / 250]: Loss = 0.0543\n",
      "[144 / 250]: Loss = 0.0466\n",
      "[145 / 250]: Loss = 0.0369\n",
      "[146 / 250]: Loss = 0.0506\n",
      "[147 / 250]: Loss = 0.0574\n",
      "[148 / 250]: Loss = 0.0515\n",
      "[149 / 250]: Loss = 0.0493\n",
      "[150 / 250]: Loss = 0.0519\n",
      "[151 / 250]: Loss = 0.0522\n",
      "[152 / 250]: Loss = 0.0481\n",
      "[153 / 250]: Loss = 0.0509\n",
      "[154 / 250]: Loss = 0.0551\n",
      "[155 / 250]: Loss = 0.0450\n",
      "[156 / 250]: Loss = 0.0548\n",
      "[157 / 250]: Loss = 0.0440\n",
      "[158 / 250]: Loss = 0.0591\n",
      "[159 / 250]: Loss = 0.0582\n",
      "[160 / 250]: Loss = 0.0465\n",
      "[161 / 250]: Loss = 0.0446\n",
      "[162 / 250]: Loss = 0.0456\n",
      "[163 / 250]: Loss = 0.0611\n",
      "[164 / 250]: Loss = 0.0500\n",
      "[165 / 250]: Loss = 0.0495\n",
      "[166 / 250]: Loss = 0.0616\n",
      "[167 / 250]: Loss = 0.0533\n",
      "[168 / 250]: Loss = 0.0588\n",
      "[169 / 250]: Loss = 0.0461\n",
      "[170 / 250]: Loss = 0.0530\n",
      "[171 / 250]: Loss = 0.0469\n",
      "[172 / 250]: Loss = 0.0402\n",
      "[173 / 250]: Loss = 0.0613\n",
      "[174 / 250]: Loss = 0.0559\n",
      "[175 / 250]: Loss = 0.0386\n",
      "[176 / 250]: Loss = 0.0443\n",
      "[177 / 250]: Loss = 0.0472\n",
      "[178 / 250]: Loss = 0.0469\n",
      "[179 / 250]: Loss = 0.0561\n",
      "[180 / 250]: Loss = 0.0440\n",
      "[181 / 250]: Loss = 0.0520\n",
      "[182 / 250]: Loss = 0.0503\n",
      "[183 / 250]: Loss = 0.0531\n",
      "[184 / 250]: Loss = 0.0615\n",
      "[185 / 250]: Loss = 0.0399\n",
      "[186 / 250]: Loss = 0.0511\n",
      "[187 / 250]: Loss = 0.0449\n",
      "[188 / 250]: Loss = 0.0514\n",
      "[189 / 250]: Loss = 0.0466\n",
      "[190 / 250]: Loss = 0.0609\n",
      "[191 / 250]: Loss = 0.0555\n",
      "[192 / 250]: Loss = 0.0530\n",
      "[193 / 250]: Loss = 0.0453\n",
      "[194 / 250]: Loss = 0.0456\n",
      "[195 / 250]: Loss = 0.0528\n",
      "[196 / 250]: Loss = 0.0520\n",
      "[197 / 250]: Loss = 0.0455\n",
      "[198 / 250]: Loss = 0.0533\n",
      "[199 / 250]: Loss = 0.0448\n",
      "[200 / 250]: Loss = 0.0581\n",
      "[201 / 250]: Loss = 0.0394\n",
      "[202 / 250]: Loss = 0.0530\n",
      "[203 / 250]: Loss = 0.0512\n",
      "[204 / 250]: Loss = 0.0348\n",
      "[205 / 250]: Loss = 0.0554\n",
      "[206 / 250]: Loss = 0.0424\n",
      "[207 / 250]: Loss = 0.0482\n",
      "[208 / 250]: Loss = 0.0533\n",
      "[209 / 250]: Loss = 0.0394\n",
      "[210 / 250]: Loss = 0.0523\n",
      "[211 / 250]: Loss = 0.0446\n",
      "[212 / 250]: Loss = 0.0412\n",
      "[213 / 250]: Loss = 0.0560\n",
      "[214 / 250]: Loss = 0.0447\n",
      "[215 / 250]: Loss = 0.0449\n",
      "[216 / 250]: Loss = 0.0503\n",
      "[217 / 250]: Loss = 0.0522\n",
      "[218 / 250]: Loss = 0.0557\n",
      "[219 / 250]: Loss = 0.0412\n",
      "[220 / 250]: Loss = 0.0467\n",
      "[221 / 250]: Loss = 0.0388\n",
      "[222 / 250]: Loss = 0.0536\n",
      "[223 / 250]: Loss = 0.0388\n",
      "[224 / 250]: Loss = 0.0553\n",
      "[225 / 250]: Loss = 0.0573\n",
      "[226 / 250]: Loss = 0.0677\n",
      "[227 / 250]: Loss = 0.0437\n",
      "[228 / 250]: Loss = 0.0493\n",
      "[229 / 250]: Loss = 0.0523\n",
      "[230 / 250]: Loss = 0.0458\n",
      "[231 / 250]: Loss = 0.0362\n",
      "[232 / 250]: Loss = 0.0566\n",
      "[233 / 250]: Loss = 0.0453\n",
      "[234 / 250]: Loss = 0.0557\n",
      "[235 / 250]: Loss = 0.0596\n",
      "[236 / 250]: Loss = 0.0462\n",
      "[237 / 250]: Loss = 0.0473\n",
      "[238 / 250]: Loss = 0.0488\n",
      "[239 / 250]: Loss = 0.0480\n",
      "[240 / 250]: Loss = 0.0405\n",
      "[241 / 250]: Loss = 0.0502\n",
      "[242 / 250]: Loss = 0.0427\n",
      "[243 / 250]: Loss = 0.0538\n",
      "[244 / 250]: Loss = 0.0470\n",
      "[245 / 250]: Loss = 0.0418\n",
      "[246 / 250]: Loss = 0.0571\n",
      "[247 / 250]: Loss = 0.0340\n",
      "[248 / 250]: Loss = 0.0442\n",
      "[249 / 250]: Loss = 0.0331\n",
      "[0 / 32]: Loss = 0.0504\n",
      "[1 / 32]: Loss = 0.0447\n",
      "[2 / 32]: Loss = 0.0511\n",
      "[3 / 32]: Loss = 0.0443\n",
      "[4 / 32]: Loss = 0.0459\n",
      "[5 / 32]: Loss = 0.0471\n",
      "[6 / 32]: Loss = 0.0526\n",
      "[7 / 32]: Loss = 0.0410\n",
      "[8 / 32]: Loss = 0.0572\n",
      "[9 / 32]: Loss = 0.0544\n",
      "[10 / 32]: Loss = 0.0567\n",
      "[11 / 32]: Loss = 0.0517\n",
      "[12 / 32]: Loss = 0.0574\n",
      "[13 / 32]: Loss = 0.0459\n",
      "[14 / 32]: Loss = 0.0501\n",
      "[15 / 32]: Loss = 0.0501\n",
      "[16 / 32]: Loss = 0.0498\n",
      "[17 / 32]: Loss = 0.0449\n",
      "[18 / 32]: Loss = 0.0590\n",
      "[19 / 32]: Loss = 0.0415\n",
      "[20 / 32]: Loss = 0.0657\n",
      "[21 / 32]: Loss = 0.0462\n",
      "[22 / 32]: Loss = 0.0470\n",
      "[23 / 32]: Loss = 0.0452\n",
      "[24 / 32]: Loss = 0.0514\n",
      "[25 / 32]: Loss = 0.0610\n",
      "[26 / 32]: Loss = 0.0465\n",
      "[27 / 32]: Loss = 0.0514\n",
      "[28 / 32]: Loss = 0.0451\n",
      "[29 / 32]: Loss = 0.0469\n",
      "[30 / 32]: Loss = 0.0540\n",
      "[31 / 32]: Loss = 0.0450\n",
      "Epoch 4 / 5, Epoch Time = 9.17s: Train Loss = 0.0500, Train AUC = 0.9776, Val Loss = 0.0500, Val AUC = 0.9787\n",
      "[0 / 250]: Loss = 0.0486\n",
      "[1 / 250]: Loss = 0.0486\n",
      "[2 / 250]: Loss = 0.0419\n",
      "[3 / 250]: Loss = 0.0448\n",
      "[4 / 250]: Loss = 0.0409\n",
      "[5 / 250]: Loss = 0.0451\n",
      "[6 / 250]: Loss = 0.0521\n",
      "[7 / 250]: Loss = 0.0494\n",
      "[8 / 250]: Loss = 0.0416\n",
      "[9 / 250]: Loss = 0.0407\n",
      "[10 / 250]: Loss = 0.0390\n",
      "[11 / 250]: Loss = 0.0463\n",
      "[12 / 250]: Loss = 0.0493\n",
      "[13 / 250]: Loss = 0.0544\n",
      "[14 / 250]: Loss = 0.0317\n",
      "[15 / 250]: Loss = 0.0325\n",
      "[16 / 250]: Loss = 0.0383\n",
      "[17 / 250]: Loss = 0.0503\n",
      "[18 / 250]: Loss = 0.0448\n",
      "[19 / 250]: Loss = 0.0589\n",
      "[20 / 250]: Loss = 0.0601\n",
      "[21 / 250]: Loss = 0.0466\n",
      "[22 / 250]: Loss = 0.0399\n",
      "[23 / 250]: Loss = 0.0327\n",
      "[24 / 250]: Loss = 0.0398\n",
      "[25 / 250]: Loss = 0.0508\n",
      "[26 / 250]: Loss = 0.0613\n",
      "[27 / 250]: Loss = 0.0511\n",
      "[28 / 250]: Loss = 0.0486\n",
      "[29 / 250]: Loss = 0.0402\n",
      "[30 / 250]: Loss = 0.0452\n",
      "[31 / 250]: Loss = 0.0444\n",
      "[32 / 250]: Loss = 0.0430\n",
      "[33 / 250]: Loss = 0.0422\n",
      "[34 / 250]: Loss = 0.0502\n",
      "[35 / 250]: Loss = 0.0411\n",
      "[36 / 250]: Loss = 0.0413\n",
      "[37 / 250]: Loss = 0.0549\n",
      "[38 / 250]: Loss = 0.0465\n",
      "[39 / 250]: Loss = 0.0491\n",
      "[40 / 250]: Loss = 0.0341\n",
      "[41 / 250]: Loss = 0.0452\n",
      "[42 / 250]: Loss = 0.0543\n",
      "[43 / 250]: Loss = 0.0573\n",
      "[44 / 250]: Loss = 0.0639\n",
      "[45 / 250]: Loss = 0.0559\n",
      "[46 / 250]: Loss = 0.0500\n",
      "[47 / 250]: Loss = 0.0451\n",
      "[48 / 250]: Loss = 0.0432\n",
      "[49 / 250]: Loss = 0.0458\n",
      "[50 / 250]: Loss = 0.0549\n",
      "[51 / 250]: Loss = 0.0400\n",
      "[52 / 250]: Loss = 0.0369\n",
      "[53 / 250]: Loss = 0.0387\n",
      "[54 / 250]: Loss = 0.0441\n",
      "[55 / 250]: Loss = 0.0378\n",
      "[56 / 250]: Loss = 0.0433\n",
      "[57 / 250]: Loss = 0.0483\n",
      "[58 / 250]: Loss = 0.0372\n",
      "[59 / 250]: Loss = 0.0400\n",
      "[60 / 250]: Loss = 0.0432\n",
      "[61 / 250]: Loss = 0.0480\n",
      "[62 / 250]: Loss = 0.0393\n",
      "[63 / 250]: Loss = 0.0400\n",
      "[64 / 250]: Loss = 0.0566\n",
      "[65 / 250]: Loss = 0.0457\n",
      "[66 / 250]: Loss = 0.0356\n",
      "[67 / 250]: Loss = 0.0501\n",
      "[68 / 250]: Loss = 0.0513\n",
      "[69 / 250]: Loss = 0.0451\n",
      "[70 / 250]: Loss = 0.0501\n",
      "[71 / 250]: Loss = 0.0472\n",
      "[72 / 250]: Loss = 0.0430\n",
      "[73 / 250]: Loss = 0.0446\n",
      "[74 / 250]: Loss = 0.0520\n",
      "[75 / 250]: Loss = 0.0495\n",
      "[76 / 250]: Loss = 0.0512\n",
      "[77 / 250]: Loss = 0.0564\n",
      "[78 / 250]: Loss = 0.0512\n",
      "[79 / 250]: Loss = 0.0430\n",
      "[80 / 250]: Loss = 0.0439\n",
      "[81 / 250]: Loss = 0.0444\n",
      "[82 / 250]: Loss = 0.0502\n",
      "[83 / 250]: Loss = 0.0523\n",
      "[84 / 250]: Loss = 0.0473\n",
      "[85 / 250]: Loss = 0.0431\n",
      "[86 / 250]: Loss = 0.0546\n",
      "[87 / 250]: Loss = 0.0389\n",
      "[88 / 250]: Loss = 0.0495\n",
      "[89 / 250]: Loss = 0.0359\n",
      "[90 / 250]: Loss = 0.0386\n",
      "[91 / 250]: Loss = 0.0658\n",
      "[92 / 250]: Loss = 0.0593\n",
      "[93 / 250]: Loss = 0.0523\n",
      "[94 / 250]: Loss = 0.0547\n",
      "[95 / 250]: Loss = 0.0446\n",
      "[96 / 250]: Loss = 0.0380\n",
      "[97 / 250]: Loss = 0.0352\n",
      "[98 / 250]: Loss = 0.0500\n",
      "[99 / 250]: Loss = 0.0503\n",
      "[100 / 250]: Loss = 0.0386\n",
      "[101 / 250]: Loss = 0.0595\n",
      "[102 / 250]: Loss = 0.0340\n",
      "[103 / 250]: Loss = 0.0614\n",
      "[104 / 250]: Loss = 0.0463\n",
      "[105 / 250]: Loss = 0.0459\n",
      "[106 / 250]: Loss = 0.0531\n",
      "[107 / 250]: Loss = 0.0422\n",
      "[108 / 250]: Loss = 0.0441\n",
      "[109 / 250]: Loss = 0.0477\n",
      "[110 / 250]: Loss = 0.0406\n",
      "[111 / 250]: Loss = 0.0476\n",
      "[112 / 250]: Loss = 0.0652\n",
      "[113 / 250]: Loss = 0.0554\n",
      "[114 / 250]: Loss = 0.0350\n",
      "[115 / 250]: Loss = 0.0572\n",
      "[116 / 250]: Loss = 0.0463\n",
      "[117 / 250]: Loss = 0.0465\n",
      "[118 / 250]: Loss = 0.0419\n",
      "[119 / 250]: Loss = 0.0370\n",
      "[120 / 250]: Loss = 0.0507\n",
      "[121 / 250]: Loss = 0.0431\n",
      "[122 / 250]: Loss = 0.0459\n",
      "[123 / 250]: Loss = 0.0428\n",
      "[124 / 250]: Loss = 0.0351\n",
      "[125 / 250]: Loss = 0.0460\n",
      "[126 / 250]: Loss = 0.0409\n",
      "[127 / 250]: Loss = 0.0439\n",
      "[128 / 250]: Loss = 0.0547\n",
      "[129 / 250]: Loss = 0.0499\n",
      "[130 / 250]: Loss = 0.0377\n",
      "[131 / 250]: Loss = 0.0534\n",
      "[132 / 250]: Loss = 0.0415\n",
      "[133 / 250]: Loss = 0.0606\n",
      "[134 / 250]: Loss = 0.0352\n",
      "[135 / 250]: Loss = 0.0482\n",
      "[136 / 250]: Loss = 0.0332\n",
      "[137 / 250]: Loss = 0.0549\n",
      "[138 / 250]: Loss = 0.0503\n",
      "[139 / 250]: Loss = 0.0459\n",
      "[140 / 250]: Loss = 0.0585\n",
      "[141 / 250]: Loss = 0.0523\n",
      "[142 / 250]: Loss = 0.0558\n",
      "[143 / 250]: Loss = 0.0427\n",
      "[144 / 250]: Loss = 0.0306\n",
      "[145 / 250]: Loss = 0.0489\n",
      "[146 / 250]: Loss = 0.0378\n",
      "[147 / 250]: Loss = 0.0428\n",
      "[148 / 250]: Loss = 0.0535\n",
      "[149 / 250]: Loss = 0.0448\n",
      "[150 / 250]: Loss = 0.0433\n",
      "[151 / 250]: Loss = 0.0424\n",
      "[152 / 250]: Loss = 0.0479\n",
      "[153 / 250]: Loss = 0.0373\n",
      "[154 / 250]: Loss = 0.0467\n",
      "[155 / 250]: Loss = 0.0535\n",
      "[156 / 250]: Loss = 0.0532\n",
      "[157 / 250]: Loss = 0.0443\n",
      "[158 / 250]: Loss = 0.0546\n",
      "[159 / 250]: Loss = 0.0459\n",
      "[160 / 250]: Loss = 0.0470\n",
      "[161 / 250]: Loss = 0.0554\n",
      "[162 / 250]: Loss = 0.0524\n",
      "[163 / 250]: Loss = 0.0453\n",
      "[164 / 250]: Loss = 0.0567\n",
      "[165 / 250]: Loss = 0.0509\n",
      "[166 / 250]: Loss = 0.0569\n",
      "[167 / 250]: Loss = 0.0372\n",
      "[168 / 250]: Loss = 0.0369\n",
      "[169 / 250]: Loss = 0.0464\n",
      "[170 / 250]: Loss = 0.0457\n",
      "[171 / 250]: Loss = 0.0539\n",
      "[172 / 250]: Loss = 0.0627\n",
      "[173 / 250]: Loss = 0.0369\n",
      "[174 / 250]: Loss = 0.0423\n",
      "[175 / 250]: Loss = 0.0584\n",
      "[176 / 250]: Loss = 0.0569\n",
      "[177 / 250]: Loss = 0.0459\n",
      "[178 / 250]: Loss = 0.0508\n",
      "[179 / 250]: Loss = 0.0562\n",
      "[180 / 250]: Loss = 0.0472\n",
      "[181 / 250]: Loss = 0.0440\n",
      "[182 / 250]: Loss = 0.0522\n",
      "[183 / 250]: Loss = 0.0503\n",
      "[184 / 250]: Loss = 0.0612\n",
      "[185 / 250]: Loss = 0.0380\n",
      "[186 / 250]: Loss = 0.0388\n",
      "[187 / 250]: Loss = 0.0382\n",
      "[188 / 250]: Loss = 0.0534\n",
      "[189 / 250]: Loss = 0.0451\n",
      "[190 / 250]: Loss = 0.0423\n",
      "[191 / 250]: Loss = 0.0399\n",
      "[192 / 250]: Loss = 0.0427\n",
      "[193 / 250]: Loss = 0.0641\n",
      "[194 / 250]: Loss = 0.0434\n",
      "[195 / 250]: Loss = 0.0463\n",
      "[196 / 250]: Loss = 0.0587\n",
      "[197 / 250]: Loss = 0.0661\n",
      "[198 / 250]: Loss = 0.0539\n",
      "[199 / 250]: Loss = 0.0486\n",
      "[200 / 250]: Loss = 0.0413\n",
      "[201 / 250]: Loss = 0.0480\n",
      "[202 / 250]: Loss = 0.0362\n",
      "[203 / 250]: Loss = 0.0546\n",
      "[204 / 250]: Loss = 0.0636\n",
      "[205 / 250]: Loss = 0.0414\n",
      "[206 / 250]: Loss = 0.0423\n",
      "[207 / 250]: Loss = 0.0457\n",
      "[208 / 250]: Loss = 0.0497\n",
      "[209 / 250]: Loss = 0.0519\n",
      "[210 / 250]: Loss = 0.0455\n",
      "[211 / 250]: Loss = 0.0521\n",
      "[212 / 250]: Loss = 0.0389\n",
      "[213 / 250]: Loss = 0.0417\n",
      "[214 / 250]: Loss = 0.0507\n",
      "[215 / 250]: Loss = 0.0502\n",
      "[216 / 250]: Loss = 0.0450\n",
      "[217 / 250]: Loss = 0.0516\n",
      "[218 / 250]: Loss = 0.0416\n",
      "[219 / 250]: Loss = 0.0478\n",
      "[220 / 250]: Loss = 0.0397\n",
      "[221 / 250]: Loss = 0.0504\n",
      "[222 / 250]: Loss = 0.0518\n",
      "[223 / 250]: Loss = 0.0416\n",
      "[224 / 250]: Loss = 0.0447\n",
      "[225 / 250]: Loss = 0.0418\n",
      "[226 / 250]: Loss = 0.0407\n",
      "[227 / 250]: Loss = 0.0467\n",
      "[228 / 250]: Loss = 0.0453\n",
      "[229 / 250]: Loss = 0.0394\n",
      "[230 / 250]: Loss = 0.0469\n",
      "[231 / 250]: Loss = 0.0422\n",
      "[232 / 250]: Loss = 0.0423\n",
      "[233 / 250]: Loss = 0.0507\n",
      "[234 / 250]: Loss = 0.0332\n",
      "[235 / 250]: Loss = 0.0398\n",
      "[236 / 250]: Loss = 0.0462\n",
      "[237 / 250]: Loss = 0.0491\n",
      "[238 / 250]: Loss = 0.0409\n",
      "[239 / 250]: Loss = 0.0418\n",
      "[240 / 250]: Loss = 0.0520\n",
      "[241 / 250]: Loss = 0.0383\n",
      "[242 / 250]: Loss = 0.0460\n",
      "[243 / 250]: Loss = 0.0417\n",
      "[244 / 250]: Loss = 0.0423\n",
      "[245 / 250]: Loss = 0.0499\n",
      "[246 / 250]: Loss = 0.0509\n",
      "[247 / 250]: Loss = 0.0450\n",
      "[248 / 250]: Loss = 0.0529\n",
      "[249 / 250]: Loss = 0.0344\n",
      "[0 / 32]: Loss = 0.0464\n",
      "[1 / 32]: Loss = 0.0443\n",
      "[2 / 32]: Loss = 0.0431\n",
      "[3 / 32]: Loss = 0.0435\n",
      "[4 / 32]: Loss = 0.0484\n",
      "[5 / 32]: Loss = 0.0576\n",
      "[6 / 32]: Loss = 0.0478\n",
      "[7 / 32]: Loss = 0.0491\n",
      "[8 / 32]: Loss = 0.0450\n",
      "[9 / 32]: Loss = 0.0503\n",
      "[10 / 32]: Loss = 0.0626\n",
      "[11 / 32]: Loss = 0.0493\n",
      "[12 / 32]: Loss = 0.0529\n",
      "[13 / 32]: Loss = 0.0520\n",
      "[14 / 32]: Loss = 0.0583\n",
      "[15 / 32]: Loss = 0.0480\n",
      "[16 / 32]: Loss = 0.0603\n",
      "[17 / 32]: Loss = 0.0469\n",
      "[18 / 32]: Loss = 0.0391\n",
      "[19 / 32]: Loss = 0.0511\n",
      "[20 / 32]: Loss = 0.0516\n",
      "[21 / 32]: Loss = 0.0430\n",
      "[22 / 32]: Loss = 0.0524\n",
      "[23 / 32]: Loss = 0.0530\n",
      "[24 / 32]: Loss = 0.0483\n",
      "[25 / 32]: Loss = 0.0444\n",
      "[26 / 32]: Loss = 0.0437\n",
      "[27 / 32]: Loss = 0.0486\n",
      "[28 / 32]: Loss = 0.0517\n",
      "[29 / 32]: Loss = 0.0471\n",
      "[30 / 32]: Loss = 0.0484\n",
      "[31 / 32]: Loss = 0.0452\n",
      "Epoch 5 / 5, Epoch Time = 9.18s: Train Loss = 0.0467, Train AUC = 0.9819, Val Loss = 0.0492, Val AUC = 0.9815\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(token_to_id)\n",
    "hidden_dim = 300\n",
    "\n",
    "model        = Net2(torch.FloatTensor(embedding_matrix), \n",
    "                    vocab_size, \n",
    "                    hidden_dim, \n",
    "                    PAD_IX\n",
    "                    ).cuda()\n",
    "\n",
    "criterion    = nn.BCEWithLogitsLoss().cuda()\n",
    "optimizer    = optim.Adam([param for param in model.parameters() if param.requires_grad], lr=0.001)\n",
    "\n",
    "X_train      = as_matrix(data_train['tokenized_comments'], \n",
    "                         token_to_id, \n",
    "                         word_dropout=0.01, \n",
    "                         UNK_IX=UNK_IX, \n",
    "                         PAD_IX=PAD_IX,\n",
    "                         max_len=MAX_LEN\n",
    "                        )\n",
    "\n",
    "train_labels = data_train.loc[:, TARGET_COLS].values \n",
    "\n",
    "X_test       = as_matrix(data_val['tokenized_comments'],\n",
    "                         token_to_id, \n",
    "                         word_dropout=0.01, \n",
    "                         UNK_IX=UNK_IX, \n",
    "                         PAD_IX=PAD_IX,\n",
    "                         max_len=MAX_LEN\n",
    "                        )\n",
    "\n",
    "test_labels  = data_val.loc[:, TARGET_COLS].values\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, train_labels), epochs_count=5, \n",
    "    batch_size=512, val_data=(X_test, test_labels), val_batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Num feature maps : 3\n",
    "kernel size      : 3\n",
    "\n",
    "Epoch 5 / 5, Epoch Time = 5.61s: Train Loss = 0.1502, Train AUC = 0.6088, Val Loss = 0.1462, Val AUC = 0.5961\n",
    "\n",
    "Num feature maps : 100\n",
    "kernel size      : 3\n",
    "\n",
    "Epoch 5 / 5, Epoch Time = 10.27s: Train Loss = 0.0633, Train AUC = 0.9024, Val Loss = 0.0653, Val AUC = 0.9299\n",
    "\n",
    "Num feature maps : 100\n",
    "kernel size      : 5\n",
    "\n",
    "Epoch 5 / 5, Epoch Time = 11.90s: Train Loss = 0.0628, Train AUC = 0.9572, Val Loss = 0.0686, Val AUC = 0.9571\n",
    "\n",
    "Num feature maps : 100\n",
    "kernel size      : 7\n",
    "\n",
    "Epoch 5 / 5, Epoch Time = 13.89s: Train Loss = 0.0647, Train AUC = 0.9273, Val Loss = 0.0676, Val AUC = 0.9603\n",
    "\n",
    "\n",
    "Num feature maps: [100, 100]\n",
    "kernel sizes    : [5, 5]\n",
    "\n",
    "Epoch 5 / 5, Epoch Time = 22.96s: Train Loss = 0.0577, Train AUC = 0.9596, Val Loss = 0.0619, Val AUC = 0.9607\n",
    "\n",
    "Num feature maps: [100, 100]\n",
    "kernel sizes    : [3, 3]\n",
    "\n",
    "Epoch 5 / 5, Epoch Time = 19.88s: Train Loss = 0.0591, Train AUC = 0.9488, Val Loss = 0.0591, Val AUC = 0.9558\n",
    "\n",
    "( Full Dataset )\n",
    "Num feature maps : [100, 100]\n",
    "kernel size      : [5, 6]\n",
    "word dropout     : 0.01\n",
    "dropout          : 0.1\n",
    "max len          : 100\n",
    "\n",
    "Epoch 5 / 5, Epoch Time = 9.96s: Train Loss = 0.0478, Train AUC = 0.9812, Val Loss = 0.0513, Val AUC = 0.9794\n",
    "\n",
    "Num feature maps : [100, 100]\n",
    "kernel size      : [5, 6]\n",
    "word dropout     : 0.01\n",
    "dropout          : 0.1\n",
    "max len          : 100\n",
    "pad_ix ( not considered in embedding layer )\n",
    "\n",
    "Epoch 5 / 5, Epoch Time = 9.85s: Train Loss = 0.0478, Train AUC = 0.9803, Val Loss = 0.0502, Val AUC = 0.9793\n",
    "\n",
    "Num feature maps : [100, 100]\n",
    "kernel size      : [4, 5]\n",
    "word dropout     : 0.01\n",
    "dropout          : 0.1\n",
    "max len          : 100\n",
    "pad_ix ( not considered in embedding layer )\n",
    "\n",
    "Epoch 5 / 5, Epoch Time = 9.18s: Train Loss = 0.0467, Train AUC = 0.9819, Val Loss = 0.0492, Val AUC = 0.9815\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
