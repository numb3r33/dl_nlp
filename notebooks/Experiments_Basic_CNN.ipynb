{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "SEED = 41\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to read in the corpus\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "t2i = defaultdict(lambda: len(t2i))\n",
    "UNK = w2i[\"<unk>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            tag, words = line.lower().strip().split(\" ||| \")\n",
    "            yield ([w2i[x] for x in words.split(\" \")], t2i[tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "train = list(read_dataset(\"../data/raw/classes/train.txt\"))\n",
    "w2i = defaultdict(lambda: UNK, w2i)\n",
    "dev = list(read_dataset(\"../data/raw/classes/test.txt\"))\n",
    "nwords = len(w2i)\n",
    "ntags = len(t2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "EMB_SIZE    = 64\n",
    "WIN_SIZE    = 3  # kernel size\n",
    "FILTER_SIZE = 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNclass(nn.Module):\n",
    "    def __init__(self, nwords, emb_size, num_filters, window_size, ntags):\n",
    "        super(CNNclass, self).__init__()\n",
    "\n",
    "        \"\"\" layers \"\"\"\n",
    "        self.embedding = nn.Embedding(nwords, emb_size)\n",
    "        # uniform initialization\n",
    "        torch.nn.init.uniform_(self.embedding.weight, -0.25, 0.25)\n",
    "        # Conv 1d\n",
    "        self.conv_1d = nn.Conv1d(in_channels=emb_size, out_channels=num_filters, kernel_size=window_size,\n",
    "                                 stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.projection_layer = nn.Linear(in_features=num_filters, out_features=ntags, bias=True)\n",
    "        # Initializing the projection layer\n",
    "        torch.nn.init.xavier_uniform_(self.projection_layer.weight)\n",
    "\n",
    "    def forward(self, words):\n",
    "        emb = self.embedding(words)                 # nwords x emb_size\n",
    "        emb = emb.unsqueeze(0).permute(0, 2, 1)     # 1 x emb_size x nwords\n",
    "        h = self.conv_1d(emb)                       # 1 x num_filters x nwords\n",
    "        # Do max pooling\n",
    "        h = h.max(dim=2)[0]                         # 1 x num_filters\n",
    "        h = self.relu(h)\n",
    "        out = self.projection_layer(h)              # size(out) = 1 x ntags\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "model     = CNNclass(nwords, EMB_SIZE, FILTER_SIZE, WIN_SIZE, ntags)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "type     = torch.LongTensor\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    type = torch.cuda.LongTensor\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run on a single instance\n",
    "words, tag = train[0]\n",
    "\n",
    "if len(words) < WIN_SIZE:\n",
    "    words += [0] * (WIN_SIZE - len(words))\n",
    "    \n",
    "words_tensor = torch.tensor(words).type(type)\n",
    "tag_tensor   = torch.tensor([tag]).type(type)\n",
    "scores       = model(words_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  2,  3,  4,  5,  6,  1,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,  9,\n",
       "        17,  5, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ITER in range(1):\n",
    "    # Perform training\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0.0\n",
    "    start = time.time()\n",
    "    \n",
    "    for words, tag in train:\n",
    "        # Padding (can be done in the conv layer as well)\n",
    "        if len(words) < WIN_SIZE:\n",
    "            words += [0] * (WIN_SIZE - len(words))\n",
    "        words_tensor = torch.tensor(words).type(type)\n",
    "        tag_tensor   = torch.tensor([tag]).type(type)\n",
    "        scores  = model(words_tensor)\n",
    "        predict = scores[0].argmax().item()\n",
    "        if predict == tag:\n",
    "            train_correct += 1\n",
    "\n",
    "        my_loss = criterion(scores, tag_tensor)\n",
    "        train_loss += my_loss.item()\n",
    "        # Do back-prop\n",
    "        optimizer.zero_grad()\n",
    "        my_loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"iter %r: train loss/sent=%.4f, acc=%.4f, time=%.2fs\" % (\n",
    "        ITER, train_loss / len(train), train_correct / len(train), time.time() - start))\n",
    "    # Perform testing\n",
    "    test_correct = 0.0\n",
    "    for words, tag in dev:\n",
    "        # Padding (can be done in the conv layer as well)\n",
    "        if len(words) < WIN_SIZE:\n",
    "            words += [0] * (WIN_SIZE - len(words))\n",
    "        words_tensor = torch.tensor(words).type(type)\n",
    "        scores = model(words_tensor)[0]\n",
    "        predict = scores.argmax().item()\n",
    "        if predict == tag:\n",
    "            test_correct += 1\n",
    "    print(\"iter %r: test acc=%.4f\" % (ITER, test_correct / len(dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
