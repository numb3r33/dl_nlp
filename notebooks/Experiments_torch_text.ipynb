{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "SEED = 41\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [['This is a cool story'],\n",
    "        ['Aruba is in Jamaica.'],\n",
    "        ['What the hell is going on here?'],\n",
    "        ['The meanining of a word is its use in the language.'],\n",
    "        ['What goes up comes down? me amigo'],\n",
    "        ['This is going to be a sweet sweet ride.'],\n",
    "        ['Man City quadruple is off.'],\n",
    "        ['Scousers are bin dippers.'],\n",
    "        ['Man Utd will be back because we always come back.']\n",
    "       ]\n",
    "\n",
    "text  = pd.DataFrame(text)\n",
    "text.columns = ['comment_text']\n",
    "text.loc[:, 'toxic']  = [1., 0., 1., 1., 0., 0., 0., 1., 0.]\n",
    "text.loc[:, 'severe_toxic']  = [0., 1., 0., 0., 1., 1., 1., 1., 0.]\n",
    "text.loc[:, 'obscene']  = [0., 1., 0., 0., 1., 1., 1., 1., 0.]\n",
    "text.loc[:, 'threat']  = [0., 1., 0., 0., 1., 1., 1., 1., 0.]\n",
    "text.loc[:, 'insult']  = [0., 1., 0., 0., 1., 1., 1., 1., 0.]\n",
    "text.loc[:, 'identity_hate']  = [0., 1., 0., 0., 1., 1., 1., 1., 0.]\n",
    "\n",
    "\n",
    "# text = [['This is a cool story'],\n",
    "#         ['Aruba is in Jamaica.']\n",
    "#        ]\n",
    "\n",
    "# text  = pd.DataFrame(text)\n",
    "# text.columns = ['text']\n",
    "# text.loc[:, 't1']  = [1., 0.]\n",
    "# text.loc[:, 't2']  = [0., 1.]\n",
    "\n",
    "# print(text)\n",
    "# text.to_csv('./sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = [['Kickstart my journey in Europe.'],\n",
    "#         ['Here is the poet whose name is written on water.'],\n",
    "#         ['Tomorrow shall come.'],\n",
    "#         ['Tomorrow is mystery, yesterday was history.']\n",
    "#        ]\n",
    "\n",
    "# text  = pd.DataFrame(text)\n",
    "# text.columns = ['text']\n",
    "# text.loc[:, 't1']  = [1., 0., 1., 1.]\n",
    "# text.loc[:, 't2']  = [0., 1., 0., 0.]\n",
    "\n",
    "\n",
    "# # text = [['This is a cool story'],\n",
    "# #         ['Aruba is in Jamaica.']\n",
    "# #        ]\n",
    "\n",
    "# # text  = pd.DataFrame(text)\n",
    "# # text.columns = ['text']\n",
    "# # text.loc[:, 't1']  = [1., 0.]\n",
    "# # text.loc[:, 't2']  = [0., 1.]\n",
    "\n",
    "# print(text)\n",
    "# text.to_csv('./test_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_labels(y): return all(v is None for v in y)\n",
    "\n",
    "def load_w2v_embedding(emb_matrix):\n",
    "    word2vec_dict   = word2vec.KeyedVectors.load_word2vec_format('../data/processed/word2vec.bin.gz', binary=True)\n",
    "    embedding_index = dict()\n",
    "    \n",
    "    for word in word2vec_dict.wv.vocab:\n",
    "        embedding_index[word] = word2vec_dict.word_vec(word)\n",
    "\n",
    "    embed_cnt = 0\n",
    "\n",
    "    for i, word in enumerate(vocab.itos):\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "\n",
    "        if embedding_vector is not None:\n",
    "            emb_matrix[i] = embedding_vector\n",
    "            embed_cnt +=1\n",
    "\n",
    "    del embedding_index\n",
    "    gc.collect()\n",
    "\n",
    "    # fill pad token with all zeros\n",
    "    emb_matrix[vocab.stoi['xxxpad']] = np.zeros(embed_size)\n",
    "    emb_matrix[vocab.stoi['xxxunk']] = np.zeros(embed_size)\n",
    "    print('total embedded {} common words'.format(embed_cnt))\n",
    "    \n",
    "    return emb_matrix\n",
    "    \n",
    "    \n",
    "class Tokenizer():\n",
    "    def __init__(self, lang='en'):\n",
    "        self.tok = spacy.blank(lang, disable=['parser', 'tagger', 'ner'])\n",
    "    \n",
    "    def tokenizer(self, t):\n",
    "        return [t.text for t in self.tok.tokenizer(t)]\n",
    "    \n",
    "class Vocab():\n",
    "    def __init__(self, itos):\n",
    "        self.itos = itos\n",
    "        self.stoi = defaultdict(int, {v:k for k,v in enumerate(self.itos)})\n",
    "    \n",
    "    def numericalize(self, t):\n",
    "        return [self.stoi.get(w, self.stoi['xxxunk']) for w in t]\n",
    "    \n",
    "    def fix_len(self, sent_len, numericalized_tokens):\n",
    "        return [nt[:sent_len] for nt in numericalized_tokens]\n",
    "    \n",
    "    def __getstate__(self):\n",
    "        return {'itos':self.itos}\n",
    "    \n",
    "    def textify(self, nums, sep=' '):\n",
    "        \"Convert a list of `nums` to their tokens.\"\n",
    "        return sep.join([self.itos[i] for i in nums]) if sep is not None else [self.itos[i] for i in nums]\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, tokens, max_vocab, min_freq):\n",
    "        freq = Counter(p for o in tokens for p in o)\n",
    "        itos = [o for o, c in freq.most_common(max_vocab) if c>= min_freq]\n",
    "        itos = cls.add_special_symbols(itos)\n",
    "        return cls(itos)\n",
    "    \n",
    "    @classmethod\n",
    "    def add_special_symbols(cls, itos):\n",
    "        pad_sym = 'xxxpad' #TODO: make sure we use config to introduce this symbol\n",
    "        unk_sym = 'xxxunk'\n",
    "        itos.append(pad_sym)\n",
    "        itos.append(unk_sym)\n",
    "        return itos\n",
    "\n",
    "class TextLMData():\n",
    "    \n",
    "    def __init__(self, path, \n",
    "                 csv,\n",
    "                 test_csv,\n",
    "                 text_col, \n",
    "                 label_cols, \n",
    "                 max_vocab,\n",
    "                 min_freq,\n",
    "                 valid_pct=0.2):\n",
    "        \n",
    "        self.path       = path\n",
    "        self.csv        = csv\n",
    "        self.test_csv   = test_csv\n",
    "        self.text_cols  = text_col\n",
    "        self.label_cols = label_cols\n",
    "        self.valid_pct  = valid_pct\n",
    "        self.max_vocab  = max_vocab\n",
    "        self.min_freq   = min_freq\n",
    "        \n",
    "        self.df      = pd.read_csv(Path(self.path)/self.csv)\n",
    "        if self.test_csv is not None: self.test_df = pd.read_csv(Path(self.path)/self.test_csv) \n",
    "        self.cut     = int(valid_pct * len(self.df)) + 1\n",
    "        \n",
    "    def process(self):\n",
    "        tok = Tokenizer()\n",
    "        \n",
    "        # consider entire corpus as text ( train + test text columns )\n",
    "        if self.test_csv:\n",
    "            text = list(self.df.loc[:, text_col].values) + list(self.test_df.loc[:, text_col])\n",
    "        else:\n",
    "            text = list(self.df.loc[:, text_col].values)\n",
    "        \n",
    "        self.tokens  = [tok.tokenizer(text) for text in text]\n",
    "        self.vocab   = Vocab.create(self.tokens, self.max_vocab, self.min_freq)\n",
    "        \n",
    "        self.ntokens = [self.vocab.numericalize(t) for t in self.tokens]\n",
    "        \n",
    "        # only full training\n",
    "        if self.valid_pct == 0 and self.test_csv is None:\n",
    "            self.trn_ds      = (self.ntokens, self.df.loc[:, label_cols].values)\n",
    "            self.vld_tokens  = ([], [])\n",
    "            self.test_tokens = ([], [])\n",
    "        \n",
    "        # holdout\n",
    "        elif self.valid_pct > 0 and self.test_csv is None:\n",
    "            self.trn_ds  = (self.ntokens[self.cut:], self.df.loc[:, label_cols].values[self.cut:])\n",
    "            self.vld_ds  = (self.ntokens[:self.cut], self.df.loc[:, label_cols].values[:self.cut])\n",
    "            self.tst_ds  = ([], [])\n",
    "        \n",
    "        # holdout and test prediction\n",
    "        elif self.valid_pct > 0 and self.test_csv is not None:\n",
    "            self.trn_tokens  = self.ntokens[:len(self.df)]\n",
    "            self.tst_ds      = (self.ntokens[len(self.df):], [])\n",
    "            \n",
    "            trn_tokens  = self.trn_tokens[self.cut:]\n",
    "            vld_tokens  = self.trn_tokens[:self.cut]\n",
    "            \n",
    "            self.trn_ds = (trn_tokens, self.df.loc[:, label_cols].values[self.cut:])\n",
    "            self.vld_ds = (vld_tokens, self.df.loc[:, label_cols].values[:self.cut])\n",
    "        \n",
    "        # full training and test prediction\n",
    "        else:\n",
    "            self.trn_ds  = (self.ntokens[:len(self.df)], self.df.loc[:, label_cols].values)\n",
    "            self.vld_ds  = ([], [])\n",
    "            self.tst_ds  = (self.ntokens[len(self.df):], [])\n",
    "            \n",
    "        return self.vocab, self.trn_ds, self.vld_ds, self.tst_ds\n",
    "    \n",
    "    def fill_emb_matrix(self, vocab, emb_type, embed_size):\n",
    "        emb_matrix = np.random.random(size=(len(self.vocab.itos), embed_size))\n",
    "        \n",
    "        if emb_type == 'w2v':\n",
    "            emb_matrix = load_w2v_embedding(emb_matrix)\n",
    "            \n",
    "        return emb_matrix\n",
    "        \n",
    "class TextClassData(Dataset):\n",
    "    def __init__(self, vocab, ds):\n",
    "        self.vocab       = vocab\n",
    "        self.ds, self.y  = ds\n",
    "                            \n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "                            \n",
    "    def __getitem__(self, index):\n",
    "        x = torch.LongTensor(self.ds[index])\n",
    "        y = None\n",
    "        if len(self.y) > 0: y = torch.FloatTensor(self.y[index])\n",
    "        \n",
    "        return x, y\n",
    "        \n",
    "def pad_collate(data, pad_idx, sent_len):\n",
    "    if len(data) == 1:\n",
    "        sequences, labels = data[0]\n",
    "        sequences = sequences.view(1, -1)\n",
    "        if labels is not None: labels    = labels.view(1, -1)\n",
    "    else:\n",
    "        sequences, labels = zip(*data)\n",
    "        if not check_labels(labels): labels = torch.cat([l.view(-1, 1) for l in labels], dim=1).t()\n",
    "        sequences = nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=pad_idx)\n",
    "    \n",
    "    sent_len  = min(sequences.size(1), sent_len)\n",
    "    sequences = sequences[:, :sent_len]\n",
    "    \n",
    "    return sequences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path       = Path('../data/processed')\n",
    "csv        = 'sample.csv'\n",
    "text_col   = 'comment_text'\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "test_csv   = None\n",
    "max_vocab  = 100000\n",
    "min_freq   = 2\n",
    "embed_size = 300\n",
    "emb_type   = 'glove'\n",
    "valid_pct  = .2 # change it to 0 for full training modmax_vocab  = 100000\n",
    "embed_size = 300\n",
    "valid_pct  = .2\n",
    "\n",
    "tmp = TextLMData(path, \n",
    "                 csv,\n",
    "                 test_csv,\n",
    "                 text_col, \n",
    "                 label_cols,\n",
    "                 max_vocab,\n",
    "                 min_freq,\n",
    "                 valid_pct=valid_pct\n",
    "                )\n",
    "\n",
    "vocab, trn_ds, vld_ds, tst_ds = tmp.process()\n",
    "emb_matrix                    = tmp.fill_emb_matrix(vocab, emb_type, embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test cases for TextLMData class\n",
    "\n",
    "# assert len(vocab.itos) == 10\n",
    "# assert len(vocab.stoi) == 10\n",
    "# assert vocab.textify(trn_ds[0][0]) == 'Aruba is in Jamaica .'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_len   = 200\n",
    "collate_fn = partial(pad_collate, pad_idx=vocab.stoi['xxxpad'], sent_len=sent_len) #TODO: make sure pad symbol is defined as a constant\n",
    "\n",
    "trn_ds     = TextClassData(vocab, trn_ds)\n",
    "vld_ds     = TextClassData(vocab, vld_ds)\n",
    "# tst_ds     = TextClassData(vocab, tst_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl     = torch.utils.data.DataLoader(trn_ds, batch_size=512, shuffle=True, collate_fn=collate_fn, num_workers=4)\n",
    "vld_dl     = torch.utils.data.DataLoader(vld_ds, batch_size=1024, shuffle=False, collate_fn=collate_fn, num_workers=4)\n",
    "# tst_dl     = torch.utils.data.DataLoader(tst_ds, batch_size=32, shuffle=False, collate_fn=collate_fn, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3,  0,  7, 14,  9,  2, 10, 10, 14,  1, 13, 13],\n",
      "        [14, 14, 14, 14,  1, 13, 13, 13, 13, 13, 13, 13],\n",
      "        [11, 14, 14,  0, 14,  1, 13, 13, 13, 13, 13, 13],\n",
      "        [ 5, 14, 14, 14, 14,  8, 14, 14, 13, 13, 13, 13],\n",
      "        [11, 14, 14,  9, 12, 14, 14, 14, 14, 12,  1, 13],\n",
      "        [ 5,  6, 14,  0,  7, 14, 14,  8, 13, 13, 13, 13],\n",
      "        [14, 14, 14,  2, 14,  0, 14, 14,  4,  6, 14,  1]])\n"
     ]
    }
   ],
   "source": [
    "x, _ = next(iter(trn_dl))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test cases for data loader\n",
    "# x, _ = next(iter(trn_dl))\n",
    "\n",
    "# assert x.size(1) == sent_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, emb_matrix, vocab_size, embed_size):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.embedding        = nn.Embedding(vocab_size, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(emb_matrix)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "#         self.ns_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "#         self.ns_embedding.weight = nn.Parameter(emb_matrix)\n",
    "        \n",
    "#         self.conv0 = nn.Conv2d(in_channels=1,\n",
    "#                                out_channels=2,\n",
    "#                                kernel_size=(2, embed_size)\n",
    "#                               )\n",
    "\n",
    "        self.n_filters    = [100, 100, 100]\n",
    "        self.filter_sizes = [3, 4, 5]\n",
    "        self.convs        = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1, \n",
    "                                              out_channels = self.n_filters[i], \n",
    "                                              kernel_size = (fs, embed_size)) \n",
    "                                    for i, fs in enumerate(self.filter_sizes)\n",
    "                                    ])\n",
    "        \n",
    "        self.relu     = nn.ReLU()\n",
    "        self.dropout  = nn.Dropout(0.5)\n",
    "        self.spatial_dropout = nn.Dropout2d(0.3)\n",
    "        self.fc       = nn.Linear(np.sum(self.n_filters), 6)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embed    = self.embedding(x)\n",
    "#         ns_embed = self.ns_embedding(x) \n",
    "        \n",
    "        # introduce channel\n",
    "        embed    = embed.unsqueeze(1)\n",
    "#         ns_embed = ns_embed.unsqueeze(1)\n",
    "        \n",
    "#         embed    = torch.cat((embed, ns_embed), dim=1)\n",
    "        \n",
    "        # spatial dropout on channels\n",
    "        embed    = self.spatial_dropout(embed)\n",
    "        \n",
    "        conved = [self.relu(conv(embed)).squeeze(3) for conv in self.convs]\n",
    "            \n",
    "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]        \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "    \n",
    "        # pass to dropout\n",
    "        out    = self.dropout(torch.cat(pooled, dim=1))\n",
    "        \n",
    "        # pass to fully connected layer\n",
    "        fc     = self.fc(out)\n",
    "        \n",
    "        return fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab.itos)\n",
    "embed_size = 300\n",
    "lr         = 1e-2\n",
    "\n",
    "model      = CNN(torch.FloatTensor(emb_matrix), vocab_size, embed_size)\n",
    "\n",
    "device     = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "optimizer  = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion  = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model      = model.to(device)\n",
    "criterion  = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss      = 0\n",
    "    per_label_preds = [[], [], [], [], [], []]\n",
    "    per_label_true  = [[], [], [], [], [], []]\n",
    "\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        X, y        = batch        \n",
    "        \n",
    "        X           = X.to(device)\n",
    "        y           = y.to(device)\n",
    "        \n",
    "        predictions = model(X)\n",
    "        \n",
    "        loss = criterion(predictions, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # convert true target\n",
    "        batch_target = y.cpu().detach().numpy()\n",
    "        logits_cpu   = predictions.cpu().detach().numpy()\n",
    "\n",
    "        # per_label_preds\n",
    "        for j in range(6):\n",
    "            label_preds     = logits_cpu[:, j]\n",
    "            per_label_preds[j].extend(label_preds)\n",
    "            per_label_true[j].extend(batch_target[:, j])\n",
    "\n",
    "        # calculate log loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        print('\\r[{} / {}]: Loss = {:.4f}'.format(\n",
    "              i, len(iterator), loss.item(), end=''))\n",
    "    \n",
    "    label_auc = []\n",
    "\n",
    "    for i in range(6):\n",
    "        label_auc.append(roc_auc_score(per_label_true[i], per_label_preds[i]))\n",
    "    \n",
    "    return epoch_loss / len(iterator), np.mean(label_auc)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss      = 0\n",
    "    per_label_preds = [[], [], [], [], [], []]\n",
    "    per_label_true  = [[], [], [], [], [], []]\n",
    "    preds           = []\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "            X, y        = batch\n",
    "            \n",
    "            X           = X.to(device)\n",
    "            predictions = model(X)\n",
    "            \n",
    "            # convert true target\n",
    "            logits_cpu   = predictions.cpu().detach().numpy()\n",
    "\n",
    "            preds.append(logits_cpu)\n",
    "            \n",
    "            if not check_labels(y): \n",
    "                y    = y.to(device)\n",
    "                loss = criterion(predictions, y)\n",
    "                batch_target = y.cpu().detach().numpy()\n",
    "                \n",
    "                \n",
    "                # per_label_preds\n",
    "                for j in range(6):\n",
    "                    label_preds     = logits_cpu[:, j]\n",
    "                    per_label_preds[j].extend(label_preds)\n",
    "                    per_label_true[j].extend(batch_target[:, j])\n",
    "\n",
    "                # calculate log loss\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                print('\\r[{} / {}]: Loss = {:.4f}'.format(\n",
    "                      i, len(iterator), loss.item(), end=''))\n",
    "    \n",
    "    label_auc = []\n",
    "\n",
    "    if len(per_label_preds[0]) > 0:\n",
    "        for i in range(6):\n",
    "            label_auc.append(roc_auc_score(per_label_true[i], per_label_preds[i]))\n",
    "\n",
    "    return epoch_loss / len(iterator), np.mean(label_auc) if len(label_auc) > 0 else 0, np.vstack(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 / 1]: Loss = 0.6953\n",
      "[0 / 1]: Loss = 4.3470\n",
      "Epoch: 01 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.695 | Train AUC: 0.57\n",
      "\t Val. Loss: 4.347 |  Val. AUC: 0.50\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_auc    = train(model, trn_dl, optimizer, criterion)\n",
    "    valid_loss, valid_auc, _ = evaluate(model, vld_dl, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "#         torch.save(model.state_dict(), 'tut4-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train AUC: {train_auc:.2f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. AUC: {valid_auc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final predictions\n",
    "_, _, final_preds = evaluate(model, tst_dl, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.559488  ,  0.19406459],\n",
       "       [-0.5456122 ,  0.13150282],\n",
       "       [-0.56275576,  0.20879813],\n",
       "       [-0.5560646 ,  0.17862955]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
