{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from collections import Counter\n",
    "\n",
    "SEED = 41\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_PATH        = '../data/raw/'\n",
    "PROCESSED_DATA_PATH  = '../data/processed/' \n",
    "\n",
    "MAX_LEN = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample():\n",
    "    return pd.read_csv(os.path.join(PROCESSED_DATA_PATH, 'train_sample.csv'))\n",
    "\n",
    "def load_full():\n",
    "    train       = pd.read_csv(os.path.join(RAW_DATA_PATH, 'train.csv'))\n",
    "    test        = pd.read_csv(os.path.join(RAW_DATA_PATH, 'test.csv'))\n",
    "    test_labels = pd.read_csv(os.path.join(RAW_DATA_PATH, 'test_labels.csv'))\n",
    "    \n",
    "    return train, test, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.47 s, sys: 224 ms, total: 1.69 s\n",
      "Wall time: 1.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train, _, _ = load_full()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 180 ms, sys: 12 ms, total: 192 ms\n",
      "Wall time: 187 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = load_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define target columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLS = ['toxic', \n",
    "               'severe_toxic', \n",
    "               'obscene', \n",
    "               'threat', \n",
    "               'insult', \n",
    "               'identity_hate'\n",
    "              ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tokenizer\n",
    "tokenizer = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.31 s, sys: 348 ms, total: 5.66 s\n",
      "Wall time: 5.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_tokenized_comments = list(map(tokenizer.tokenize, train.comment_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.94 s, sys: 3.92 s, total: 9.87 s\n",
      "Wall time: 9.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train.loc[:, 'tokenized_comments'] = list(map(' '.join, map(tokenizer.tokenize, train.comment_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create word freq mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = Counter()\n",
    "\n",
    "for tok_comments in train_tokenized_comments:\n",
    "    token_counts.update(tok_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary: 32838\n"
     ]
    }
   ],
   "source": [
    "# we can put a threshold on the token frequency to reduce the vocabulary\n",
    "tokens    = {}\n",
    "min_count = 10\n",
    "\n",
    "for token, freq in token_counts.items():\n",
    "    if freq >= min_count:\n",
    "        tokens[token] = freq\n",
    "        \n",
    "print('Size of the vocabulary: {}'.format(len(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wv_embedding_matrix(words):\n",
    "    word2vec_dict = word2vec.KeyedVectors.load_word2vec_format(os.path.join(PROCESSED_DATA_PATH, 'word2vec.bin.gz'), binary=True)\n",
    "    embed_size    = 300\n",
    "\n",
    "    embedding_index = dict()\n",
    "    for word in word2vec_dict.wv.vocab:\n",
    "        embedding_index[word] = word2vec_dict.word_vec(word)\n",
    "\n",
    "    print('Loaded %d word vectors'%(len(embedding_index)))\n",
    "\n",
    "    all_embs          = np.stack(list(embedding_index.values()))\n",
    "    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "    UNK, PAD       = 'UNK', 'PAD'\n",
    "    UNK_IX, PAD_IX = len(words), len(words) + 1\n",
    "\n",
    "    nb_words = len(words) + 2\n",
    "\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "\n",
    "    embed_cnt = 0\n",
    "    for i, word in enumerate(list(words.keys()) + [UNK, PAD]):\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            embed_cnt +=1\n",
    "\n",
    "    print('total embedded ', embed_cnt, ' common words')\n",
    "    del embedding_index\n",
    "    gc.collect()\n",
    "\n",
    "    return embedding_matrix, UNK, PAD, UNK_IX, PAD_IX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token to ID mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token to index (manual)\n",
    "# UNK, PAD       = 'UNK', 'PAD'\n",
    "# UNK_IX, PAD_IX =  0, 1\n",
    "\n",
    "# token_to_id = {UNK: UNK_IX,\n",
    "#                PAD: PAD_IX\n",
    "#               }\n",
    "\n",
    "# for token in tokens.keys():\n",
    "#     token_to_id[token] = len(token_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3000000 word vectors\n",
      "total embedded  29714  common words\n",
      "CPU times: user 2min 38s, sys: 8.93 s, total: 2min 47s\n",
      "Wall time: 2min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# token to index ( word2vec embeddings )\n",
    "embedding_matrix, UNK, PAD, UNK_IX, PAD_IX = load_wv_embedding_matrix(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id      = {word: index for index, word in enumerate(tokens.keys())}\n",
    "token_to_id[UNK] = UNK_IX\n",
    "token_to_id[PAD] = PAD_IX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad Sequences and convert map tokens to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IX, PAD_IX = map(token_to_id.get, [UNK, PAD])\n",
    "\n",
    "def as_matrix(sequences, token_to_id, word_dropout, UNK_IX, PAD_IX, max_len=None):\n",
    "    \"\"\" Convert a list of tokens into a matrix with padding \"\"\"\n",
    "\n",
    "    if isinstance(sequences[0], str):\n",
    "        sequences = list(map(str.split, sequences))\n",
    "\n",
    "    max_len = min(max(map(len, sequences)), max_len or float('inf'))\n",
    "    matrix = np.full((len(sequences), max_len), np.int32(PAD_IX))\n",
    "\n",
    "    for i,seq in enumerate(sequences):\n",
    "        row_ix = [token_to_id.get(word, UNK_IX) for word in seq[:max_len]]\n",
    "        matrix[i, :len(row_ix)] = row_ix\n",
    "\n",
    "    if word_dropout != 0:\n",
    "        matrix = apply_word_dropout(matrix, 1 - word_dropout, replace_with=UNK_IX, pad_ix=PAD_IX)\n",
    "\n",
    "    return matrix\n",
    "\n",
    "def apply_word_dropout(matrix, keep_prop, replace_with, pad_ix):\n",
    "    dropout_mask = np.random.choice(2, np.shape(matrix), p=[keep_prop, 1-keep_prop])\n",
    "    dropout_mask &= matrix != pad_ix\n",
    "    return np.choose(dropout_mask, [matrix, np.full_like(matrix, replace_with)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size =  127656\n",
      "Validation size =  31915\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_val = train_test_split(train, test_size=0.2, random_state=42)\n",
    "data_train.index     = range(len(data_train))\n",
    "data_val.index       = range(len(data_val))\n",
    "\n",
    "print(\"Train size = \", len(data_train))\n",
    "print(\"Validation size = \", len(data_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(matrix, labels, batch_size, predict_mode='train'):\n",
    "    indices = np.arange(len(matrix))\n",
    "    if predict_mode == 'train':\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, len(matrix), batch_size):\n",
    "        end = min(start + batch_size, len(matrix))\n",
    "        \n",
    "        batch_indices = indices[start: end]\n",
    "        X = matrix[batch_indices]\n",
    "        \n",
    "        if predict_mode != 'train': yield X\n",
    "        else: yield X, labels[batch_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "    \n",
    "class MultiChannel(nn.Module):\n",
    "    def __init__(self, pre_trained_embedding, vocab_size, embed_size, num_classes):\n",
    "        super(MultiChannel, self).__init__()\n",
    "        \n",
    "        self.embed_size  = embed_size\n",
    "        self.vocab_size  = vocab_size\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = 2\n",
    "        self.nfms        = 32\n",
    "        self.ks          = [2, 3, 4, 5]\n",
    "        \n",
    "        # first embedding layer ( static )\n",
    "        self.static_embedding        = nn.Embedding(self.vocab_size, self.embed_size)\n",
    "        self.static_embedding.weight = nn.Parameter(pre_trained_embedding)\n",
    "        \n",
    "        # make it non-trainable\n",
    "        self.static_embedding.weight.requires_grad  = False\n",
    "        \n",
    "        # second embedding layer ( non-static )\n",
    "        self.non_static_embedding        = nn.Embedding(self.vocab_size, self.embed_size)\n",
    "        self.non_static_embedding.weight = nn.Parameter(pre_trained_embedding)\n",
    "        \n",
    "        # define conv layers\n",
    "        self.conv_layer1 = nn.Conv2d(self.in_channels, self.nfms, kernel_size=(self.ks[0], self.embed_size), padding=1)\n",
    "        self.conv_layer2 = nn.Conv2d(self.in_channels, self.nfms, kernel_size=(self.ks[1], self.embed_size), padding=1)\n",
    "        self.conv_layer3 = nn.Conv2d(self.in_channels, self.nfms, kernel_size=(self.ks[2], self.embed_size), padding=1)\n",
    "        self.conv_layer4 = nn.Conv2d(self.in_channels, self.nfms, kernel_size=(self.ks[3], self.embed_size), padding=1) \n",
    "        \n",
    "        # define activation function\n",
    "        self.relu        = nn.ReLU()\n",
    "        \n",
    "        # define max pooling layer\n",
    "        self.max_pool1   = nn.MaxPool2d(kernel_size=(MAX_LEN - self.ks[0] + 1, 1))\n",
    "        self.max_pool2   = nn.MaxPool2d(kernel_size=(MAX_LEN - self.ks[1] + 1, 1))\n",
    "        self.max_pool3   = nn.MaxPool2d(kernel_size=(MAX_LEN - self.ks[2] + 1, 1))\n",
    "        self.max_pool4   = nn.MaxPool2d(kernel_size=(MAX_LEN - self.ks[3] + 1, 1))\n",
    "        \n",
    "        # fully connected layer\n",
    "        self.fc = nn.Linear(self.nfms * (3 * len(self.ks)), self.num_classes)\n",
    "        \n",
    "        # flatten layer\n",
    "        self.flatten = Flatten()\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # spatial dropout\n",
    "        self.spatial_dropout = nn.Dropout2d(0.4)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        s_embed  = self.static_embedding(x)\n",
    "        ns_embed = self.non_static_embedding(x)\n",
    "        \n",
    "        # batch, seq, embedding -> batch, embedding, seq\n",
    "        s_embed_t  = torch.transpose(s_embed, 1, 2)\n",
    "        s_embed_t  = self.spatial_dropout(s_embed_t)\n",
    "        s_embed    = torch.transpose(s_embed_t, 1, 2)\n",
    "        \n",
    "        ns_embed_t = torch.transpose(ns_embed, 1, 2)\n",
    "        ns_embed_t = self.spatial_dropout(ns_embed_t)\n",
    "        ns_embed   = torch.transpose(ns_embed_t, 1, 2)\n",
    "        \n",
    "        del s_embed_t\n",
    "        del ns_embed_t\n",
    "        \n",
    "        # change embedding to batch, channel, seq and elements\n",
    "        s_embed  = s_embed.unsqueeze(1)\n",
    "        ns_embed = ns_embed.unsqueeze(1)\n",
    "        \n",
    "        out      = torch.cat((s_embed, ns_embed), dim=1)\n",
    "        \n",
    "        # pass through first conv layer\n",
    "        out1     = self.conv_layer1(out)\n",
    "        out1     = self.relu(out1)\n",
    "        \n",
    "        # pass through second conv layer\n",
    "        out2     = self.conv_layer2(out)\n",
    "        out2     = self.relu(out2)\n",
    "        \n",
    "        # pass through third conv layer\n",
    "        out3     = self.conv_layer3(out)\n",
    "        out3     = self.relu(out3)\n",
    "        \n",
    "        # pass through fourth conv layer\n",
    "        out4     = self.conv_layer4(out)\n",
    "        out4     = self.relu(out4)\n",
    "        \n",
    "#         print('Conv')\n",
    "#         print('out1 : {}'.format(out1.shape))\n",
    "#         print('out2 : {}'.format(out2.shape))\n",
    "#         print('out3 : {}'.format(out3.shape))\n",
    "#         print()\n",
    "        \n",
    "        # max pooling over sequence\n",
    "        out1      = self.max_pool1(out1)\n",
    "        out2      = self.max_pool2(out2)\n",
    "        out3      = self.max_pool3(out3)\n",
    "        out4      = self.max_pool3(out4)\n",
    "        \n",
    "#         print('Max Pooling')\n",
    "#         print('out1 : {}'.format(out1.shape))\n",
    "#         print('out2 : {}'.format(out2.shape))\n",
    "#         print('out3 : {}'.format(out3.shape))\n",
    "        \n",
    "        # concatenate along first axis\n",
    "        out = torch.cat((out1, out2, out3, out4), dim=1)\n",
    "        \n",
    "        # flatten channels, seq and element -> batch, channels * seq * element\n",
    "        out      = self.flatten(out)\n",
    "        \n",
    "#         print('shape after flatten ', out.shape)\n",
    "        \n",
    "        # dropout\n",
    "        out      = self.dropout(out)\n",
    "                \n",
    "        # pass through fully connected layer\n",
    "        out      = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, criterion, data, batch_size, optimizer=None):\n",
    "    epoch_loss, total_size = 0, 0\n",
    "    per_label_preds = [[], [], [], [], [], []]\n",
    "    per_label_true  = [[], [], [], [], [], []]\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    model.train(is_train)\n",
    "    \n",
    "    data, labels = data\n",
    "    batchs_count = math.ceil(data.shape[0] / batch_size)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        for i, (X_batch, y_batch) in enumerate(iterate_batches(data, labels, batch_size)):\n",
    "            X_batch, y_batch = torch.cuda.LongTensor(X_batch), torch.cuda.FloatTensor(y_batch)\n",
    "\n",
    "            logits = model(X_batch)\n",
    "            loss   = criterion(logits, y_batch)\n",
    "            \n",
    "            if is_train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            # convert true target\n",
    "            batch_target = y_batch.cpu().detach().numpy()\n",
    "            logits_cpu   = logits.cpu().detach().numpy()\n",
    "            \n",
    "            # per_label_preds\n",
    "            for j in range(6):\n",
    "                label_preds     = logits_cpu[:, j]\n",
    "                per_label_preds[j].extend(label_preds)\n",
    "                per_label_true[j].extend(batch_target[:, j])\n",
    "                            \n",
    "            # calculate log loss\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            print('\\r[{} / {}]: Loss = {:.4f}'.format(\n",
    "                  i, batchs_count, loss.item(), end=''))\n",
    "    \n",
    "    label_auc = []\n",
    "    \n",
    "    for i in range(6):\n",
    "        label_auc.append(roc_auc_score(per_label_true[i], per_label_preds[i]))\n",
    "    \n",
    "    return epoch_loss / batchs_count, np.mean(label_auc)\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, \n",
    "        batch_size=32, val_data=None, val_batch_size=None):\n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        start_time = time.time()\n",
    "        train_loss, train_auc = do_epoch(\n",
    "            model, criterion, train_data, batch_size, optimizer\n",
    "        )\n",
    "        \n",
    "        output_info = '\\rEpoch {} / {}, Epoch Time = {:.2f}s: Train Loss = {:.4f}, Train AUC = {:.4f}'\n",
    "        if not val_data is None:\n",
    "            val_loss, val_auc   = do_epoch(model, criterion, val_data, val_batch_size, None)\n",
    "            \n",
    "            epoch_time   = time.time() - start_time\n",
    "            output_info += ', Val Loss = {:.4f}, Val AUC = {:.4f}'\n",
    "            print(output_info.format(epoch+1, epochs_count, epoch_time, \n",
    "                                     train_loss,\n",
    "                                     train_auc,\n",
    "                                     val_loss,\n",
    "                                     val_auc\n",
    "                                    ))\n",
    "        else:\n",
    "            epoch_time = time.time() - start_time\n",
    "            print(output_info.format(epoch+1, epochs_count, epoch_time, train_loss, train_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on a single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = as_matrix(data_train['tokenized_comments'], \n",
    "                   token_to_id, \n",
    "                   word_dropout=0, \n",
    "                   UNK_IX=UNK_IX, \n",
    "                   PAD_IX=PAD_IX,\n",
    "                   max_len=MAX_LEN\n",
    "                  )\n",
    "\n",
    "labels = data_train.loc[:, TARGET_COLS].values\n",
    "X, y   = next(iterate_batches(matrix, labels, batch_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.cuda.LongTensor(X)\n",
    "y = torch.cuda.LongTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size  = len(token_to_id)\n",
    "embed_size  = 300\n",
    "num_classes = 6\n",
    "\n",
    "model = MultiChannel(torch.FloatTensor(embedding_matrix), \n",
    "                     vocab_size, \n",
    "                     embed_size,\n",
    "                     num_classes\n",
    "                    ).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out1 : torch.Size([2, 32, 101, 3])\n",
      "out2 : torch.Size([2, 32, 100, 3])\n",
      "out3 : torch.Size([2, 32, 99, 3])\n",
      "\n",
      "out1 : torch.Size([2, 32, 1, 3])\n",
      "out2 : torch.Size([2, 32, 1, 3])\n",
      "out3 : torch.Size([2, 32, 1, 3])\n",
      "shape after flatten  torch.Size([2, 384])\n",
      "tensor([[ 0.1082, -0.1250,  0.0540,  0.0787,  0.1425, -0.0455],\n",
      "        [ 0.1440, -0.1389,  0.1897,  0.1586,  0.1045,  0.1465]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "logits = model(X)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on full batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 / 250]: Loss = 0.7188\n",
      "[1 / 250]: Loss = 0.5770\n",
      "[2 / 250]: Loss = 0.4508\n",
      "[3 / 250]: Loss = 0.3552\n",
      "[4 / 250]: Loss = 0.2681\n",
      "[5 / 250]: Loss = 0.1899\n",
      "[6 / 250]: Loss = 0.1967\n",
      "[7 / 250]: Loss = 0.1959\n",
      "[8 / 250]: Loss = 0.1869\n",
      "[9 / 250]: Loss = 0.1457\n",
      "[10 / 250]: Loss = 0.1814\n",
      "[11 / 250]: Loss = 0.1775\n",
      "[12 / 250]: Loss = 0.2292\n",
      "[13 / 250]: Loss = 0.2387\n",
      "[14 / 250]: Loss = 0.1985\n",
      "[15 / 250]: Loss = 0.2140\n",
      "[16 / 250]: Loss = 0.2229\n",
      "[17 / 250]: Loss = 0.1788\n",
      "[18 / 250]: Loss = 0.1559\n",
      "[19 / 250]: Loss = 0.1723\n",
      "[20 / 250]: Loss = 0.1358\n",
      "[21 / 250]: Loss = 0.1387\n",
      "[22 / 250]: Loss = 0.1284\n",
      "[23 / 250]: Loss = 0.1480\n",
      "[24 / 250]: Loss = 0.1655\n",
      "[25 / 250]: Loss = 0.1761\n",
      "[26 / 250]: Loss = 0.1281\n",
      "[27 / 250]: Loss = 0.1622\n",
      "[28 / 250]: Loss = 0.1459\n",
      "[29 / 250]: Loss = 0.1351\n",
      "[30 / 250]: Loss = 0.1187\n",
      "[31 / 250]: Loss = 0.1295\n",
      "[32 / 250]: Loss = 0.1594\n",
      "[33 / 250]: Loss = 0.1936\n",
      "[34 / 250]: Loss = 0.1332\n",
      "[35 / 250]: Loss = 0.1527\n",
      "[36 / 250]: Loss = 0.1246\n",
      "[37 / 250]: Loss = 0.1102\n",
      "[38 / 250]: Loss = 0.1443\n",
      "[39 / 250]: Loss = 0.1707\n",
      "[40 / 250]: Loss = 0.1600\n",
      "[41 / 250]: Loss = 0.1376\n",
      "[42 / 250]: Loss = 0.1141\n",
      "[43 / 250]: Loss = 0.1424\n",
      "[44 / 250]: Loss = 0.1309\n",
      "[45 / 250]: Loss = 0.1487\n",
      "[46 / 250]: Loss = 0.1257\n",
      "[47 / 250]: Loss = 0.1097\n",
      "[48 / 250]: Loss = 0.1125\n",
      "[49 / 250]: Loss = 0.1352\n",
      "[50 / 250]: Loss = 0.1402\n",
      "[51 / 250]: Loss = 0.1323\n",
      "[52 / 250]: Loss = 0.1433\n",
      "[53 / 250]: Loss = 0.1132\n",
      "[54 / 250]: Loss = 0.1319\n",
      "[55 / 250]: Loss = 0.1298\n",
      "[56 / 250]: Loss = 0.1106\n",
      "[57 / 250]: Loss = 0.0950\n",
      "[58 / 250]: Loss = 0.1062\n",
      "[59 / 250]: Loss = 0.1063\n",
      "[60 / 250]: Loss = 0.0986\n",
      "[61 / 250]: Loss = 0.1117\n",
      "[62 / 250]: Loss = 0.1022\n",
      "[63 / 250]: Loss = 0.0964\n",
      "[64 / 250]: Loss = 0.1133\n",
      "[65 / 250]: Loss = 0.1266\n",
      "[66 / 250]: Loss = 0.1091\n",
      "[67 / 250]: Loss = 0.1239\n",
      "[68 / 250]: Loss = 0.0911\n",
      "[69 / 250]: Loss = 0.1170\n",
      "[70 / 250]: Loss = 0.0971\n",
      "[71 / 250]: Loss = 0.1040\n",
      "[72 / 250]: Loss = 0.0743\n",
      "[73 / 250]: Loss = 0.1083\n",
      "[74 / 250]: Loss = 0.1104\n",
      "[75 / 250]: Loss = 0.0912\n",
      "[76 / 250]: Loss = 0.0928\n",
      "[77 / 250]: Loss = 0.1076\n",
      "[78 / 250]: Loss = 0.1159\n",
      "[79 / 250]: Loss = 0.1103\n",
      "[80 / 250]: Loss = 0.0928\n",
      "[81 / 250]: Loss = 0.1144\n",
      "[82 / 250]: Loss = 0.0889\n",
      "[83 / 250]: Loss = 0.0954\n",
      "[84 / 250]: Loss = 0.0923\n",
      "[85 / 250]: Loss = 0.0788\n",
      "[86 / 250]: Loss = 0.0886\n",
      "[87 / 250]: Loss = 0.1109\n",
      "[88 / 250]: Loss = 0.0946\n",
      "[89 / 250]: Loss = 0.0955\n",
      "[90 / 250]: Loss = 0.0826\n",
      "[91 / 250]: Loss = 0.0925\n",
      "[92 / 250]: Loss = 0.1063\n",
      "[93 / 250]: Loss = 0.0990\n",
      "[94 / 250]: Loss = 0.0813\n",
      "[95 / 250]: Loss = 0.0759\n",
      "[96 / 250]: Loss = 0.0776\n",
      "[97 / 250]: Loss = 0.0701\n",
      "[98 / 250]: Loss = 0.0843\n",
      "[99 / 250]: Loss = 0.0856\n",
      "[100 / 250]: Loss = 0.0825\n",
      "[101 / 250]: Loss = 0.0612\n",
      "[102 / 250]: Loss = 0.0829\n",
      "[103 / 250]: Loss = 0.0556\n",
      "[104 / 250]: Loss = 0.0817\n",
      "[105 / 250]: Loss = 0.0805\n",
      "[106 / 250]: Loss = 0.0779\n",
      "[107 / 250]: Loss = 0.0745\n",
      "[108 / 250]: Loss = 0.0757\n",
      "[109 / 250]: Loss = 0.0535\n",
      "[110 / 250]: Loss = 0.0798\n",
      "[111 / 250]: Loss = 0.0604\n",
      "[112 / 250]: Loss = 0.0661\n",
      "[113 / 250]: Loss = 0.0652\n",
      "[114 / 250]: Loss = 0.0744\n",
      "[115 / 250]: Loss = 0.0815\n",
      "[116 / 250]: Loss = 0.0847\n",
      "[117 / 250]: Loss = 0.0748\n",
      "[118 / 250]: Loss = 0.0737\n",
      "[119 / 250]: Loss = 0.0813\n",
      "[120 / 250]: Loss = 0.0666\n",
      "[121 / 250]: Loss = 0.0742\n",
      "[122 / 250]: Loss = 0.0661\n",
      "[123 / 250]: Loss = 0.0642\n",
      "[124 / 250]: Loss = 0.0602\n",
      "[125 / 250]: Loss = 0.0710\n",
      "[126 / 250]: Loss = 0.0730\n",
      "[127 / 250]: Loss = 0.0497\n",
      "[128 / 250]: Loss = 0.0763\n",
      "[129 / 250]: Loss = 0.0655\n",
      "[130 / 250]: Loss = 0.0681\n",
      "[131 / 250]: Loss = 0.0570\n",
      "[132 / 250]: Loss = 0.0613\n",
      "[133 / 250]: Loss = 0.0560\n",
      "[134 / 250]: Loss = 0.0541\n",
      "[135 / 250]: Loss = 0.0569\n",
      "[136 / 250]: Loss = 0.0695\n",
      "[137 / 250]: Loss = 0.0716\n",
      "[138 / 250]: Loss = 0.0586\n",
      "[139 / 250]: Loss = 0.0773\n",
      "[140 / 250]: Loss = 0.0551\n",
      "[141 / 250]: Loss = 0.0712\n",
      "[142 / 250]: Loss = 0.0617\n",
      "[143 / 250]: Loss = 0.0669\n",
      "[144 / 250]: Loss = 0.0623\n",
      "[145 / 250]: Loss = 0.0656\n",
      "[146 / 250]: Loss = 0.0682\n",
      "[147 / 250]: Loss = 0.0604\n",
      "[148 / 250]: Loss = 0.0613\n",
      "[149 / 250]: Loss = 0.0502\n",
      "[150 / 250]: Loss = 0.0502\n",
      "[151 / 250]: Loss = 0.0658\n",
      "[152 / 250]: Loss = 0.0600\n",
      "[153 / 250]: Loss = 0.0537\n",
      "[154 / 250]: Loss = 0.0617\n",
      "[155 / 250]: Loss = 0.0664\n",
      "[156 / 250]: Loss = 0.0547\n",
      "[157 / 250]: Loss = 0.0520\n",
      "[158 / 250]: Loss = 0.0656\n",
      "[159 / 250]: Loss = 0.0638\n",
      "[160 / 250]: Loss = 0.0448\n",
      "[161 / 250]: Loss = 0.0734\n",
      "[162 / 250]: Loss = 0.0609\n",
      "[163 / 250]: Loss = 0.0613\n",
      "[164 / 250]: Loss = 0.0558\n",
      "[165 / 250]: Loss = 0.0422\n",
      "[166 / 250]: Loss = 0.0531\n",
      "[167 / 250]: Loss = 0.0512\n",
      "[168 / 250]: Loss = 0.0521\n",
      "[169 / 250]: Loss = 0.0685\n",
      "[170 / 250]: Loss = 0.0684\n",
      "[171 / 250]: Loss = 0.0446\n",
      "[172 / 250]: Loss = 0.0622\n",
      "[173 / 250]: Loss = 0.0560\n",
      "[174 / 250]: Loss = 0.0639\n",
      "[175 / 250]: Loss = 0.0681\n",
      "[176 / 250]: Loss = 0.0513\n",
      "[177 / 250]: Loss = 0.0603\n",
      "[178 / 250]: Loss = 0.0490\n",
      "[179 / 250]: Loss = 0.0564\n",
      "[180 / 250]: Loss = 0.0660\n",
      "[181 / 250]: Loss = 0.0791\n",
      "[182 / 250]: Loss = 0.0478\n",
      "[183 / 250]: Loss = 0.0602\n",
      "[184 / 250]: Loss = 0.0496\n",
      "[185 / 250]: Loss = 0.0571\n",
      "[186 / 250]: Loss = 0.0594\n",
      "[187 / 250]: Loss = 0.0614\n",
      "[188 / 250]: Loss = 0.0638\n",
      "[189 / 250]: Loss = 0.0664\n",
      "[190 / 250]: Loss = 0.0522\n",
      "[191 / 250]: Loss = 0.0595\n",
      "[192 / 250]: Loss = 0.0714\n",
      "[193 / 250]: Loss = 0.0507\n",
      "[194 / 250]: Loss = 0.0613\n",
      "[195 / 250]: Loss = 0.0656\n",
      "[196 / 250]: Loss = 0.0616\n",
      "[197 / 250]: Loss = 0.0639\n",
      "[198 / 250]: Loss = 0.0526\n",
      "[199 / 250]: Loss = 0.0515\n",
      "[200 / 250]: Loss = 0.0642\n",
      "[201 / 250]: Loss = 0.0566\n",
      "[202 / 250]: Loss = 0.0569\n",
      "[203 / 250]: Loss = 0.0582\n",
      "[204 / 250]: Loss = 0.0446\n",
      "[205 / 250]: Loss = 0.0518\n",
      "[206 / 250]: Loss = 0.0712\n",
      "[207 / 250]: Loss = 0.0626\n",
      "[208 / 250]: Loss = 0.0473\n",
      "[209 / 250]: Loss = 0.0538\n",
      "[210 / 250]: Loss = 0.0547\n",
      "[211 / 250]: Loss = 0.0557\n",
      "[212 / 250]: Loss = 0.0585\n",
      "[213 / 250]: Loss = 0.0467\n",
      "[214 / 250]: Loss = 0.0582\n",
      "[215 / 250]: Loss = 0.0598\n",
      "[216 / 250]: Loss = 0.0581\n",
      "[217 / 250]: Loss = 0.0679\n",
      "[218 / 250]: Loss = 0.0621\n",
      "[219 / 250]: Loss = 0.0571\n",
      "[220 / 250]: Loss = 0.0509\n",
      "[221 / 250]: Loss = 0.0467\n",
      "[222 / 250]: Loss = 0.0668\n",
      "[223 / 250]: Loss = 0.0658\n",
      "[224 / 250]: Loss = 0.0542\n",
      "[225 / 250]: Loss = 0.0516\n",
      "[226 / 250]: Loss = 0.0473\n",
      "[227 / 250]: Loss = 0.0514\n",
      "[228 / 250]: Loss = 0.0555\n",
      "[229 / 250]: Loss = 0.0673\n",
      "[230 / 250]: Loss = 0.0446\n",
      "[231 / 250]: Loss = 0.0409\n",
      "[232 / 250]: Loss = 0.0635\n",
      "[233 / 250]: Loss = 0.0468\n",
      "[234 / 250]: Loss = 0.0533\n",
      "[235 / 250]: Loss = 0.0717\n",
      "[236 / 250]: Loss = 0.0558\n",
      "[237 / 250]: Loss = 0.0374\n",
      "[238 / 250]: Loss = 0.0447\n",
      "[239 / 250]: Loss = 0.0868\n",
      "[240 / 250]: Loss = 0.0697\n",
      "[241 / 250]: Loss = 0.0517\n",
      "[242 / 250]: Loss = 0.0562\n",
      "[243 / 250]: Loss = 0.0516\n",
      "[244 / 250]: Loss = 0.0551\n",
      "[245 / 250]: Loss = 0.0443\n",
      "[246 / 250]: Loss = 0.0600\n",
      "[247 / 250]: Loss = 0.0591\n",
      "[248 / 250]: Loss = 0.0552\n",
      "[249 / 250]: Loss = 0.0476\n",
      "[0 / 32]: Loss = 0.0552\n",
      "[1 / 32]: Loss = 0.0522\n",
      "[2 / 32]: Loss = 0.0521\n",
      "[3 / 32]: Loss = 0.0579\n",
      "[4 / 32]: Loss = 0.0449\n",
      "[5 / 32]: Loss = 0.0520\n",
      "[6 / 32]: Loss = 0.0508\n",
      "[7 / 32]: Loss = 0.0704\n",
      "[8 / 32]: Loss = 0.0538\n",
      "[9 / 32]: Loss = 0.0570\n",
      "[10 / 32]: Loss = 0.0607\n",
      "[11 / 32]: Loss = 0.0541\n",
      "[12 / 32]: Loss = 0.0586\n",
      "[13 / 32]: Loss = 0.0501\n",
      "[14 / 32]: Loss = 0.0494\n",
      "[15 / 32]: Loss = 0.0500\n",
      "[16 / 32]: Loss = 0.0502\n",
      "[17 / 32]: Loss = 0.0546\n",
      "[18 / 32]: Loss = 0.0513\n",
      "[19 / 32]: Loss = 0.0504\n",
      "[20 / 32]: Loss = 0.0482\n",
      "[21 / 32]: Loss = 0.0550\n",
      "[22 / 32]: Loss = 0.0560\n",
      "[23 / 32]: Loss = 0.0526\n",
      "[24 / 32]: Loss = 0.0548\n",
      "[25 / 32]: Loss = 0.0676\n",
      "[26 / 32]: Loss = 0.0534\n",
      "[27 / 32]: Loss = 0.0574\n",
      "[28 / 32]: Loss = 0.0469\n",
      "[29 / 32]: Loss = 0.0509\n",
      "[30 / 32]: Loss = 0.0528\n",
      "[31 / 32]: Loss = 0.0271\n",
      "Epoch 1 / 5, Epoch Time = 95.16s: Train Loss = 0.0949, Train AUC = 0.8304, Val Loss = 0.0531, Val AUC = 0.9702\n",
      "[0 / 250]: Loss = 0.0528\n",
      "[1 / 250]: Loss = 0.0506\n",
      "[2 / 250]: Loss = 0.0479\n",
      "[3 / 250]: Loss = 0.0454\n",
      "[4 / 250]: Loss = 0.0652\n",
      "[5 / 250]: Loss = 0.0394\n",
      "[6 / 250]: Loss = 0.0539\n",
      "[7 / 250]: Loss = 0.0619\n",
      "[8 / 250]: Loss = 0.0429\n",
      "[9 / 250]: Loss = 0.0502\n",
      "[10 / 250]: Loss = 0.0420\n",
      "[11 / 250]: Loss = 0.0621\n",
      "[12 / 250]: Loss = 0.0526\n",
      "[13 / 250]: Loss = 0.0697\n",
      "[14 / 250]: Loss = 0.0446\n",
      "[15 / 250]: Loss = 0.0626\n",
      "[16 / 250]: Loss = 0.0644\n",
      "[17 / 250]: Loss = 0.0655\n",
      "[18 / 250]: Loss = 0.0507\n",
      "[19 / 250]: Loss = 0.0672\n",
      "[20 / 250]: Loss = 0.0558\n",
      "[21 / 250]: Loss = 0.0439\n",
      "[22 / 250]: Loss = 0.0575\n",
      "[23 / 250]: Loss = 0.0593\n",
      "[24 / 250]: Loss = 0.0381\n",
      "[25 / 250]: Loss = 0.0460\n",
      "[26 / 250]: Loss = 0.0517\n",
      "[27 / 250]: Loss = 0.0559\n",
      "[28 / 250]: Loss = 0.0652\n",
      "[29 / 250]: Loss = 0.0542\n",
      "[30 / 250]: Loss = 0.0654\n",
      "[31 / 250]: Loss = 0.0577\n",
      "[32 / 250]: Loss = 0.0529\n",
      "[33 / 250]: Loss = 0.0456\n",
      "[34 / 250]: Loss = 0.0564\n",
      "[35 / 250]: Loss = 0.0615\n",
      "[36 / 250]: Loss = 0.0440\n",
      "[37 / 250]: Loss = 0.0503\n",
      "[38 / 250]: Loss = 0.0304\n",
      "[39 / 250]: Loss = 0.0517\n",
      "[40 / 250]: Loss = 0.0599\n",
      "[41 / 250]: Loss = 0.0537\n",
      "[42 / 250]: Loss = 0.0729\n",
      "[43 / 250]: Loss = 0.0538\n",
      "[44 / 250]: Loss = 0.0594\n",
      "[45 / 250]: Loss = 0.0383\n",
      "[46 / 250]: Loss = 0.0501\n",
      "[47 / 250]: Loss = 0.0653\n",
      "[48 / 250]: Loss = 0.0553\n",
      "[49 / 250]: Loss = 0.0586\n",
      "[50 / 250]: Loss = 0.0464\n",
      "[51 / 250]: Loss = 0.0538\n",
      "[52 / 250]: Loss = 0.0520\n",
      "[53 / 250]: Loss = 0.0520\n",
      "[54 / 250]: Loss = 0.0634\n",
      "[55 / 250]: Loss = 0.0481\n",
      "[56 / 250]: Loss = 0.0580\n",
      "[57 / 250]: Loss = 0.0447\n",
      "[58 / 250]: Loss = 0.0511\n",
      "[59 / 250]: Loss = 0.0536\n",
      "[60 / 250]: Loss = 0.0515\n",
      "[61 / 250]: Loss = 0.0511\n",
      "[62 / 250]: Loss = 0.0459\n",
      "[63 / 250]: Loss = 0.0481\n",
      "[64 / 250]: Loss = 0.0569\n",
      "[65 / 250]: Loss = 0.0506\n",
      "[66 / 250]: Loss = 0.0433\n",
      "[67 / 250]: Loss = 0.0580\n",
      "[68 / 250]: Loss = 0.0461\n",
      "[69 / 250]: Loss = 0.0471\n",
      "[70 / 250]: Loss = 0.0504\n",
      "[71 / 250]: Loss = 0.0470\n",
      "[72 / 250]: Loss = 0.0589\n",
      "[73 / 250]: Loss = 0.0593\n",
      "[74 / 250]: Loss = 0.0460\n",
      "[75 / 250]: Loss = 0.0565\n",
      "[76 / 250]: Loss = 0.0492\n",
      "[77 / 250]: Loss = 0.0493\n",
      "[78 / 250]: Loss = 0.0547\n",
      "[79 / 250]: Loss = 0.0448\n",
      "[80 / 250]: Loss = 0.0534\n",
      "[81 / 250]: Loss = 0.0484\n",
      "[82 / 250]: Loss = 0.0455\n",
      "[83 / 250]: Loss = 0.0487\n",
      "[84 / 250]: Loss = 0.0523\n",
      "[85 / 250]: Loss = 0.0413\n",
      "[86 / 250]: Loss = 0.0533\n",
      "[87 / 250]: Loss = 0.0400\n",
      "[88 / 250]: Loss = 0.0535\n",
      "[89 / 250]: Loss = 0.0364\n",
      "[90 / 250]: Loss = 0.0457\n",
      "[91 / 250]: Loss = 0.0564\n",
      "[92 / 250]: Loss = 0.0601\n",
      "[93 / 250]: Loss = 0.0649\n",
      "[94 / 250]: Loss = 0.0469\n",
      "[95 / 250]: Loss = 0.0524\n",
      "[96 / 250]: Loss = 0.0513\n",
      "[97 / 250]: Loss = 0.0393\n",
      "[98 / 250]: Loss = 0.0500\n",
      "[99 / 250]: Loss = 0.0473\n",
      "[100 / 250]: Loss = 0.0515\n",
      "[101 / 250]: Loss = 0.0536\n",
      "[102 / 250]: Loss = 0.0573\n",
      "[103 / 250]: Loss = 0.0484\n",
      "[104 / 250]: Loss = 0.0514\n",
      "[105 / 250]: Loss = 0.0589\n",
      "[106 / 250]: Loss = 0.0458\n",
      "[107 / 250]: Loss = 0.0511\n",
      "[108 / 250]: Loss = 0.0575\n",
      "[109 / 250]: Loss = 0.0501\n",
      "[110 / 250]: Loss = 0.0451\n",
      "[111 / 250]: Loss = 0.0419\n",
      "[112 / 250]: Loss = 0.0542\n",
      "[113 / 250]: Loss = 0.0507\n",
      "[114 / 250]: Loss = 0.0540\n",
      "[115 / 250]: Loss = 0.0569\n",
      "[116 / 250]: Loss = 0.0505\n",
      "[117 / 250]: Loss = 0.0419\n",
      "[118 / 250]: Loss = 0.0428\n",
      "[119 / 250]: Loss = 0.0456\n",
      "[120 / 250]: Loss = 0.0428\n",
      "[121 / 250]: Loss = 0.0549\n",
      "[122 / 250]: Loss = 0.0635\n",
      "[123 / 250]: Loss = 0.0547\n",
      "[124 / 250]: Loss = 0.0557\n",
      "[125 / 250]: Loss = 0.0646\n",
      "[126 / 250]: Loss = 0.0408\n",
      "[127 / 250]: Loss = 0.0554\n",
      "[128 / 250]: Loss = 0.0425\n",
      "[129 / 250]: Loss = 0.0398\n",
      "[130 / 250]: Loss = 0.0463\n",
      "[131 / 250]: Loss = 0.0473\n",
      "[132 / 250]: Loss = 0.0560\n",
      "[133 / 250]: Loss = 0.0582\n",
      "[134 / 250]: Loss = 0.0622\n",
      "[135 / 250]: Loss = 0.0540\n",
      "[136 / 250]: Loss = 0.0475\n",
      "[137 / 250]: Loss = 0.0450\n",
      "[138 / 250]: Loss = 0.0576\n",
      "[139 / 250]: Loss = 0.0516\n",
      "[140 / 250]: Loss = 0.0378\n",
      "[141 / 250]: Loss = 0.0584\n",
      "[142 / 250]: Loss = 0.0393\n",
      "[143 / 250]: Loss = 0.0547\n",
      "[144 / 250]: Loss = 0.0568\n",
      "[145 / 250]: Loss = 0.0440\n",
      "[146 / 250]: Loss = 0.0504\n",
      "[147 / 250]: Loss = 0.0473\n",
      "[148 / 250]: Loss = 0.0551\n",
      "[149 / 250]: Loss = 0.0553\n",
      "[150 / 250]: Loss = 0.0519\n",
      "[151 / 250]: Loss = 0.0500\n",
      "[152 / 250]: Loss = 0.0432\n",
      "[153 / 250]: Loss = 0.0478\n",
      "[154 / 250]: Loss = 0.0667\n",
      "[155 / 250]: Loss = 0.0584\n",
      "[156 / 250]: Loss = 0.0495\n",
      "[157 / 250]: Loss = 0.0640\n",
      "[158 / 250]: Loss = 0.0550\n",
      "[159 / 250]: Loss = 0.0541\n",
      "[160 / 250]: Loss = 0.0511\n",
      "[161 / 250]: Loss = 0.0525\n",
      "[162 / 250]: Loss = 0.0598\n",
      "[163 / 250]: Loss = 0.0377\n",
      "[164 / 250]: Loss = 0.0433\n",
      "[165 / 250]: Loss = 0.0415\n",
      "[166 / 250]: Loss = 0.0567\n",
      "[167 / 250]: Loss = 0.0421\n",
      "[168 / 250]: Loss = 0.0422\n",
      "[169 / 250]: Loss = 0.0496\n",
      "[170 / 250]: Loss = 0.0511\n",
      "[171 / 250]: Loss = 0.0478\n",
      "[172 / 250]: Loss = 0.0440\n",
      "[173 / 250]: Loss = 0.0420\n",
      "[174 / 250]: Loss = 0.0500\n",
      "[175 / 250]: Loss = 0.0528\n",
      "[176 / 250]: Loss = 0.0452\n",
      "[177 / 250]: Loss = 0.0515\n",
      "[178 / 250]: Loss = 0.0495\n",
      "[179 / 250]: Loss = 0.0584\n",
      "[180 / 250]: Loss = 0.0477\n",
      "[181 / 250]: Loss = 0.0475\n",
      "[182 / 250]: Loss = 0.0619\n",
      "[183 / 250]: Loss = 0.0298\n",
      "[184 / 250]: Loss = 0.0418\n",
      "[185 / 250]: Loss = 0.0476\n",
      "[186 / 250]: Loss = 0.0527\n",
      "[187 / 250]: Loss = 0.0590\n",
      "[188 / 250]: Loss = 0.0617\n",
      "[189 / 250]: Loss = 0.0461\n",
      "[190 / 250]: Loss = 0.0466\n",
      "[191 / 250]: Loss = 0.0510\n",
      "[192 / 250]: Loss = 0.0424\n",
      "[193 / 250]: Loss = 0.0522\n",
      "[194 / 250]: Loss = 0.0494\n",
      "[195 / 250]: Loss = 0.0454\n",
      "[196 / 250]: Loss = 0.0443\n",
      "[197 / 250]: Loss = 0.0411\n",
      "[198 / 250]: Loss = 0.0387\n",
      "[199 / 250]: Loss = 0.0365\n",
      "[200 / 250]: Loss = 0.0517\n",
      "[201 / 250]: Loss = 0.0576\n",
      "[202 / 250]: Loss = 0.0579\n",
      "[203 / 250]: Loss = 0.0398\n",
      "[204 / 250]: Loss = 0.0341\n",
      "[205 / 250]: Loss = 0.0435\n",
      "[206 / 250]: Loss = 0.0379\n",
      "[207 / 250]: Loss = 0.0482\n",
      "[208 / 250]: Loss = 0.0480\n",
      "[209 / 250]: Loss = 0.0517\n",
      "[210 / 250]: Loss = 0.0436\n",
      "[211 / 250]: Loss = 0.0476\n",
      "[212 / 250]: Loss = 0.0467\n",
      "[213 / 250]: Loss = 0.0485\n",
      "[214 / 250]: Loss = 0.0337\n",
      "[215 / 250]: Loss = 0.0529\n",
      "[216 / 250]: Loss = 0.0584\n",
      "[217 / 250]: Loss = 0.0637\n",
      "[218 / 250]: Loss = 0.0418\n",
      "[219 / 250]: Loss = 0.0443\n",
      "[220 / 250]: Loss = 0.0491\n",
      "[221 / 250]: Loss = 0.0541\n",
      "[222 / 250]: Loss = 0.0459\n",
      "[223 / 250]: Loss = 0.0565\n",
      "[224 / 250]: Loss = 0.0545\n",
      "[225 / 250]: Loss = 0.0545\n",
      "[226 / 250]: Loss = 0.0445\n",
      "[227 / 250]: Loss = 0.0524\n",
      "[228 / 250]: Loss = 0.0417\n",
      "[229 / 250]: Loss = 0.0495\n",
      "[230 / 250]: Loss = 0.0566\n",
      "[231 / 250]: Loss = 0.0515\n",
      "[232 / 250]: Loss = 0.0552\n",
      "[233 / 250]: Loss = 0.0455\n",
      "[234 / 250]: Loss = 0.0356\n",
      "[235 / 250]: Loss = 0.0317\n",
      "[236 / 250]: Loss = 0.0455\n",
      "[237 / 250]: Loss = 0.0492\n",
      "[238 / 250]: Loss = 0.0369\n",
      "[239 / 250]: Loss = 0.0453\n",
      "[240 / 250]: Loss = 0.0456\n",
      "[241 / 250]: Loss = 0.0540\n",
      "[242 / 250]: Loss = 0.0473\n",
      "[243 / 250]: Loss = 0.0456\n",
      "[244 / 250]: Loss = 0.0557\n",
      "[245 / 250]: Loss = 0.0588\n",
      "[246 / 250]: Loss = 0.0303\n",
      "[247 / 250]: Loss = 0.0551\n",
      "[248 / 250]: Loss = 0.0551\n",
      "[249 / 250]: Loss = 0.0337\n",
      "[0 / 32]: Loss = 0.0427\n",
      "[1 / 32]: Loss = 0.0497\n",
      "[2 / 32]: Loss = 0.0502\n",
      "[3 / 32]: Loss = 0.0413\n",
      "[4 / 32]: Loss = 0.0405\n",
      "[5 / 32]: Loss = 0.0491\n",
      "[6 / 32]: Loss = 0.0509\n",
      "[7 / 32]: Loss = 0.0486\n",
      "[8 / 32]: Loss = 0.0531\n",
      "[9 / 32]: Loss = 0.0495\n",
      "[10 / 32]: Loss = 0.0458\n",
      "[11 / 32]: Loss = 0.0531\n",
      "[12 / 32]: Loss = 0.0492\n",
      "[13 / 32]: Loss = 0.0459\n",
      "[14 / 32]: Loss = 0.0467\n",
      "[15 / 32]: Loss = 0.0565\n",
      "[16 / 32]: Loss = 0.0458\n",
      "[17 / 32]: Loss = 0.0505\n",
      "[18 / 32]: Loss = 0.0537\n",
      "[19 / 32]: Loss = 0.0439\n",
      "[20 / 32]: Loss = 0.0495\n",
      "[21 / 32]: Loss = 0.0431\n",
      "[22 / 32]: Loss = 0.0464\n",
      "[23 / 32]: Loss = 0.0422\n",
      "[24 / 32]: Loss = 0.0447\n",
      "[25 / 32]: Loss = 0.0445\n",
      "[26 / 32]: Loss = 0.0449\n",
      "[27 / 32]: Loss = 0.0532\n",
      "[28 / 32]: Loss = 0.0501\n",
      "[29 / 32]: Loss = 0.0390\n",
      "[30 / 32]: Loss = 0.0568\n",
      "[31 / 32]: Loss = 0.0328\n",
      "Epoch 2 / 5, Epoch Time = 94.89s: Train Loss = 0.0505, Train AUC = 0.9735, Val Loss = 0.0473, Val AUC = 0.9782\n",
      "[0 / 250]: Loss = 0.0344\n",
      "[1 / 250]: Loss = 0.0503\n",
      "[2 / 250]: Loss = 0.0407\n",
      "[3 / 250]: Loss = 0.0409\n",
      "[4 / 250]: Loss = 0.0533\n",
      "[5 / 250]: Loss = 0.0508\n",
      "[6 / 250]: Loss = 0.0554\n",
      "[7 / 250]: Loss = 0.0445\n",
      "[8 / 250]: Loss = 0.0414\n",
      "[9 / 250]: Loss = 0.0396\n",
      "[10 / 250]: Loss = 0.0499\n",
      "[11 / 250]: Loss = 0.0485\n",
      "[12 / 250]: Loss = 0.0603\n",
      "[13 / 250]: Loss = 0.0347\n",
      "[14 / 250]: Loss = 0.0470\n",
      "[15 / 250]: Loss = 0.0433\n",
      "[16 / 250]: Loss = 0.0588\n",
      "[17 / 250]: Loss = 0.0450\n",
      "[18 / 250]: Loss = 0.0398\n",
      "[19 / 250]: Loss = 0.0463\n",
      "[20 / 250]: Loss = 0.0461\n",
      "[21 / 250]: Loss = 0.0424\n",
      "[22 / 250]: Loss = 0.0392\n",
      "[23 / 250]: Loss = 0.0376\n",
      "[24 / 250]: Loss = 0.0483\n",
      "[25 / 250]: Loss = 0.0444\n",
      "[26 / 250]: Loss = 0.0465\n",
      "[27 / 250]: Loss = 0.0432\n",
      "[28 / 250]: Loss = 0.0457\n",
      "[29 / 250]: Loss = 0.0414\n",
      "[30 / 250]: Loss = 0.0524\n",
      "[31 / 250]: Loss = 0.0457\n",
      "[32 / 250]: Loss = 0.0435\n",
      "[33 / 250]: Loss = 0.0395\n",
      "[34 / 250]: Loss = 0.0494\n",
      "[35 / 250]: Loss = 0.0454\n",
      "[36 / 250]: Loss = 0.0412\n",
      "[37 / 250]: Loss = 0.0428\n",
      "[38 / 250]: Loss = 0.0406\n",
      "[39 / 250]: Loss = 0.0412\n",
      "[40 / 250]: Loss = 0.0334\n",
      "[41 / 250]: Loss = 0.0411\n",
      "[42 / 250]: Loss = 0.0385\n",
      "[43 / 250]: Loss = 0.0435\n",
      "[44 / 250]: Loss = 0.0457\n",
      "[45 / 250]: Loss = 0.0504\n",
      "[46 / 250]: Loss = 0.0377\n",
      "[47 / 250]: Loss = 0.0408\n",
      "[48 / 250]: Loss = 0.0401\n",
      "[49 / 250]: Loss = 0.0518\n",
      "[50 / 250]: Loss = 0.0394\n",
      "[51 / 250]: Loss = 0.0509\n",
      "[52 / 250]: Loss = 0.0534\n",
      "[53 / 250]: Loss = 0.0561\n",
      "[54 / 250]: Loss = 0.0463\n",
      "[55 / 250]: Loss = 0.0514\n",
      "[56 / 250]: Loss = 0.0411\n",
      "[57 / 250]: Loss = 0.0407\n",
      "[58 / 250]: Loss = 0.0436\n",
      "[59 / 250]: Loss = 0.0358\n",
      "[60 / 250]: Loss = 0.0448\n",
      "[61 / 250]: Loss = 0.0355\n",
      "[62 / 250]: Loss = 0.0511\n",
      "[63 / 250]: Loss = 0.0442\n",
      "[64 / 250]: Loss = 0.0413\n",
      "[65 / 250]: Loss = 0.0360\n",
      "[66 / 250]: Loss = 0.0492\n",
      "[67 / 250]: Loss = 0.0473\n",
      "[68 / 250]: Loss = 0.0385\n",
      "[69 / 250]: Loss = 0.0385\n",
      "[70 / 250]: Loss = 0.0358\n",
      "[71 / 250]: Loss = 0.0432\n",
      "[72 / 250]: Loss = 0.0454\n",
      "[73 / 250]: Loss = 0.0441\n",
      "[74 / 250]: Loss = 0.0425\n",
      "[75 / 250]: Loss = 0.0430\n",
      "[76 / 250]: Loss = 0.0391\n",
      "[77 / 250]: Loss = 0.0523\n",
      "[78 / 250]: Loss = 0.0507\n",
      "[79 / 250]: Loss = 0.0516\n",
      "[80 / 250]: Loss = 0.0322\n",
      "[81 / 250]: Loss = 0.0618\n",
      "[82 / 250]: Loss = 0.0399\n",
      "[83 / 250]: Loss = 0.0564\n",
      "[84 / 250]: Loss = 0.0434\n",
      "[85 / 250]: Loss = 0.0462\n",
      "[86 / 250]: Loss = 0.0694\n",
      "[87 / 250]: Loss = 0.0303\n",
      "[88 / 250]: Loss = 0.0455\n",
      "[89 / 250]: Loss = 0.0454\n",
      "[90 / 250]: Loss = 0.0418\n",
      "[91 / 250]: Loss = 0.0449\n",
      "[92 / 250]: Loss = 0.0475\n",
      "[93 / 250]: Loss = 0.0352\n",
      "[94 / 250]: Loss = 0.0595\n",
      "[95 / 250]: Loss = 0.0386\n",
      "[96 / 250]: Loss = 0.0528\n",
      "[97 / 250]: Loss = 0.0428\n",
      "[98 / 250]: Loss = 0.0443\n",
      "[99 / 250]: Loss = 0.0423\n",
      "[100 / 250]: Loss = 0.0461\n",
      "[101 / 250]: Loss = 0.0423\n",
      "[102 / 250]: Loss = 0.0374\n",
      "[103 / 250]: Loss = 0.0387\n",
      "[104 / 250]: Loss = 0.0431\n",
      "[105 / 250]: Loss = 0.0428\n",
      "[106 / 250]: Loss = 0.0489\n",
      "[107 / 250]: Loss = 0.0447\n",
      "[108 / 250]: Loss = 0.0458\n",
      "[109 / 250]: Loss = 0.0414\n",
      "[110 / 250]: Loss = 0.0398\n",
      "[111 / 250]: Loss = 0.0367\n",
      "[112 / 250]: Loss = 0.0534\n",
      "[113 / 250]: Loss = 0.0540\n",
      "[114 / 250]: Loss = 0.0436\n",
      "[115 / 250]: Loss = 0.0379\n",
      "[116 / 250]: Loss = 0.0373\n",
      "[117 / 250]: Loss = 0.0406\n",
      "[118 / 250]: Loss = 0.0314\n",
      "[119 / 250]: Loss = 0.0375\n",
      "[120 / 250]: Loss = 0.0400\n",
      "[121 / 250]: Loss = 0.0419\n",
      "[122 / 250]: Loss = 0.0522\n",
      "[123 / 250]: Loss = 0.0453\n",
      "[124 / 250]: Loss = 0.0380\n",
      "[125 / 250]: Loss = 0.0484\n",
      "[126 / 250]: Loss = 0.0457\n",
      "[127 / 250]: Loss = 0.0439\n",
      "[128 / 250]: Loss = 0.0571\n",
      "[129 / 250]: Loss = 0.0494\n",
      "[130 / 250]: Loss = 0.0487\n",
      "[131 / 250]: Loss = 0.0418\n",
      "[132 / 250]: Loss = 0.0417\n",
      "[133 / 250]: Loss = 0.0529\n",
      "[134 / 250]: Loss = 0.0420\n",
      "[135 / 250]: Loss = 0.0361\n",
      "[136 / 250]: Loss = 0.0343\n",
      "[137 / 250]: Loss = 0.0412\n",
      "[138 / 250]: Loss = 0.0443\n",
      "[139 / 250]: Loss = 0.0484\n",
      "[140 / 250]: Loss = 0.0422\n",
      "[141 / 250]: Loss = 0.0447\n",
      "[142 / 250]: Loss = 0.0523\n",
      "[143 / 250]: Loss = 0.0395\n",
      "[144 / 250]: Loss = 0.0377\n",
      "[145 / 250]: Loss = 0.0422\n",
      "[146 / 250]: Loss = 0.0557\n",
      "[147 / 250]: Loss = 0.0369\n",
      "[148 / 250]: Loss = 0.0482\n",
      "[149 / 250]: Loss = 0.0307\n",
      "[150 / 250]: Loss = 0.0316\n",
      "[151 / 250]: Loss = 0.0358\n",
      "[152 / 250]: Loss = 0.0356\n",
      "[153 / 250]: Loss = 0.0399\n",
      "[154 / 250]: Loss = 0.0417\n",
      "[155 / 250]: Loss = 0.0261\n",
      "[156 / 250]: Loss = 0.0381\n",
      "[157 / 250]: Loss = 0.0370\n",
      "[158 / 250]: Loss = 0.0495\n",
      "[159 / 250]: Loss = 0.0490\n",
      "[160 / 250]: Loss = 0.0398\n",
      "[161 / 250]: Loss = 0.0514\n",
      "[162 / 250]: Loss = 0.0436\n",
      "[163 / 250]: Loss = 0.0455\n",
      "[164 / 250]: Loss = 0.0423\n",
      "[165 / 250]: Loss = 0.0442\n",
      "[166 / 250]: Loss = 0.0494\n",
      "[167 / 250]: Loss = 0.0490\n",
      "[168 / 250]: Loss = 0.0425\n",
      "[169 / 250]: Loss = 0.0491\n",
      "[170 / 250]: Loss = 0.0361\n",
      "[171 / 250]: Loss = 0.0471\n",
      "[172 / 250]: Loss = 0.0433\n",
      "[173 / 250]: Loss = 0.0475\n",
      "[174 / 250]: Loss = 0.0552\n",
      "[175 / 250]: Loss = 0.0534\n",
      "[176 / 250]: Loss = 0.0508\n",
      "[177 / 250]: Loss = 0.0450\n",
      "[178 / 250]: Loss = 0.0392\n",
      "[179 / 250]: Loss = 0.0453\n",
      "[180 / 250]: Loss = 0.0406\n",
      "[181 / 250]: Loss = 0.0473\n",
      "[182 / 250]: Loss = 0.0483\n",
      "[183 / 250]: Loss = 0.0365\n",
      "[184 / 250]: Loss = 0.0410\n",
      "[185 / 250]: Loss = 0.0421\n",
      "[186 / 250]: Loss = 0.0609\n",
      "[187 / 250]: Loss = 0.0532\n",
      "[188 / 250]: Loss = 0.0337\n",
      "[189 / 250]: Loss = 0.0435\n",
      "[190 / 250]: Loss = 0.0429\n",
      "[191 / 250]: Loss = 0.0400\n",
      "[192 / 250]: Loss = 0.0423\n",
      "[193 / 250]: Loss = 0.0355\n",
      "[194 / 250]: Loss = 0.0523\n",
      "[195 / 250]: Loss = 0.0552\n",
      "[196 / 250]: Loss = 0.0376\n",
      "[197 / 250]: Loss = 0.0452\n",
      "[198 / 250]: Loss = 0.0486\n",
      "[199 / 250]: Loss = 0.0594\n",
      "[200 / 250]: Loss = 0.0424\n",
      "[201 / 250]: Loss = 0.0333\n",
      "[202 / 250]: Loss = 0.0429\n",
      "[203 / 250]: Loss = 0.0469\n",
      "[204 / 250]: Loss = 0.0397\n",
      "[205 / 250]: Loss = 0.0422\n",
      "[206 / 250]: Loss = 0.0584\n",
      "[207 / 250]: Loss = 0.0487\n",
      "[208 / 250]: Loss = 0.0386\n",
      "[209 / 250]: Loss = 0.0432\n",
      "[210 / 250]: Loss = 0.0463\n",
      "[211 / 250]: Loss = 0.0402\n",
      "[212 / 250]: Loss = 0.0389\n",
      "[213 / 250]: Loss = 0.0455\n",
      "[214 / 250]: Loss = 0.0504\n",
      "[215 / 250]: Loss = 0.0499\n",
      "[216 / 250]: Loss = 0.0350\n",
      "[217 / 250]: Loss = 0.0492\n",
      "[218 / 250]: Loss = 0.0488\n",
      "[219 / 250]: Loss = 0.0592\n",
      "[220 / 250]: Loss = 0.0444\n",
      "[221 / 250]: Loss = 0.0513\n",
      "[222 / 250]: Loss = 0.0477\n",
      "[223 / 250]: Loss = 0.0462\n",
      "[224 / 250]: Loss = 0.0471\n",
      "[225 / 250]: Loss = 0.0517\n",
      "[226 / 250]: Loss = 0.0453\n",
      "[227 / 250]: Loss = 0.0428\n",
      "[228 / 250]: Loss = 0.0470\n",
      "[229 / 250]: Loss = 0.0458\n",
      "[230 / 250]: Loss = 0.0497\n",
      "[231 / 250]: Loss = 0.0400\n",
      "[232 / 250]: Loss = 0.0429\n",
      "[233 / 250]: Loss = 0.0399\n",
      "[234 / 250]: Loss = 0.0373\n",
      "[235 / 250]: Loss = 0.0402\n",
      "[236 / 250]: Loss = 0.0346\n",
      "[237 / 250]: Loss = 0.0447\n",
      "[238 / 250]: Loss = 0.0483\n",
      "[239 / 250]: Loss = 0.0433\n",
      "[240 / 250]: Loss = 0.0401\n",
      "[241 / 250]: Loss = 0.0553\n",
      "[242 / 250]: Loss = 0.0413\n",
      "[243 / 250]: Loss = 0.0446\n",
      "[244 / 250]: Loss = 0.0492\n",
      "[245 / 250]: Loss = 0.0373\n",
      "[246 / 250]: Loss = 0.0519\n",
      "[247 / 250]: Loss = 0.0480\n",
      "[248 / 250]: Loss = 0.0352\n",
      "[249 / 250]: Loss = 0.0301\n",
      "[0 / 32]: Loss = 0.0453\n",
      "[1 / 32]: Loss = 0.0458\n",
      "[2 / 32]: Loss = 0.0437\n",
      "[3 / 32]: Loss = 0.0431\n",
      "[4 / 32]: Loss = 0.0426\n",
      "[5 / 32]: Loss = 0.0525\n",
      "[6 / 32]: Loss = 0.0434\n",
      "[7 / 32]: Loss = 0.0472\n",
      "[8 / 32]: Loss = 0.0501\n",
      "[9 / 32]: Loss = 0.0581\n",
      "[10 / 32]: Loss = 0.0345\n",
      "[11 / 32]: Loss = 0.0525\n",
      "[12 / 32]: Loss = 0.0532\n",
      "[13 / 32]: Loss = 0.0399\n",
      "[14 / 32]: Loss = 0.0388\n",
      "[15 / 32]: Loss = 0.0420\n",
      "[16 / 32]: Loss = 0.0483\n",
      "[17 / 32]: Loss = 0.0598\n",
      "[18 / 32]: Loss = 0.0519\n",
      "[19 / 32]: Loss = 0.0394\n",
      "[20 / 32]: Loss = 0.0398\n",
      "[21 / 32]: Loss = 0.0403\n",
      "[22 / 32]: Loss = 0.0422\n",
      "[23 / 32]: Loss = 0.0385\n",
      "[24 / 32]: Loss = 0.0441\n",
      "[25 / 32]: Loss = 0.0514\n",
      "[26 / 32]: Loss = 0.0497\n",
      "[27 / 32]: Loss = 0.0425\n",
      "[28 / 32]: Loss = 0.0448\n",
      "[29 / 32]: Loss = 0.0463\n",
      "[30 / 32]: Loss = 0.0465\n",
      "[31 / 32]: Loss = 0.0443\n",
      "Epoch 3 / 5, Epoch Time = 94.81s: Train Loss = 0.0443, Train AUC = 0.9812, Val Loss = 0.0457, Val AUC = 0.9811\n",
      "[0 / 250]: Loss = 0.0299\n",
      "[1 / 250]: Loss = 0.0433\n",
      "[2 / 250]: Loss = 0.0336\n",
      "[3 / 250]: Loss = 0.0307\n",
      "[4 / 250]: Loss = 0.0424\n",
      "[5 / 250]: Loss = 0.0431\n",
      "[6 / 250]: Loss = 0.0446\n",
      "[7 / 250]: Loss = 0.0494\n",
      "[8 / 250]: Loss = 0.0394\n",
      "[9 / 250]: Loss = 0.0361\n",
      "[10 / 250]: Loss = 0.0418\n",
      "[11 / 250]: Loss = 0.0500\n",
      "[12 / 250]: Loss = 0.0336\n",
      "[13 / 250]: Loss = 0.0388\n",
      "[14 / 250]: Loss = 0.0283\n",
      "[15 / 250]: Loss = 0.0344\n",
      "[16 / 250]: Loss = 0.0358\n",
      "[17 / 250]: Loss = 0.0484\n",
      "[18 / 250]: Loss = 0.0506\n",
      "[19 / 250]: Loss = 0.0326\n",
      "[20 / 250]: Loss = 0.0315\n",
      "[21 / 250]: Loss = 0.0363\n",
      "[22 / 250]: Loss = 0.0401\n",
      "[23 / 250]: Loss = 0.0467\n",
      "[24 / 250]: Loss = 0.0386\n",
      "[25 / 250]: Loss = 0.0405\n",
      "[26 / 250]: Loss = 0.0376\n",
      "[27 / 250]: Loss = 0.0371\n",
      "[28 / 250]: Loss = 0.0507\n",
      "[29 / 250]: Loss = 0.0307\n",
      "[30 / 250]: Loss = 0.0423\n",
      "[31 / 250]: Loss = 0.0519\n",
      "[32 / 250]: Loss = 0.0345\n",
      "[33 / 250]: Loss = 0.0373\n",
      "[34 / 250]: Loss = 0.0334\n",
      "[35 / 250]: Loss = 0.0413\n",
      "[36 / 250]: Loss = 0.0380\n",
      "[37 / 250]: Loss = 0.0238\n",
      "[38 / 250]: Loss = 0.0332\n",
      "[39 / 250]: Loss = 0.0404\n",
      "[40 / 250]: Loss = 0.0488\n",
      "[41 / 250]: Loss = 0.0351\n",
      "[42 / 250]: Loss = 0.0434\n",
      "[43 / 250]: Loss = 0.0375\n",
      "[44 / 250]: Loss = 0.0395\n",
      "[45 / 250]: Loss = 0.0347\n",
      "[46 / 250]: Loss = 0.0409\n",
      "[47 / 250]: Loss = 0.0448\n",
      "[48 / 250]: Loss = 0.0413\n",
      "[49 / 250]: Loss = 0.0392\n",
      "[50 / 250]: Loss = 0.0438\n",
      "[51 / 250]: Loss = 0.0297\n",
      "[52 / 250]: Loss = 0.0411\n",
      "[53 / 250]: Loss = 0.0384\n",
      "[54 / 250]: Loss = 0.0412\n",
      "[55 / 250]: Loss = 0.0293\n",
      "[56 / 250]: Loss = 0.0322\n",
      "[57 / 250]: Loss = 0.0446\n",
      "[58 / 250]: Loss = 0.0449\n",
      "[59 / 250]: Loss = 0.0401\n",
      "[60 / 250]: Loss = 0.0426\n",
      "[61 / 250]: Loss = 0.0469\n",
      "[62 / 250]: Loss = 0.0324\n",
      "[63 / 250]: Loss = 0.0451\n",
      "[64 / 250]: Loss = 0.0411\n",
      "[65 / 250]: Loss = 0.0295\n",
      "[66 / 250]: Loss = 0.0422\n",
      "[67 / 250]: Loss = 0.0455\n",
      "[68 / 250]: Loss = 0.0391\n",
      "[69 / 250]: Loss = 0.0444\n",
      "[70 / 250]: Loss = 0.0445\n",
      "[71 / 250]: Loss = 0.0429\n",
      "[72 / 250]: Loss = 0.0362\n",
      "[73 / 250]: Loss = 0.0436\n",
      "[74 / 250]: Loss = 0.0381\n",
      "[75 / 250]: Loss = 0.0322\n",
      "[76 / 250]: Loss = 0.0411\n",
      "[77 / 250]: Loss = 0.0396\n",
      "[78 / 250]: Loss = 0.0368\n",
      "[79 / 250]: Loss = 0.0332\n",
      "[80 / 250]: Loss = 0.0423\n",
      "[81 / 250]: Loss = 0.0379\n",
      "[82 / 250]: Loss = 0.0355\n",
      "[83 / 250]: Loss = 0.0438\n",
      "[84 / 250]: Loss = 0.0263\n",
      "[85 / 250]: Loss = 0.0454\n",
      "[86 / 250]: Loss = 0.0431\n",
      "[87 / 250]: Loss = 0.0484\n",
      "[88 / 250]: Loss = 0.0378\n",
      "[89 / 250]: Loss = 0.0378\n",
      "[90 / 250]: Loss = 0.0420\n",
      "[91 / 250]: Loss = 0.0371\n",
      "[92 / 250]: Loss = 0.0382\n",
      "[93 / 250]: Loss = 0.0358\n",
      "[94 / 250]: Loss = 0.0396\n",
      "[95 / 250]: Loss = 0.0373\n",
      "[96 / 250]: Loss = 0.0413\n",
      "[97 / 250]: Loss = 0.0488\n",
      "[98 / 250]: Loss = 0.0288\n",
      "[99 / 250]: Loss = 0.0375\n",
      "[100 / 250]: Loss = 0.0334\n",
      "[101 / 250]: Loss = 0.0483\n",
      "[102 / 250]: Loss = 0.0403\n",
      "[103 / 250]: Loss = 0.0362\n",
      "[104 / 250]: Loss = 0.0431\n",
      "[105 / 250]: Loss = 0.0254\n",
      "[106 / 250]: Loss = 0.0381\n",
      "[107 / 250]: Loss = 0.0400\n",
      "[108 / 250]: Loss = 0.0386\n",
      "[109 / 250]: Loss = 0.0369\n",
      "[110 / 250]: Loss = 0.0450\n",
      "[111 / 250]: Loss = 0.0400\n",
      "[112 / 250]: Loss = 0.0386\n",
      "[113 / 250]: Loss = 0.0371\n",
      "[114 / 250]: Loss = 0.0554\n",
      "[115 / 250]: Loss = 0.0316\n",
      "[116 / 250]: Loss = 0.0371\n",
      "[117 / 250]: Loss = 0.0401\n",
      "[118 / 250]: Loss = 0.0475\n",
      "[119 / 250]: Loss = 0.0388\n",
      "[120 / 250]: Loss = 0.0285\n",
      "[121 / 250]: Loss = 0.0364\n",
      "[122 / 250]: Loss = 0.0353\n",
      "[123 / 250]: Loss = 0.0400\n",
      "[124 / 250]: Loss = 0.0399\n",
      "[125 / 250]: Loss = 0.0272\n",
      "[126 / 250]: Loss = 0.0431\n",
      "[127 / 250]: Loss = 0.0371\n",
      "[128 / 250]: Loss = 0.0409\n",
      "[129 / 250]: Loss = 0.0228\n",
      "[130 / 250]: Loss = 0.0373\n",
      "[131 / 250]: Loss = 0.0508\n",
      "[132 / 250]: Loss = 0.0436\n",
      "[133 / 250]: Loss = 0.0369\n",
      "[134 / 250]: Loss = 0.0502\n",
      "[135 / 250]: Loss = 0.0345\n",
      "[136 / 250]: Loss = 0.0404\n",
      "[137 / 250]: Loss = 0.0404\n",
      "[138 / 250]: Loss = 0.0360\n",
      "[139 / 250]: Loss = 0.0327\n",
      "[140 / 250]: Loss = 0.0458\n",
      "[141 / 250]: Loss = 0.0465\n",
      "[142 / 250]: Loss = 0.0430\n",
      "[143 / 250]: Loss = 0.0435\n",
      "[144 / 250]: Loss = 0.0484\n",
      "[145 / 250]: Loss = 0.0405\n",
      "[146 / 250]: Loss = 0.0450\n",
      "[147 / 250]: Loss = 0.0416\n",
      "[148 / 250]: Loss = 0.0379\n",
      "[149 / 250]: Loss = 0.0407\n",
      "[150 / 250]: Loss = 0.0407\n",
      "[151 / 250]: Loss = 0.0410\n",
      "[152 / 250]: Loss = 0.0388\n",
      "[153 / 250]: Loss = 0.0471\n",
      "[154 / 250]: Loss = 0.0314\n",
      "[155 / 250]: Loss = 0.0295\n",
      "[156 / 250]: Loss = 0.0418\n",
      "[157 / 250]: Loss = 0.0429\n",
      "[158 / 250]: Loss = 0.0432\n",
      "[159 / 250]: Loss = 0.0434\n",
      "[160 / 250]: Loss = 0.0469\n",
      "[161 / 250]: Loss = 0.0461\n",
      "[162 / 250]: Loss = 0.0377\n",
      "[163 / 250]: Loss = 0.0454\n",
      "[164 / 250]: Loss = 0.0322\n",
      "[165 / 250]: Loss = 0.0507\n",
      "[166 / 250]: Loss = 0.0511\n",
      "[167 / 250]: Loss = 0.0401\n",
      "[168 / 250]: Loss = 0.0477\n",
      "[169 / 250]: Loss = 0.0442\n",
      "[170 / 250]: Loss = 0.0450\n",
      "[171 / 250]: Loss = 0.0399\n",
      "[172 / 250]: Loss = 0.0436\n",
      "[173 / 250]: Loss = 0.0351\n",
      "[174 / 250]: Loss = 0.0424\n",
      "[175 / 250]: Loss = 0.0382\n",
      "[176 / 250]: Loss = 0.0363\n",
      "[177 / 250]: Loss = 0.0408\n",
      "[178 / 250]: Loss = 0.0408\n",
      "[179 / 250]: Loss = 0.0401\n",
      "[180 / 250]: Loss = 0.0318\n",
      "[181 / 250]: Loss = 0.0302\n",
      "[182 / 250]: Loss = 0.0512\n",
      "[183 / 250]: Loss = 0.0429\n",
      "[184 / 250]: Loss = 0.0366\n",
      "[185 / 250]: Loss = 0.0495\n",
      "[186 / 250]: Loss = 0.0300\n",
      "[187 / 250]: Loss = 0.0421\n",
      "[188 / 250]: Loss = 0.0339\n",
      "[189 / 250]: Loss = 0.0421\n",
      "[190 / 250]: Loss = 0.0454\n",
      "[191 / 250]: Loss = 0.0379\n",
      "[192 / 250]: Loss = 0.0558\n",
      "[193 / 250]: Loss = 0.0479\n",
      "[194 / 250]: Loss = 0.0467\n",
      "[195 / 250]: Loss = 0.0358\n",
      "[196 / 250]: Loss = 0.0345\n",
      "[197 / 250]: Loss = 0.0512\n",
      "[198 / 250]: Loss = 0.0368\n",
      "[199 / 250]: Loss = 0.0330\n",
      "[200 / 250]: Loss = 0.0459\n",
      "[201 / 250]: Loss = 0.0329\n",
      "[202 / 250]: Loss = 0.0356\n",
      "[203 / 250]: Loss = 0.0401\n",
      "[204 / 250]: Loss = 0.0483\n",
      "[205 / 250]: Loss = 0.0421\n",
      "[206 / 250]: Loss = 0.0292\n",
      "[207 / 250]: Loss = 0.0381\n",
      "[208 / 250]: Loss = 0.0484\n",
      "[209 / 250]: Loss = 0.0368\n",
      "[210 / 250]: Loss = 0.0453\n",
      "[211 / 250]: Loss = 0.0403\n",
      "[212 / 250]: Loss = 0.0436\n",
      "[213 / 250]: Loss = 0.0402\n",
      "[214 / 250]: Loss = 0.0427\n",
      "[215 / 250]: Loss = 0.0427\n",
      "[216 / 250]: Loss = 0.0314\n",
      "[217 / 250]: Loss = 0.0380\n",
      "[218 / 250]: Loss = 0.0352\n",
      "[219 / 250]: Loss = 0.0336\n",
      "[220 / 250]: Loss = 0.0336\n",
      "[221 / 250]: Loss = 0.0367\n",
      "[222 / 250]: Loss = 0.0322\n",
      "[223 / 250]: Loss = 0.0488\n",
      "[224 / 250]: Loss = 0.0386\n",
      "[225 / 250]: Loss = 0.0405\n",
      "[226 / 250]: Loss = 0.0463\n",
      "[227 / 250]: Loss = 0.0415\n",
      "[228 / 250]: Loss = 0.0408\n",
      "[229 / 250]: Loss = 0.0431\n",
      "[230 / 250]: Loss = 0.0389\n",
      "[231 / 250]: Loss = 0.0378\n",
      "[232 / 250]: Loss = 0.0297\n",
      "[233 / 250]: Loss = 0.0417\n",
      "[234 / 250]: Loss = 0.0458\n",
      "[235 / 250]: Loss = 0.0359\n",
      "[236 / 250]: Loss = 0.0362\n",
      "[237 / 250]: Loss = 0.0504\n",
      "[238 / 250]: Loss = 0.0367\n",
      "[239 / 250]: Loss = 0.0422\n",
      "[240 / 250]: Loss = 0.0371\n",
      "[241 / 250]: Loss = 0.0425\n",
      "[242 / 250]: Loss = 0.0364\n",
      "[243 / 250]: Loss = 0.0472\n",
      "[244 / 250]: Loss = 0.0435\n",
      "[245 / 250]: Loss = 0.0391\n",
      "[246 / 250]: Loss = 0.0402\n",
      "[247 / 250]: Loss = 0.0378\n",
      "[248 / 250]: Loss = 0.0308\n",
      "[249 / 250]: Loss = 0.0374\n",
      "[0 / 32]: Loss = 0.0437\n",
      "[1 / 32]: Loss = 0.0487\n",
      "[2 / 32]: Loss = 0.0556\n",
      "[3 / 32]: Loss = 0.0533\n",
      "[4 / 32]: Loss = 0.0385\n",
      "[5 / 32]: Loss = 0.0451\n",
      "[6 / 32]: Loss = 0.0539\n",
      "[7 / 32]: Loss = 0.0460\n",
      "[8 / 32]: Loss = 0.0381\n",
      "[9 / 32]: Loss = 0.0369\n",
      "[10 / 32]: Loss = 0.0490\n",
      "[11 / 32]: Loss = 0.0392\n",
      "[12 / 32]: Loss = 0.0457\n",
      "[13 / 32]: Loss = 0.0403\n",
      "[14 / 32]: Loss = 0.0389\n",
      "[15 / 32]: Loss = 0.0480\n",
      "[16 / 32]: Loss = 0.0437\n",
      "[17 / 32]: Loss = 0.0439\n",
      "[18 / 32]: Loss = 0.0440\n",
      "[19 / 32]: Loss = 0.0516\n",
      "[20 / 32]: Loss = 0.0448\n",
      "[21 / 32]: Loss = 0.0631\n",
      "[22 / 32]: Loss = 0.0494\n",
      "[23 / 32]: Loss = 0.0461\n",
      "[24 / 32]: Loss = 0.0531\n",
      "[25 / 32]: Loss = 0.0498\n",
      "[26 / 32]: Loss = 0.0408\n",
      "[27 / 32]: Loss = 0.0383\n",
      "[28 / 32]: Loss = 0.0466\n",
      "[29 / 32]: Loss = 0.0471\n",
      "[30 / 32]: Loss = 0.0460\n",
      "[31 / 32]: Loss = 0.0618\n",
      "Epoch 4 / 5, Epoch Time = 94.79s: Train Loss = 0.0398, Train AUC = 0.9862, Val Loss = 0.0466, Val AUC = 0.9831\n",
      "[0 / 250]: Loss = 0.0335\n",
      "[1 / 250]: Loss = 0.0275\n",
      "[2 / 250]: Loss = 0.0397\n",
      "[3 / 250]: Loss = 0.0334\n",
      "[4 / 250]: Loss = 0.0201\n",
      "[5 / 250]: Loss = 0.0409\n",
      "[6 / 250]: Loss = 0.0337\n",
      "[7 / 250]: Loss = 0.0368\n",
      "[8 / 250]: Loss = 0.0365\n",
      "[9 / 250]: Loss = 0.0328\n",
      "[10 / 250]: Loss = 0.0362\n",
      "[11 / 250]: Loss = 0.0278\n",
      "[12 / 250]: Loss = 0.0479\n",
      "[13 / 250]: Loss = 0.0312\n",
      "[14 / 250]: Loss = 0.0435\n",
      "[15 / 250]: Loss = 0.0469\n",
      "[16 / 250]: Loss = 0.0331\n",
      "[17 / 250]: Loss = 0.0364\n",
      "[18 / 250]: Loss = 0.0402\n",
      "[19 / 250]: Loss = 0.0262\n",
      "[20 / 250]: Loss = 0.0357\n",
      "[21 / 250]: Loss = 0.0305\n",
      "[22 / 250]: Loss = 0.0368\n",
      "[23 / 250]: Loss = 0.0407\n",
      "[24 / 250]: Loss = 0.0390\n",
      "[25 / 250]: Loss = 0.0334\n",
      "[26 / 250]: Loss = 0.0348\n",
      "[27 / 250]: Loss = 0.0363\n",
      "[28 / 250]: Loss = 0.0389\n",
      "[29 / 250]: Loss = 0.0333\n",
      "[30 / 250]: Loss = 0.0536\n",
      "[31 / 250]: Loss = 0.0388\n",
      "[32 / 250]: Loss = 0.0387\n",
      "[33 / 250]: Loss = 0.0464\n",
      "[34 / 250]: Loss = 0.0344\n",
      "[35 / 250]: Loss = 0.0386\n",
      "[36 / 250]: Loss = 0.0440\n",
      "[37 / 250]: Loss = 0.0407\n",
      "[38 / 250]: Loss = 0.0424\n",
      "[39 / 250]: Loss = 0.0375\n",
      "[40 / 250]: Loss = 0.0311\n",
      "[41 / 250]: Loss = 0.0289\n",
      "[42 / 250]: Loss = 0.0305\n",
      "[43 / 250]: Loss = 0.0356\n",
      "[44 / 250]: Loss = 0.0295\n",
      "[45 / 250]: Loss = 0.0302\n",
      "[46 / 250]: Loss = 0.0375\n",
      "[47 / 250]: Loss = 0.0288\n",
      "[48 / 250]: Loss = 0.0246\n",
      "[49 / 250]: Loss = 0.0381\n",
      "[50 / 250]: Loss = 0.0442\n",
      "[51 / 250]: Loss = 0.0376\n",
      "[52 / 250]: Loss = 0.0444\n",
      "[53 / 250]: Loss = 0.0332\n",
      "[54 / 250]: Loss = 0.0360\n",
      "[55 / 250]: Loss = 0.0330\n",
      "[56 / 250]: Loss = 0.0286\n",
      "[57 / 250]: Loss = 0.0440\n",
      "[58 / 250]: Loss = 0.0321\n",
      "[59 / 250]: Loss = 0.0289\n",
      "[60 / 250]: Loss = 0.0459\n",
      "[61 / 250]: Loss = 0.0375\n",
      "[62 / 250]: Loss = 0.0405\n",
      "[63 / 250]: Loss = 0.0456\n",
      "[64 / 250]: Loss = 0.0388\n",
      "[65 / 250]: Loss = 0.0451\n",
      "[66 / 250]: Loss = 0.0333\n",
      "[67 / 250]: Loss = 0.0311\n",
      "[68 / 250]: Loss = 0.0293\n",
      "[69 / 250]: Loss = 0.0306\n",
      "[70 / 250]: Loss = 0.0316\n",
      "[71 / 250]: Loss = 0.0371\n",
      "[72 / 250]: Loss = 0.0391\n",
      "[73 / 250]: Loss = 0.0475\n",
      "[74 / 250]: Loss = 0.0471\n",
      "[75 / 250]: Loss = 0.0384\n",
      "[76 / 250]: Loss = 0.0265\n",
      "[77 / 250]: Loss = 0.0395\n",
      "[78 / 250]: Loss = 0.0443\n",
      "[79 / 250]: Loss = 0.0337\n",
      "[80 / 250]: Loss = 0.0424\n",
      "[81 / 250]: Loss = 0.0328\n",
      "[82 / 250]: Loss = 0.0373\n",
      "[83 / 250]: Loss = 0.0435\n",
      "[84 / 250]: Loss = 0.0338\n",
      "[85 / 250]: Loss = 0.0267\n",
      "[86 / 250]: Loss = 0.0321\n",
      "[87 / 250]: Loss = 0.0371\n",
      "[88 / 250]: Loss = 0.0386\n",
      "[89 / 250]: Loss = 0.0371\n",
      "[90 / 250]: Loss = 0.0408\n",
      "[91 / 250]: Loss = 0.0238\n",
      "[92 / 250]: Loss = 0.0373\n",
      "[93 / 250]: Loss = 0.0376\n",
      "[94 / 250]: Loss = 0.0414\n",
      "[95 / 250]: Loss = 0.0365\n",
      "[96 / 250]: Loss = 0.0325\n",
      "[97 / 250]: Loss = 0.0331\n",
      "[98 / 250]: Loss = 0.0346\n",
      "[99 / 250]: Loss = 0.0355\n",
      "[100 / 250]: Loss = 0.0476\n",
      "[101 / 250]: Loss = 0.0335\n",
      "[102 / 250]: Loss = 0.0466\n",
      "[103 / 250]: Loss = 0.0385\n",
      "[104 / 250]: Loss = 0.0338\n",
      "[105 / 250]: Loss = 0.0324\n",
      "[106 / 250]: Loss = 0.0350\n",
      "[107 / 250]: Loss = 0.0399\n",
      "[108 / 250]: Loss = 0.0278\n",
      "[109 / 250]: Loss = 0.0356\n",
      "[110 / 250]: Loss = 0.0337\n",
      "[111 / 250]: Loss = 0.0378\n",
      "[112 / 250]: Loss = 0.0358\n",
      "[113 / 250]: Loss = 0.0363\n",
      "[114 / 250]: Loss = 0.0317\n",
      "[115 / 250]: Loss = 0.0282\n",
      "[116 / 250]: Loss = 0.0335\n",
      "[117 / 250]: Loss = 0.0326\n",
      "[118 / 250]: Loss = 0.0369\n",
      "[119 / 250]: Loss = 0.0257\n",
      "[120 / 250]: Loss = 0.0407\n",
      "[121 / 250]: Loss = 0.0303\n",
      "[122 / 250]: Loss = 0.0430\n",
      "[123 / 250]: Loss = 0.0344\n",
      "[124 / 250]: Loss = 0.0357\n",
      "[125 / 250]: Loss = 0.0314\n",
      "[126 / 250]: Loss = 0.0386\n",
      "[127 / 250]: Loss = 0.0331\n",
      "[128 / 250]: Loss = 0.0386\n",
      "[129 / 250]: Loss = 0.0421\n",
      "[130 / 250]: Loss = 0.0416\n",
      "[131 / 250]: Loss = 0.0395\n",
      "[132 / 250]: Loss = 0.0367\n",
      "[133 / 250]: Loss = 0.0356\n",
      "[134 / 250]: Loss = 0.0473\n",
      "[135 / 250]: Loss = 0.0376\n",
      "[136 / 250]: Loss = 0.0322\n",
      "[137 / 250]: Loss = 0.0357\n",
      "[138 / 250]: Loss = 0.0338\n",
      "[139 / 250]: Loss = 0.0405\n",
      "[140 / 250]: Loss = 0.0296\n",
      "[141 / 250]: Loss = 0.0323\n",
      "[142 / 250]: Loss = 0.0429\n",
      "[143 / 250]: Loss = 0.0366\n",
      "[144 / 250]: Loss = 0.0368\n",
      "[145 / 250]: Loss = 0.0390\n",
      "[146 / 250]: Loss = 0.0325\n",
      "[147 / 250]: Loss = 0.0390\n",
      "[148 / 250]: Loss = 0.0504\n",
      "[149 / 250]: Loss = 0.0375\n",
      "[150 / 250]: Loss = 0.0423\n",
      "[151 / 250]: Loss = 0.0330\n",
      "[152 / 250]: Loss = 0.0402\n",
      "[153 / 250]: Loss = 0.0266\n",
      "[154 / 250]: Loss = 0.0361\n",
      "[155 / 250]: Loss = 0.0390\n",
      "[156 / 250]: Loss = 0.0370\n",
      "[157 / 250]: Loss = 0.0344\n",
      "[158 / 250]: Loss = 0.0356\n",
      "[159 / 250]: Loss = 0.0330\n",
      "[160 / 250]: Loss = 0.0290\n",
      "[161 / 250]: Loss = 0.0387\n",
      "[162 / 250]: Loss = 0.0358\n",
      "[163 / 250]: Loss = 0.0291\n",
      "[164 / 250]: Loss = 0.0406\n",
      "[165 / 250]: Loss = 0.0248\n",
      "[166 / 250]: Loss = 0.0342\n",
      "[167 / 250]: Loss = 0.0297\n",
      "[168 / 250]: Loss = 0.0320\n",
      "[169 / 250]: Loss = 0.0275\n",
      "[170 / 250]: Loss = 0.0421\n",
      "[171 / 250]: Loss = 0.0426\n",
      "[172 / 250]: Loss = 0.0312\n",
      "[173 / 250]: Loss = 0.0252\n",
      "[174 / 250]: Loss = 0.0309\n",
      "[175 / 250]: Loss = 0.0296\n",
      "[176 / 250]: Loss = 0.0306\n",
      "[177 / 250]: Loss = 0.0367\n",
      "[178 / 250]: Loss = 0.0335\n",
      "[179 / 250]: Loss = 0.0304\n",
      "[180 / 250]: Loss = 0.0313\n",
      "[181 / 250]: Loss = 0.0323\n",
      "[182 / 250]: Loss = 0.0276\n",
      "[183 / 250]: Loss = 0.0335\n",
      "[184 / 250]: Loss = 0.0416\n",
      "[185 / 250]: Loss = 0.0337\n",
      "[186 / 250]: Loss = 0.0356\n",
      "[187 / 250]: Loss = 0.0277\n",
      "[188 / 250]: Loss = 0.0332\n",
      "[189 / 250]: Loss = 0.0333\n",
      "[190 / 250]: Loss = 0.0396\n",
      "[191 / 250]: Loss = 0.0311\n",
      "[192 / 250]: Loss = 0.0275\n",
      "[193 / 250]: Loss = 0.0409\n",
      "[194 / 250]: Loss = 0.0442\n",
      "[195 / 250]: Loss = 0.0394\n",
      "[196 / 250]: Loss = 0.0294\n",
      "[197 / 250]: Loss = 0.0280\n",
      "[198 / 250]: Loss = 0.0304\n",
      "[199 / 250]: Loss = 0.0337\n",
      "[200 / 250]: Loss = 0.0367\n",
      "[201 / 250]: Loss = 0.0349\n",
      "[202 / 250]: Loss = 0.0397\n",
      "[203 / 250]: Loss = 0.0408\n",
      "[204 / 250]: Loss = 0.0408\n",
      "[205 / 250]: Loss = 0.0400\n",
      "[206 / 250]: Loss = 0.0375\n",
      "[207 / 250]: Loss = 0.0304\n",
      "[208 / 250]: Loss = 0.0297\n",
      "[209 / 250]: Loss = 0.0423\n",
      "[210 / 250]: Loss = 0.0451\n",
      "[211 / 250]: Loss = 0.0512\n",
      "[212 / 250]: Loss = 0.0400\n",
      "[213 / 250]: Loss = 0.0361\n",
      "[214 / 250]: Loss = 0.0302\n",
      "[215 / 250]: Loss = 0.0340\n",
      "[216 / 250]: Loss = 0.0418\n",
      "[217 / 250]: Loss = 0.0254\n",
      "[218 / 250]: Loss = 0.0327\n",
      "[219 / 250]: Loss = 0.0423\n",
      "[220 / 250]: Loss = 0.0410\n",
      "[221 / 250]: Loss = 0.0392\n",
      "[222 / 250]: Loss = 0.0381\n",
      "[223 / 250]: Loss = 0.0418\n",
      "[224 / 250]: Loss = 0.0427\n",
      "[225 / 250]: Loss = 0.0451\n",
      "[226 / 250]: Loss = 0.0419\n",
      "[227 / 250]: Loss = 0.0361\n",
      "[228 / 250]: Loss = 0.0306\n",
      "[229 / 250]: Loss = 0.0374\n",
      "[230 / 250]: Loss = 0.0311\n",
      "[231 / 250]: Loss = 0.0331\n",
      "[232 / 250]: Loss = 0.0317\n",
      "[233 / 250]: Loss = 0.0287\n",
      "[234 / 250]: Loss = 0.0405\n",
      "[235 / 250]: Loss = 0.0331\n",
      "[236 / 250]: Loss = 0.0411\n",
      "[237 / 250]: Loss = 0.0317\n",
      "[238 / 250]: Loss = 0.0303\n",
      "[239 / 250]: Loss = 0.0311\n",
      "[240 / 250]: Loss = 0.0325\n",
      "[241 / 250]: Loss = 0.0320\n",
      "[242 / 250]: Loss = 0.0378\n",
      "[243 / 250]: Loss = 0.0410\n",
      "[244 / 250]: Loss = 0.0448\n",
      "[245 / 250]: Loss = 0.0316\n",
      "[246 / 250]: Loss = 0.0359\n",
      "[247 / 250]: Loss = 0.0294\n",
      "[248 / 250]: Loss = 0.0385\n",
      "[249 / 250]: Loss = 0.0448\n",
      "[0 / 32]: Loss = 0.0462\n",
      "[1 / 32]: Loss = 0.0536\n",
      "[2 / 32]: Loss = 0.0464\n",
      "[3 / 32]: Loss = 0.0392\n",
      "[4 / 32]: Loss = 0.0378\n",
      "[5 / 32]: Loss = 0.0499\n",
      "[6 / 32]: Loss = 0.0463\n",
      "[7 / 32]: Loss = 0.0489\n",
      "[8 / 32]: Loss = 0.0533\n",
      "[9 / 32]: Loss = 0.0381\n",
      "[10 / 32]: Loss = 0.0558\n",
      "[11 / 32]: Loss = 0.0561\n",
      "[12 / 32]: Loss = 0.0480\n",
      "[13 / 32]: Loss = 0.0464\n",
      "[14 / 32]: Loss = 0.0414\n",
      "[15 / 32]: Loss = 0.0422\n",
      "[16 / 32]: Loss = 0.0568\n",
      "[17 / 32]: Loss = 0.0471\n",
      "[18 / 32]: Loss = 0.0464\n",
      "[19 / 32]: Loss = 0.0506\n",
      "[20 / 32]: Loss = 0.0403\n",
      "[21 / 32]: Loss = 0.0443\n",
      "[22 / 32]: Loss = 0.0479\n",
      "[23 / 32]: Loss = 0.0503\n",
      "[24 / 32]: Loss = 0.0498\n",
      "[25 / 32]: Loss = 0.0433\n",
      "[26 / 32]: Loss = 0.0424\n",
      "[27 / 32]: Loss = 0.0452\n",
      "[28 / 32]: Loss = 0.0504\n",
      "[29 / 32]: Loss = 0.0510\n",
      "[30 / 32]: Loss = 0.0458\n",
      "[31 / 32]: Loss = 0.0466\n",
      "Epoch 5 / 5, Epoch Time = 94.78s: Train Loss = 0.0360, Train AUC = 0.9898, Val Loss = 0.0471, Val AUC = 0.9844\n"
     ]
    }
   ],
   "source": [
    "vocab_size  = len(token_to_id)\n",
    "embed_size  = 300\n",
    "num_classes = 6\n",
    "\n",
    "model        = MultiChannel(torch.FloatTensor(embedding_matrix), \n",
    "                     vocab_size, \n",
    "                     embed_size,\n",
    "                     num_classes\n",
    "                    ).cuda()\n",
    "\n",
    "criterion    = nn.BCEWithLogitsLoss().cuda()\n",
    "optimizer    = optim.Adam([param for param in model.parameters() if param.requires_grad], lr=0.001)\n",
    "\n",
    "X_train      = as_matrix(data_train['tokenized_comments'], \n",
    "                         token_to_id, \n",
    "                         word_dropout=0.001, \n",
    "                         UNK_IX=UNK_IX, \n",
    "                         PAD_IX=PAD_IX,\n",
    "                         max_len=MAX_LEN\n",
    "                        )\n",
    "\n",
    "train_labels = data_train.loc[:, TARGET_COLS].values \n",
    "\n",
    "X_test       = as_matrix(data_val['tokenized_comments'],\n",
    "                         token_to_id, \n",
    "                         word_dropout=0.001, \n",
    "                         UNK_IX=UNK_IX, \n",
    "                         PAD_IX=PAD_IX,\n",
    "                         max_len=MAX_LEN\n",
    "                        )\n",
    "\n",
    "test_labels  = data_val.loc[:, TARGET_COLS].values\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, train_labels), epochs_count=5, \n",
    "    batch_size=512, val_data=(X_test, test_labels), val_batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch 5 / 5, Epoch Time = 13.93s: Train Loss = 0.0485, Train AUC = 0.9726, Val Loss = 0.0570, Val AUC = 0.9683"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
